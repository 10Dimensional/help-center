[
    {
        "date": "2017-04-11T00:00:00Z",
        "uri": "/content/docs/distributing-an-application/add-nodes-replicated.md",
        "content": "\nWhen an application is configured by the vendor with a clustering strategy, Replicated makes it possible for the end customer to install additional nodes on remote instances to run a distributed application. Installations of Replicated using the easy installation script will install an operator on the local node automatically.\n\nOn the Cluster page on the On-Prem Console an \"Add Node\" button will be visible. This will prompt the end customer with two simple options for adding a node.\n\nScripted Installation\nThe scripted install is the recommended means for adding an additional node to Replicated. The end customer will be prompted for the private and optionally the public address of the server.\n\nDocker Installation\nIf a scripted install is not possible, additionally a docker script is provided for installing additional nodes.\n",
        "title": "Adding Nodes To A Replicated Cluster",
        "description": "Instructions for installing additional nodes with a Replicated cluster",
        "keywords": "installing, swarm"
    },
    {
        "date": "2017-04-17T00:00:00Z",
        "uri": "/content/docs/distributing-an-application/add-nodes-swarm.md",
        "content": "\nWhen it is necessary to add additional nodes to satisfy the scheduling requirements of an application, Replicated makes it easy for the end customer to add additional Swarm nodes on remote instances to run a distributed application.\n\nOn the Cluster page on the Admin Console an \"Add Node\" button will be visible. This will prompt the end customer with two simple options for adding a node.\n\nScripted Installation\nThe scripted install is the recommended means for adding an additional node to the Swarm cluster. The end customer will be prompted for the private and optionally the public address of the server.\n\nDocker Installation\nIf a scripted install is not possible, additionally a docker CLI command is provided for adding additional nodes.\n",
        "title": "Adding Nodes To A Swarm Cluster",
        "description": "Instructions for installing additional nodes with a Swarm cluster",
        "keywords": "installing, swarm"
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/distributing-an-application/add-nodes.md",
        "content": "\nThe instructions to add additional nodes are different depending the running scheduler.\n\nReplicated Scheduler\nTo add additional nodes when running on the Replicated scheduler, refer to the instructions on the /cluster page of the Admin Console. For details, visit the instructions for adding additional Replicated nodes.\n\nSwarm Scheduler\nTo add additional nodes when running on the Swarm scheduler, refer to the instructions on the /cluster page of the Admin Console. For details, visit the instructions for adding additional Swarm nodes.\n\nAirgapped Installations\nWhen adding a remote node in an airgapped installation, each node will require that Docker is already installed. When adding a node via the easy installation method, the Replicated airgap archive must be copied over manually to the remote node. The archive can be downloaded here.",
        "title": "Add Nodes",
        "description": "When a Replicated-orchestrated application is configured with a clustering strategy, additional nodes can be installed on remote instances to take part in the cluster.",
        "weight": "307",
        "categories": [
            "Distributing"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/distributing-an-application/airgapped-installations.md",
        "content": "\nAn \"airgapped\" environment is a network that has no path to inbound or outbound internet traffic at all.\nSome enterprise customers require that you ship a package they can install in their airgapped environment.\n\nReplicated supports this type of installation, using the following steps:\n\nPrepare the environment\nThe customer will be responsible for delivering a server running a supported version of Docker. Replicated\nsupports Docker from {{}} to {{}}. We recommend that you use the latest version of Docker available in this range for your operating system.\n\nThe Replicated airgap installation script does not install docker-engine. We've written a\nguide with some tips that might help get Docker installed into air gapped machines with various operating systems.\n\nInstall Replicated\nReplicated can be installed by downloading the latest release from\nhttps://s3.amazonaws.com/replicated-airgap-work/replicated.tar.gz and running the following commands:\n\ntar xzvf replicated.tar.gz\ncat ./install.sh | sudo bash -s airgap\n\nSwarm Mode\n\n{{}}\ntar xzvf replicated.tar.gz\ncat ./swarm-init.sh | sudo bash -s airgap\n\nDownload & Rename Airgap Package\nOn the license properties page in the vendor portal, enable Airgap installations for this license and copy the\ndownload link. This URL is designed to be delivered to that customer. They will use this link to download\ncurrent airgap packages when you promote a release. When they download new airgap packages to their server,\nit is important that your customer set the --trust-server-names and --content-disposition flags for wget\nor rename the file to something ending with .airgap.\n\nYour customer will need the .airgap package and the normal Replicated license (.rli) file. Be sure to download\nthe license file after enabling the airgap feature on the license. Airgap-enabled licenses have more metadata\nembedded than non-airgap licenses. Airgap enabled licenses can be used to install in non-airgap mode, but\nnon-airgap licenses cannot be used to install in airgap mode.\n\nInstall Airgap Package\nNext, navigate to the management console at https://:8800. Accept the self signed certificate, pass\nthe preflight checks, and you will see the license upload screen. Upload the airgap enabled license and then select the airgapped install option.\nYou will have to provide a path to the .airgap file and upload the .rli file here.\n\nOnce this screen is completed, Replicated runs as normal. In the :8800/console/settings page, there is a section\nto set the Airgap mode settings. You can install updates and sync the license by downloading new versions of these,\nrenaming them with the .airgap extension and placing them in the locations specified on the /console/settings\npage.\n\nAdding Additional Nodes\n\nIn order to add additional nodes to your cluster, just navigate to the Cluster page of the Admin Console, click the \"Add Node\" button, and follow the instructions there. For more detailed instructions see the add nodes page of the docs.",
        "title": "Airgapped Installations",
        "description": "The steps required of the end customer to install a Replicated application into an air gapped environment.",
        "categories": [
            "Distributing"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/distributing-an-application/create-licenses.md",
        "content": "\nAn \"airgapped\" environment is a network that has no path to inbound or outbound internet traffic at all.\nSome enterprise customers require that you ship a package they can install in their airgapped environment.\n\nReplicated supports this type of installation, using the following steps:\n\nPrepare the environment\nThe customer will be responsible for delivering a server running a supported version of Docker. Replicated\nsupports Docker from {{}} to {{}}. We recommend that you use the latest version of Docker available in this range for your operating system.\n\nThe Replicated airgap installation script does not install docker-engine. We've written a\nguide with some tips that might help get Docker installed into air gapped machines with various operating systems.\n\nInstall Replicated\nReplicated can be installed by downloading the latest release from\nhttps://s3.amazonaws.com/replicated-airgap-work/replicated.tar.gz and running the following commands:\n\ntar xzvf replicated.tar.gz\ncat ./install.sh | sudo bash -s airgap\n\nSwarm Mode\n\n{{}}\ntar xzvf replicated.tar.gz\ncat ./swarm-init.sh | sudo bash -s airgap\n\nDownload & Rename Airgap Package\nOn the license properties page in the vendor portal, enable Airgap installations for this license and copy the\ndownload link. This URL is designed to be delivered to that customer. They will use this link to download\ncurrent airgap packages when you promote a release. When they download new airgap packages to their server,\nit is important that your customer set the --trust-server-names and --content-disposition flags for wget\nor rename the file to something ending with .airgap.\n\nYour customer will need the .airgap package and the normal Replicated license (.rli) file. Be sure to download\nthe license file after enabling the airgap feature on the license. Airgap-enabled licenses have more metadata\nembedded than non-airgap licenses. Airgap enabled licenses can be used to install in non-airgap mode, but\nnon-airgap licenses cannot be used to install in airgap mode.\n\nInstall Airgap Package\nNext, navigate to the management console at https://:8800. Accept the self signed certificate, pass\nthe preflight checks, and you will see the license upload screen. Upload the airgap enabled license and then select the airgapped install option.\nYou will have to provide a path to the .airgap file and upload the .rli file here.\n\nOnce this screen is completed, Replicated runs as normal. In the :8800/console/settings page, there is a section\nto set the Airgap mode settings. You can install updates and sync the license by downloading new versions of these,\nrenaming them with the .airgap extension and placing them in the locations specified on the /console/settings\npage.\n\nAdding Additional Nodes\n\nIn order to add additional nodes to your cluster, just navigate to the Cluster page of the Admin Console, click the \"Add Node\" button, and follow the instructions there. For more detailed instructions see the add nodes page of the docs.",
        "title": "Airgapped Installations",
        "description": "The steps required of the end customer to install a Replicated application into an air gapped environment.",
        "categories": [
            "Distributing"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/distributing-an-application/index.md",
        "content": "\nOnce the application YAML is created, the next task is to create licenses and install them to test.  This section\nof the docs explains how to install, upgrade and distribute your application.",
        "title": "Distributing an Application",
        "description": "This section of the docs explains how to install, upgrade and distribute your application.",
        "weight": "301",
        "type": "section",
        "categories": [
            "Distributing"
        ],
        "hideSection": true
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/distributing-an-application/installing-1.2.md",
        "content": "\n{{}}\nThe content in this document is for a previous version of Replicated. If you are looking\nfor the current version, it is available at\n}}distributing-an-application/installing/\"{{}}distributing-an-application/installing/\n{{}}\n\nHost Setup\nBefore installing your app, you need to install Replicated on a compatible machine.\n\nSupported Operating Systems\nWe support the server versions of the following OS's\n\nDebian 7.7+\nUbuntu 14.04 / 15.10\nFedora 21 / 22\nRed Hat Enterprise Linux 7.1\nCentOS 7+\nAmazon AMI 2014.03 / 2014.09 / 2015.03 / 2015.09\n\nPlease note your machine must support *docker-engine {{}} - {{}} (with{{}} being the\nrecommended version). This also requires a 64-bit distribution with a kernel minimum of\n3.10*. For detailed requirements and installation guides see the docker installation docs.\n\nCurrent Replicated Versions\n| Daemon\t| Stable Version |\n|-------|----------------|\n| replicated | 1.2.134  1 November, 2016 |\n| replicated-ui | 1.2.80  1 November, 2016 |\n| replicated-agent | 1.2.51  1 November, 2016 |\n\nEasy Installation\nWe provide an easy-to-use one-line installation process (via shell script) which will\ndetect your OS, ask a few questions and install the Replicated components for you\nincluding docker-engine. More details on the installation script:\n\nWith Timeout Prompts\ncurl -sSL https://get.replicated.com | sudo sh\n\nWait Indefinitely\ncurl -sSL https://get.replicated.com | sudo bash -s no-auto\n\nWhen you're ready to start shipping to customers, you can either proxy this install\nscript or provide TLS certs for us to CNAME it for you. An example of customer facing\ninstallation guide can be found at our unpublished demo app: GetElk\n\nAccessing the On-prem UI\nNow you're ready to start deploying your app! The Replicated On-Prem UI is web-based,\nand can be accessed via port 8800 over HTTPS of the server you've installed Replicated\non (make sure that port 8800 is accessible from your local computer).\n\nYou'll need to create & download a license file for yourself on the vendor portal & then\njust follow the instructions from there.\n\nAdvanced Installation Options\nManual Installation\nIf you'd rather install the components manually, you can! Just use the following steps.\nIf you're using an apt-based OS, such as Debian or Ubuntu, you'll want to use the \"apt\"\ntab when using the scripts shown below. If you're using a yum-based OS, such as RHEL or\nCentOS, go for the \"yum\" tab.\n\nInstall Docker\nCurrently the Replicated installation script installs Docker version {{}}\nRefer to the Docker Installation Guide for Debian,\nUbuntu, CentOS,\nFedora, or RHEL.\n\nAdd the Replicated repository to package manager\n\nUbuntu/Debian\necho \"deb https://get.replicated.com/apt all stable\" | sudo tee /etc/apt/sources.list.d/replicated.list\n\nCentOS/RHEL/Fedora\necho -e \"[replicated]\\nname = Replicated Repository\\nbaseurl = https://get.replicated.com/yum/stable\\n\" | sudo tee /etc/yum.repos.d/replicated.repo\n\nDownload the Replicated GPG key\n\nUbuntu/Debian\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 68386EDB2C8B75CA615A8C985D4781862AFFAC40\n\nCentOS/RHEL/Fedora\ngpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 822819BB\ngpg --export -a 822819BB  /tmp/replicated_pub.asc\nsudo rpm --import /tmp/replicated_pub.asc\n\nInstall Replicated via package manager\n\nUbuntu/Debian\napt-get update\napt-get install replicated replicated-ui replicated-updater\n\nCentOS/RHEL/Fedora\nyum clean all\nyum install replicated replicated-ui replicated-updater\n\nInstalling Behind A Proxy\nReplicated introduced an enhanced installer that should simplify the proxy installation\nexperience. However, previous methods of identifying proxied installs are still supported.\n\nPost-Installation Maintenance\nRestarting Replicated\nRestarting replicated varies depending on your host OS, please see below for the correct\ninstructions to restarting replicated.\n\nUbuntu/Debian\nrestart replicated-ui\nrestart replicated\nrestart replicated-agent\n\nCentOS/RHEL/Fedora\nsystemctl restart replicated replicated-updater replicated-ui replicated-agent\n\nIf you need to reset your console password please refer to the reseting your password\nin the On-Prem CLI section.\n\nList Installed Replicated Version\nNote: You can also use the CLI to determine the version of the daemon\n\nUbuntu/Debian\ndpkg --list | grep replicated\n\nCentOS/RHEL/Fedora\nyum list installed | grep replicated\n\nUpdate Replicated via package manager\nYou may need to add the Replicated repository and GPG key to the package manager before\nrunning commands, see Manual Installation for more details.\n\nUbuntu/Debian\napt-get update\napt-get install replicated replicated-ui replicated-agent\n\nCentOS/RHEL/Fedora\nyum makecache\nyum update replicated replicated-ui replicated-agent\n\nIf you have additional hosts you will independently need to run the following on each of them.\n\ncurl -sSL https://get.replicated.com/agent | sudo sh\n\nLog Rotation\nBy default, the Replicated installer script will set up log rotation for you. This section\ndescribes how to get it up and running manually just in case.\n\nReplicated components write log files to /var/log/replicated. These logs contain useful\ndiagnostic information, and can grow somewhat large. We recommend setting up a log rotation\npolicy for these files. The easiest way to accomplish this is via the logrotate facility\npresent on most Linux-based systems. Below is an example policy file that you can create\nin /etc/logrotate.d/replicated to be picked up by logrotate. It will rotate the\nReplicated log files whenever they become larger than 500 KB, keeping at most 4 rotated\nlog files around on the filesystem.\n\n/var/log/replicated/*.log {\n  size 500k\n  rotate 4\n  nocreate\n  compress\n  notifempty\n  missingok\n}\n\nRemoving Replicated\nTo remove Replicated from a given host you can run the following command.\n\nUbuntu/Debian\ndpkg --purge replicated replicated-ui replicated-agent replicated-updater\nrm -rf /var/lib/replicated /etc/replicated-agent.conf /etc/replicated.conf /etc/replicated-agent\n\nCentOS/RHEL/Fedora\nyum remove -y replicated replicated-ui replicated-agent replicated-updater\nrm -rf /var/lib/replicated /etc/replicated-agent.conf /etc/replicated.conf /etc/replicated-agent\n\nTroubleshooting Agent Installation\nWhen provisioning a remote agent machine via SSH, you may receive an error like\nUnable to use 'sudo' on agent host.\n\nTo fix this, you'll need to disable the requiretty setting in your sudo configuration.\nOn most Linux-based systems, you can simply run sudo visudo from a shell, which will open\nan editor for your sudo configuration. Add an exclamation point to any occurrences of\nrequiretty, e.g. !requiretty, then save the file.\n\nAfter you have made this change, you can try provisioning the machine again from the\nReplicated UI.\n\nPlease note that this may have security implications, so make sure you check the\ndocumentation for your OS and consider the effects carefully.\n\nMigrating to Replicated v2\nReplicated provides a one line migration script to upgrade your v1 installation to v2. The script will first stop your app and backup all Replicated data in case there is a need for a restore. To invoke the migration script all you have to do is run the script below and follow the prompts.\n\ncurl -sSL https://get.replicated.com/migrate-v2 | sudo bash\n\n{{}}\nTo prevent loss of data, backing up your server is highly recommended before performing a migration.\n{{}}",
        "title": "Installing Replicated 1.2",
        "description": "How to install the legacy version of Replicated 1.2 via Deb and Yum packages."
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/distributing-an-application/installing-manually.md",
        "content": "\nIf you choose not to run the installation script use this guide; note that the install script also installs Docker, detects network configuration and allows proxy settings and provides support to auto-upgrade Replicated during your application release cycle.\n\nTo manually install start by checking you are running on a support operating system and follow the 4 steps.\n\n1. Install Docker\nWe recommend Docker version {{}}.  Refer to the Docker Installation Guide for Debian, Ubuntu, CentOS, Fedora, or RHEL.\n\n2. Run Replicated & UI Containers\nexport DOCKERHOSTIP=172.17.0.1  # Set this appropriately to docker0 address\nexport LOCAL_ADDRESS=10.240.0.2  # Set this to the internal address of the server (usually eth0, but not 127.0.0.1)\n\necho 'alias replicated=\"sudo docker exec -it replicated replicated\"'  /etc/replicated.alias\n\ndocker run -d --name=replicated \\\n        -p 9874-9879:9874-9879/tcp \\\n        -v /etc/replicated.alias:/etc/replicated.alias \\\n        -v /var/lib/replicated:/var/lib/replicated \\\n        -v /var/run/replicated:/var/run/replicated \\\n        -v /etc/docker/certs.d:/host/etc/docker/certs.d \\\n        -v /var/run/docker.sock:/host/var/run/docker.sock \\\n        -v /proc:/host/proc:ro \\\n        -v /etc:/host/etc:ro \\\n        -e DOCKERHOSTIP=$DOCKERHOSTIP \\\n        -e LOCALADDRESS=$LOCALADDRESS \\\n        quay.io/replicated/replicated:latest\n\ndocker run -d --name=replicated-ui \\\n        -p 8800:8800/tcp \\\n        -v /var/run/replicated:/var/run/replicated \\\n        quay.io/replicated/replicated-ui:latest\n\n3. Upload the License\nNavigate to http://&lt;your server address&gt;:8800.\nFollow the prompts to configure certificates, upload license, and run the preflight checks.\n\n4. Run Operator Container\nClick on the Cluster tab (:8800/cluster)\n\nClick the Add Node button\nSelect Docker Run option\nCopy the command from the text area below\nPaste and run the command in the terminal window\n\nAt this point, the new node should show up on the Cluster page.\n\n4. Start the Application\nClick on the Dashboard tab (:8800/dashboard)\nClick the Start Now button\n\nWhen first launching there may be no \"Start Now\" button.  This is because Replicated is still pulling application images. If this is the case, then just wait for the pull to finish.\n\nSince a new node running Replicated Operator has joined the cluster, Replicated will want to run preflight checks on it before starting the application. If that's the case, the Start Now button will be replaced with the Run Checks button.\n",
        "title": "Manually Installing Replicated",
        "description": "Instructions for manually installing Replicated 2",
        "keywords": "installing, removing"
    },
    {},
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/distributing-an-application/installing-via-script.md",
        "content": "\nWe provide an easy-to-use one-line installation process (via shell script) which will detect your OS, ask a few questions and install both docker-engine and the required Replicated components.\n\nIf you want to always release your application with a specific version of Replicated you can read how to always install a specific version of Replicated.\n\nBasic Install\n\nSave the install script to file and run.  We recommend reading and understanding the install script prior to running.\n\ncurl -sSL -o install.sh https://get.replicated.com/docker\nsudo bash ./install.sh\n\nQuick install\n\ncurl -sSL https://get.replicated.com/docker | sudo bash\n\nRelease Channel Install Scripts\n\nEvery release channel for your application has a custom install link. Using the channel install link allows the Replicated installer will optimize the install process. Based on your app YAML the installer will choose the highest allowed version of Replicated using your configured replicated_version range.\n\nTo find the install link, login to your vendor.replicated.com account, select your app and click \"build history\" for your channel and click \"Copy install script url\".\n\nFlags\n\nThe install script takes optional flags to configure Replicated for your environment.\n\n|Flag|Usage|\n|----|-----|\n|http-proxy |Sets the HTTP proxy for Docker and Replicated|\n|no-proxy|Skip the proxy prompt|\n|private-address |Set the nodes private IP address|\n|public-address |Set the nodes public IP (service) address|\n|no-auto|Prompts will wait indefinitely instead of 20 second timeouts|\n|ui-bind-port |Change the default UI port binding from port 8800|\n|docker-version |Install with a specific version of Docker|\n|no-docker|Skip the docker installation|\n|bypass-storagedriver-warnings|For automation bypasses the warning for devicemapper with loopback|\n\nExample call with flags:\n\ncurl -sSL https://get.replicated.com/docker | sudo bash -s no-auto ui-bind-port=8000\n\n{{}}\nWhen you're ready to start shipping to customers we recommend that you proxy this install script with a TLS cert matching your domain name.  The script should always be served with TLS.\n{{}}\n\nInstalling Behind A Proxy\nThe Replicated installation script supports environments where an HTTP proxy server is required to access the Internet. The installation script will prompt for the proxy address and will set up Replicated and Docker to use the supplied value.\n\nAn example of running the Replicated installation script with a proxy server is:\ncurl -x http://: https://get.replicated.com/docker | sudo bash\n\nPost-Installation Maintenance\n\nUpgrade to latest Replicated build.\nIf you would like to upgrade Replicated to the latest release simply rerun the installation script and that will upgrade the Replicated components to the latest build.\n\nRestarting Replicated\nIf you installed Replicated using the easy installation script, the script will have created an init service you can use to control Replicated. In this case, restarting replicated varies depending on your host OS.\n\nUbuntu/Debian\nservice replicated restart\nservice replicated-ui restart\nservice replicated-operator restart\n\nCentOS/RHEL/Fedora\nsudo systemctl restart replicated replicated-ui replicated-operator\n\nRemoving Replicated\nTo remove Replicated run the following script.\n\nUbuntu/Debian\nservice replicated stop\nservice replicated-ui stop\nservice replicated-operator stop\ndocker stop replicated-premkit\ndocker stop replicated-statsd\ndocker rm -f replicated replicated-ui replicated-operator replicated-premkit replicated-statsd\ndocker images | grep \"quay\\.io/replicated\" | awk '{print $3}' | xargs sudo docker rmi -f\napt-get remove -y replicated replicated-ui replicated-operator\napt-get purge -y replicated replicated-ui replicated-operator\nrm -rf /var/lib/replicated* /etc/replicated* /etc/init/replicated* /etc/init.d/replicated* /etc/default/replicated* /var/log/upstart/replicated* /etc/systemd/system/replicated*\n\nCentOS/RHEL/Fedora\nsystemctl stop replicated replicated-ui replicated-operator\nservice replicated stop\nservice replicated-ui stop\nservice replicated-operator stop\ndocker stop replicated-premkit\ndocker stop replicated-statsd\ndocker rm -f replicated replicated-ui replicated-operator replicated-premkit replicated-statsd\ndocker images | grep \"quay\\.io/replicated\" | awk '{print $3}' | xargs sudo docker rmi -f\nyum remove -y replicated replicated-ui replicated-operator\nrm -rf /var/lib/replicated* /etc/replicated* /etc/init/replicated* /etc/default/replicated* /etc/systemd/system/replicated* /etc/sysconfig/replicated* /etc/systemd/system/multi-user.target.wants/replicated* /run/replicated*\n`",
        "title": "Installing Replicated with the Easy Install Script",
        "description": "Instructions for installing Replicated via the easy install script.",
        "keywords": "installing"
    },
    {
        "date": "2017-04-11T00:00:00Z",
        "uri": "/content/docs/distributing-an-application/installing-with-swam.md",
        "content": "\nWe distribute an installation script that can be used to install Replicated into a new or existing Swarm cluster. The cluster does not have to be created at this point, the Replicated install script can install Docker Engine and provision a new Swarm cluster.\n\n{{}}\nThe Swarm scheduler requires a new YAML format as discussed here: Replicated with Docker Swarm. Only one scheduler, Replicated or Swarm, can be used in any given installation and switching between schedulers is not supported.\n{{}}\n\nBasic install (recommended):\n\nThe basic install will install Docker (as needed) and Replicated. It will save the install script to a file which you can inspect and then run. We recommend reading and understanding the install script prior to running.\n\ncurl -sSL -o install.sh  https://get.replicated.com/swarm-init\nsudo bash ./install.sh\n\nQuick Install:\n\nThe quick Swarm install will install Docker (as needed) and Replicated. Use this method if you have no need to view/change the installer script and you just want a one-line install.\n\ncurl -sSL https://get.replicated.com/swarm-init | sudo bash\n\nFlags:\nThe install script can take flags to help your customers with specialized enterprise setups.\n\n|Flag|Usage|\n|----|-----|\n|airgap|airgap implies \"no proxy\" and \"skip docker\"|\n|bypass-storagedriver-warnings|Bypass the storagedriver warning|\n|daemon-token|Authentication token used by operators for automating a cluster installation|\n|docker-version|Install a specific version of docker|\n|http-proxy|If present, then use proxy|\n|log-level|If present, this will be the log level of the Replicated daemon (debug, info, or error).|\n|no-docker|Skip docker installation|\n|no-proxy|If present, do not use a proxy|\n|public-address|The public IP address for stack|\n|swarm-advertise-addr|The swarm advertise address|\n|swarm-listen-addr|The swarm listen address|\n|swarm-stack-namespace|The swarm stack namespace to use|\n|ui-bind-port|The port to bind the UI to|\n\nExample quick install with flags:\ncurl -sSL https://get.replicated.com/swarm-init | sudo bash -s no-proxy ui-bind-port=8000\n\nAdvanced Install:\n\nThe advanced Swarm install requires the host is running Docker with a version between {{}} - {{}}.\n\nThis method will save the Docker Compose YAML to a file and then run a command using the YAML file as the input. We recommend reading and understanding the Compose file prior to running.\n\ndocker swarm init\ncurl -sSL -o docker-compose.yml \"https://get.replicated.com/docker-compose.yml?swarmnodeaddress=$(docker info --format '{{.Swarm.NodeAddr}}')\"\ndocker node update --label-add replicated-role=master \"$(docker info --format '{{.Swarm.NodeID}}')\"\nexport LCCTYPE=C;echo \"$(head -c 128 /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1)\" | docker secret create daemontoken -\ndocker stack deploy -c docker-compose.yml replicated\n\nInstalling Behind A Proxy\n\nProxy support for Swarm will be included in a future release of Replicated.\n\nUninstall Entire Swarm Stack\nTo remove the entire Swarm stack run the following script.\n{{}}\nThis will remove everything including images, volumes, secrets, etc.. Don't do this unless you are planning on completely starting over.\n{{}}\n\ndocker stack ls | grep replicated_ | awk '{print $1}' | xargs docker stack rm\nsleep 5; docker service rm premkit_replicated\nsleep 5; docker service rm statsd_replicated\nsleep 5; docker volume rm replicated-premkit-data-volume\nsleep 5; docker volume ls | grep statsd | awk '{print $2}' | xargs docker volume rm\nsleep 5; docker network rm statsdreplicated premkitreplicated\nsleep 5; docker stack rm replicated\nsleep 5; docker ps -a | grep piper | awk '{print $1}' | xargs docker rm\nsleep 10; docker volume ls | grep replicated | awk '{print $2}' | xargs docker volume rm\nsleep 5; docker images | grep 'replicated\\|premkit' | awk '{print $3}' | xargs docker rmi\nsleep 5; docker secret ls | grep 'replicated\\daemon_token' | awk '{print $1}' | xargs docker secret rm\n`",
        "title": "Installing Replicated with Docker Swarm",
        "description": "Instructions for installing Replicated with a Swarm cluster",
        "keywords": "installing, swarm",
        "aliases": [
            "/distributing-an-application/installing-on-swarm/"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/distributing-an-application/installing.md",
        "content": "\n{{}}\nThe content in this document is specific to Replicated 2.0. If you are looking for the Replicated 1.2 version of this document, it is available at {{}}distributing-an-application/installing-1.2/\n{{}}\n\nPrerequisites\nBefore installing Replicated review the list of\nsupported operating systems.\n\nReplicated Version\nThe current release of Replicated is version {{}} released on {{}}.  Prior versions are available and can also be installed and instructions how depend on your install method. To read about the latest features of Replicated see the Replicated changelog.\n\nTL;DR\n\ncurl -sSL -o install.sh https://get.replicated.com/docker\nsudo bash ./install.sh\n\nPick your Install Method\n\nEasy Installation\nWe provide an easy-to-use one-line installation process (via shell script) which will detect your OS, ask a few questions and install the Replicated.  Included in the install will be init scripts, configuration files and as needed the Docker Engine.\n\nContinue with the easy install script\nManual Install\nWe provide a short set of steps to install Replicated manually, for situations where a customer is unable to run the easy install script or you need to install Replicated on an operating system not yet covered by the easy install.  The manual install giving you full control over the install process.\n\nContinue with a manual install\n\nAirgapped Install\nSome installations may not have outbound Internet access.  Replicated provides you the option to do airgapped installs to support those customers.  To install, your customer will download Replicated, your license and the airgap installation file containing your application.\n\nContinue with an airgapped install\n\nKubernetes Install\n{{}} Replicated can be installed to a Kubernetes cluster. This requires a provisioned Kubernetes cluster and the application release to be installed must include Kubernetes YAML.\n\nContinue with a Kubernetes install\n\nDocker Swarm Install\n{{}} Replicated can be installed with a Swarm cluster. This doesn't require any additional setup, but the application YAML must be written as Swarm services and the machines must be capable of running Docker 1.13.1 or newer.\n\nContinue with a Swarm install",
        "title": "Installing Replicated",
        "description": "Instructions for installing Replicated via the easy install script, manually or behind a proxy. Also includes instructions for uninstalling Replicated.",
        "keywords": "installing, removing, migrating",
        "weight": "304",
        "categories": [
            "Distributing"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/distributing-an-application/maintenance.md",
        "content": "\nCommand Line Interface\n\nYou can use the command line interface to list the installed app, start and stop\nyour application, get the current status, ask for updates or support bundles and more.\n\nReset Console Password\n\nIf you need to reset your console password you can do so using the command line interface running as the\nroot user.  See our Reseting the On-Prem Admin Password\nKB article.\n\nAutomate Install\n\nReplicated allows for install automation,\nallowing you to install TLS certificates and configure hostnames and control your\napplication settings, including for LDAP integration.",
        "title": "Maintenance",
        "description": "Maintenance processes to use after Replicated is installed",
        "weight": "306",
        "categories": [
            "Distributing"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/distributing-an-application/promote-releases.md",
        "content": "\nEvery Replicated license points to a Release Channel. When a license is installed, it will pull down and install the release that is currently at the top of its channel. It’s recommended to create customer licenses on the Stable channel, and only promote releases to Stable that are ready for all customers to install.\n\nOnce an application is installed, the active instance can be updated by promoting a release to the channel that instance is licensed to (likely Stable).  Each instance will periodically check for new releases. When a new release is found, the Replicated installation will show a button that allows end customers managing the instance to read the release notes and install the update.\nA license only checks it’s own release channel.\n\nTo promote a release, you can use the vendor portal and click the Promote button:\n\nWhen a release is promoted it should be given a version label and detailed release notes. The release notes support markdown and will be shown to your customer. Additionally, each promoted release must be given a required status (required or not required).\n\n{{% page_notes %}}\nBefore you can create or install a license, a release must be promoted to the channel.\nUpdate checking defaults to every 5 hours but can be configured by end customers.\nIt is possible to change a license value to have updates automatically installed when detected by the running instance.\nLicense values are synced with the values set in the vendor portal at every update check.\nReleases will not be editable after being promoted to a channel.\nRelease notes, version numbers, the required status may be edited after promotion by visiting the channel's history.\n{{% /page_notes %}}",
        "title": "Promote Releases",
        "description": "The process for leveraging Replicated's release channel management functionality to stage versions and deliver updates to customers.",
        "weight": "303",
        "categories": [
            "Distributing"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/distributing-an-application/support-operating-systems.md",
        "content": "\nReplicated supports Linux-based servers that can run current versions of Docker.\n\nYour machine must support docker-engine {{}} - {{}}\n(with {{}} being the recommended version). This also requires a 64-bit distribution with a\nkernel minimum of 3.10.\n\nReplicated provides an easy install script to work with the the following list of operating system.\n\nDebian 7.7+\nUbuntu 14.04 / 15.10 / 16.04\nFedora 21 / 22\nRed Hat Enterprise Linux 6.5+\nCentOS 6+\nAmazon AMI 2014.03 / 2014.09 / 2015.03 / 2015.09 / 2016.03 / 2016.09\nOracle Linux 6.5+\n\nAs new operating systems start to support Docker we extend our install script and QA process to include them.  Operating\nsystems that support Docker but are not yet included in the easy install script can still be used\nusing the manual install instructions.\n\nFor detailed requirements and installation guides see the docker installation docs.\n",
        "title": "Supported Operating Systems",
        "description": "View the supported operating systems",
        "keywords": "operating system, os"
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/distributing-an-application/upgrading-1.2.md",
        "content": "\nEvery Replicated license points to a Release Channel. When a license is installed, it will pull down and install the release that is currently at the top of its channel. It’s recommended to create customer licenses on the Stable channel, and only promote releases to Stable that are ready for all customers to install.\n\nOnce an application is installed, the active instance can be updated by promoting a release to the channel that instance is licensed to (likely Stable).  Each instance will periodically check for new releases. When a new release is found, the Replicated installation will show a button that allows end customers managing the instance to read the release notes and install the update.\nA license only checks it’s own release channel.\n\nTo promote a release, you can use the vendor portal and click the Promote button:\n\nWhen a release is promoted it should be given a version label and detailed release notes. The release notes support markdown and will be shown to your customer. Additionally, each promoted release must be given a required status (required or not required).\n\n{{% page_notes %}}\nBefore you can create or install a license, a release must be promoted to the channel.\nUpdate checking defaults to every 5 hours but can be configured by end customers.\nIt is possible to change a license value to have updates automatically installed when detected by the running instance.\nLicense values are synced with the values set in the vendor portal at every update check.\nReleases will not be editable after being promoted to a channel.\nRelease notes, version numbers, the required status may be edited after promotion by visiting the channel's history.\n{{% /page_notes %}}",
        "title": "Promote Releases",
        "description": "The process for leveraging Replicated's release channel management functionality to stage versions and deliver updates to customers.",
        "weight": "303",
        "categories": [
            "Distributing"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/distributing-an-application/upgrading.md",
        "content": "\n{{}}\nThe content in this document is specific to updating an existing easy install script installed\nReplicated instance. If you are looking for the\nReplicated 1.2 version of this document it is available at\n{{}}distributing-an-application/upgrading-1.2/\n{{}}\n\nYou can update all Replicated component versions to latest by re-running the installation\nscript.\n\ncurl -sSL https://get.replicated.com/docker | sudo bash\n\nIf you have additional nodes you will independently need to run the following on each of them.\n\ncurl -sSL https://get.replicated.com/operator | sudo bash\n\nUpgrading Airgap Installs\nAirgap installations can be upgraded by downloading a newer version of the Replicated release, uncompressing it and re-running the install script using the airgap flag.  The latest Replicated release can be found at https://s3.amazonaws.com/replicated-airgap-work/replicated.tar.gz.\n\ntar xzvf replicated.tar.gz\ncat ./install.sh | sudo bash -s airgap\n\nMigrating from Replicated v1 to v2\nReplicated provides a one line migration script to upgrade your v1 installation to v2. The script will first stop your app\nand backup all Replicated data in case there is a need for a restore. To invoke the migration script all you have to do\nis run the script below and follow the prompts.\n\ncurl -sSL https://get.replicated.com/migrate-v2 | sudo bash\n\n{{}}\nTo prevent loss of data, backing up your server is highly recommended before performing a migration.\n{{}}",
        "title": "Upgrading Replicated",
        "description": "The process for end customers to update Replicated services to access the latest improvements to the underlying system since their installation.",
        "weight": "305",
        "categories": [
            "Distributing"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/examples/certificate-chain.md",
        "content": "\nCertificate Chain App\n\nThis app is an example of some of the more advanced features of Replicated.\n\nUsing the cert command in your app's YAML definition, you can transparently and easily generate new TLS certificates for customer use in your app. For example, one of your containers may be serving an API via HTTPS for your other containers to consume. Ideally, the communication between the services running in your containers would be encrypted and authenticated. That's where the cert command becomes useful.\n\nAll certificates generated by the cert command are not self-signed; they are signed by a unique certificate authority (CA) created by the Replicated management container. Template functions are provided for accessing this CA so that your services can add it to their trust stores and perform proper validation of the chain.\n\nThis example app consists of two containers, one which runs a tiny HTTPS server program, and another which runs a similarly-tiny HTTPS client continuously issuing requests to the server. We want the communication between these two containerized services to be both encrypted and authenticated, so we make use of TLS and the Replicated cert command.\n\nThe Go source for these two small programs are provided here for your reference.\n\npackage main\n\nimport (\n  \"log\"\n  \"net/http\"\n)\n\nfunc handler(w http.ResponseWriter, req *http.Request) {\n  w.Header().Set(\"Content-Type\", \"text/plain\")\n  w.Write([]byte(\"This is an example server.\\n\"))\n}\n\nfunc main() {\n  http.HandleFunc(\"/\", handler)\n  err := http.ListenAndServeTLS(\":10443\", \"/opt/certchain.pem\", \"/opt/privatekey.pem\", nil)\n  if err != nil {\n    log.Fatal(err)\n  }\n}\n\npackage main\n\nimport (\n  \"crypto/tls\"\n  \"crypto/x509\"\n  \"io/ioutil\"\n  \"log\"\n  \"net/http\"\n  \"time\"\n)\n\nfunc main() {\n  pem, err := ioutil.ReadFile(\"/opt/ca.pem\")\n  if err != nil {\n    log.Fatalln(err)\n  }\n\n  pool := x509.NewCertPool()\n  if !pool.AppendCertsFromPEM(pem) {\n    log.Fatalln(\"Couldn't import CA\")\n  }\n\n  tr := &http.Transport{\n    TLSClientConfig: &tls.Config{RootCAs: pool},\n  }\n\n  client := &http.Client{Transport: tr}\n\n  for {\n    time.Sleep(time.Second * 5)\n    resp, err := client.Get(\"https://server.replexample.int:10443/\")\n    if err != nil {\n      log.Println(err)\n      continue\n    }\n\n    if resp.StatusCode != 200 {\n      body, _ := ioutil.ReadAll(resp.Body)\n      log.Printf(\"HTTPS request failed. Response follows:\\n  Header: %+v\\n  Body: %s\\n\", resp.Header, string(body))\n    }\n\n    resp.Body.Close()\n  }\n}\n\nAs you can see, the client service accesses the server via a FQDN (server.replexample.int) instead of an IP address. This hostname matches the Common Name field of the certificate generated using the cert command, so validation will succeed.\n\nThis particular hostname exists only inside the client's container. This is enabled by a field in the app's YAML definition called extra_hosts. Any number of hostname & IP address pairings can be defined this way.\n\nAdditionally, both services require access to parts of the certificate chain that has been generated by Replicated: the client needs access to the CA at /opt/ca.pem, and the server needs access to a concatenation of the certificate & the CA at /opt/certchain.pem as well as the certificate's corresponding private key at /opt/privatekey.pem. These files are made available to the services via template functions in the app's YAML definition.\n\nThe app YAML is presented below with comments describing the function of each section.\n\nreplicatedapiversion: \"1.0.0\"\nname: \"CACertExample\"\n\nThis is what kicks off the generation of the certificate.\nNotice the arguments: we want a 2048-bit key, and CN *.replexample.int\nThe output is stored in the \"cert_out\" identifier. There are 3 values\nreturned by this command. By index, these are:\n0: the generated private key\n1: the generated certificate\n2: the certificate authority used to sign the generated certificate\ncmds:\nname: cert_out\n  cmd: cert\n  args:\n  2048\n  \"*.replexample.int\"\n\nHere, in the config section, we write the return values of 'cert_out'\nto a series of separate config options. These are marked 'hidden'\nso that they won't be shown on the settings screen.\nWriting these values into config options allows them to be used in\ntemplate functions (see the \"components\" section below).\nconfig:\nname: HiddenCertValues\n  hidden: true\n  items:\n  type: file\n    name: newcert_privatekey\n    hidden: true\n    data_cmd:\n      name: cert_out\n      value_at: 0\n  type: file\n    name: newcert_cert\n    hidden: true\n    data_cmd:\n      name: cert_out\n      value_at: 1\n  type: file\n    name: newcert_ca\n    hidden: true\n    data_cmd:\n      name: cert_out\n      value_at: 2\n\ncomponents:\nname: ComponentOne\n  containers:\n  source: public\n    imagename: repljoe/cacertexampleserver\n    version: \"1\"\n    ports:\n    port_type: tcp\n      public_port: \"10443\"\n      private_port: \"10443\"\n    config_files:\n\nHere we write the concatenated certificate & CA together to\na file that will be read by the server program. The template\nfunction 'ConfigOptionData' returns the raw data stored in a\nconfig option.\n    filename: /opt/cert_chain.pem\n      contents: |\n        {{repl ConfigOptionData \"newcert_cert\" }}\n        {{repl ConfigOptionData \"newcert_ca\" }}\n      file_mode: \"0600\"\n    customer_files:\n\nThe private key is a simpler case: we can write the contents\nof the config option directly to a file using an entry here in\nthe 'config_files' section.\n    name: newcert_privatekey\n      filename: /opt/private_key.pem\n      file_mode: \"0600\"\nname: ComponentTwo\n  cluster: true\n  tags:\n  client\n  containers:\n  source: public\n    imagename: repljoe/cacertexampleclient\n    version: \"3\"\n\nIn order for the certificate chain to be properly validated,\nthe client needs to connect to the server using a hostname which\nmatches the Common Name field in the certificate it (the server)\nis using.\nIn our 'cert' command at the top of this file, we explicitly\nrequested that the CN be a wildcard: *.replexample.int\nWe use the 'extra_hosts' directive here to define a hostname which\nwill satisfy the certificate validation process.\nThe IP address of the host with the server container is returned\ndynamically by the 'NodePrivateIPAddress' template function.\n    extra_hosts:\n    hostname: server.replexample.int\n      address: '{{repl NodePrivateIPAddress \"ComponentOne\" \"repljoe/cacertexample_server\" }}'\n    customer_files:\n    name: newcert_ca\n      filename: /opt/ca.pem\n      file_mode: \"0600\"\n`",
        "title": "Certificate Chain Example",
        "description": "An advanced walk through for using the Replicated cert command in an app's YAML definition to generate new TLS certificates for customer use in an application.",
        "weight": "403",
        "categories": [
            "Examples"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/examples/counter-app.md",
        "content": "\nCounter App\nThis is an example of a general-purpose Replicated app definition. You can use this app as a template for your own, as it covers the basic functionality of the Replicated platform.\n\nView the example-counter project on Github\n\nMy Counter App version 1.0\n\nreplicatedapiversion: 1.0.0\nname: My Counter App\nversion: \"1.0\"\nrelease_notes: The initial release of my counter application.\nproperties:\n  app_url: http://{{repl ConfigOption \"hostname\" }}\n  logourl: \"\" # TODO: customerfiles\n  console_title: My Counter App Console\nbackup:\n  enabled: false\ncmds:\nname: host_ip\n  cmd: publicip\n  args: []\ncomponents:\nname: DB\n  containers:\n  source: public\n    image_name: redis\n    version: latest\n    cmd: \"[\\\"redis-server\\\", \\\"--appendonly\\\", \\\"yes\\\"]\"\n    publish_events:\n    name: Container redis started\n      trigger: container-start\n      data: \"\"\n      subscriptions:\n      component: App\n        container: freighter/counter\n        action: start\n    config_files: []\n    customer_files: []\n    env_vars: []\n    ports: []\n    volumes:\n    host_path: /data\n      container_path: /data\n    support_files: []\n    restart:\n      policy: on-failure\n      max: 1000\nname: App\n  containers:\n  source: public\n    image_name: nginx\n    version: latest\n    cmd: \"\"\n    publish_events:\n    name: Container nginx started\n      trigger: container-start\n      data: \"\"\n      subscriptions: []\n    config_files:\n    filename: /etc/nginx/conf.d/default.conf\n      contents: |\n        server {\n          listen       80;\n          server_name  localhost;\n\n          location / {\n            proxysetheader X-Real-IP  $remote_addr;\n            proxysetheader X-Forwarded-For $remote_addr;\n            proxysetheader Host $host;\n            proxy_pass http://{{repl NodePrivateIPAddress \"App\" \"freighter/counter\" }}:{{repl ContainerExposedPort \"App\" \"freighter/counter\" \"3000\" }};\n          }\n        }\n    customer_files: []\n    env_vars: []\n    ports:\n    private_port: \"80\"\n      public_port: \"80\"\n      port_type: tcp\n      when: \"\"\n    volumes: []\n    support_files: []\n  source: public\n    image_name: freighter/counter\n    version: \"1.0\"\n    cmd: \"\"\n    publish_events:\n    name: Container freighter/counter started\n      trigger: container-start\n      data: \"\"\n      subscriptions:\n      component: App\n        container: nginx\n        action: start\n    config_files: []\n    customer_files: []\n    env_vars:\n    name: REDIS_HOST\n      static_val: '{{repl NodePrivateIPAddress \"DB\" \"redis\" }}'\n    name: REDIS_PORT\n      static_val: '{{repl ContainerExposedPort \"DB\" \"redis\" \"6379\" }}'\n    ports: []\n    volumes: []\n    support_files: []\n    restart:\n      policy: always\nconfig:\nname: hostname\n  title: Hostname\n  description: Ensure this domain name is routable on your network.\n  items:\n  name: hostname\n    title: Hostname\n    type: text\n    recommended: false\n    default: \"\"\n    value_cmd:\n      name: host_ip\n      value_at: 0\n    when: \"\"\n    affix: \"\"\n    required: true\n    items: []\n`",
        "title": "Counter App Example",
        "description": "An example of a general-purpose Replicated app definition. This can be used as a starting point as it covers the basic functionality of the Replicated platform.",
        "weight": "402",
        "categories": [
            "Examples"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/examples/getelk.md",
        "content": "\nGetELK\nWe've wrapped the ELK stack (Elasticsearch, Logstash and Kibana) in Replicated to be easy to configure, install and update. This is a complex example that uses much of the functionality of the Replicated config YAML.\n\nreplicatedapiversion: \"2.3.5\"\nname: ELK\nconsolesupportmarkdown: |\nemail: support@getelk.com\nproperties:\n  appurl: http{{repl if ConfigOptionEquals \"httpsenabled\" \"1\" }}s{{repl end }}://{{repl ConfigOption \"hostname\" }}\n  console_title: getELK Admin Installer\nstate:\n  ready:\n    command: tcpportaccept\n    args:\n    '{{repl NodePublicIPAddress \"Elasticsearch\" \"getelk/elasticsearch\" }}'\n    \"9200\"\nbackup:\n  enabled: true\n  pause_all: true\nmonitors:\n  cpuacct:\n  Elasticsearch,getelk/elasticsearch\n  memory:\n  Elasticsearch,getelk/elasticsearch\ncomponents:\nname: Elasticsearch\n  containers:\n  source: public\n    image_name: getelk/elasticsearch\n    version: 1.5.0-3\n    publish_events:\n    name: Container getelk/elasticsearch started\n      trigger: container-start\n      subscriptions:\n      component: Logstash & Kibana\n        container: getelk/logstash\n        action: start\n      component: Logstash & Kibana\n        container: getelk/kibana\n        action: start\n    config_files:\n    filename: /elasticsearch/config/elasticsearch.yml\n      source: github\n      owner: getelk\n      repo: elasticsearch\n      path: files/elasticsearch.yml\n      ref: 97c7227ea98c3447540e3462b96da95152d3347d\n    ports:\n    private_port: \"9200\"\n      public_port: \"9200\"\n      port_type: tcp\n    volumes:\n    host_path: /data\n      container_path: /data\n  source: public\n    image_name: getelk/elasticsearch-head\n    version: 0.2.0\n    publish_events:\n    name: Container elasticsearch-head started\n      trigger: container-start\nname: Logstash & Kibana\n  containers:\n  source: public\n    image_name: getelk/logstash\n    version: 1.4.2-7\n    publish_events:\n    name: Container getelk/logstash started\n      trigger: container-start\n    config_files:\n    filename: /opt/conf/logstash.conf\n      source: github\n      owner: getelk\n      repo: logstash\n      path: files/logstash.conf\n      ref: 1679872e02b828f2cac666b36af1738f1a0b2221\n    customer_files:\n    name: logstashinputlumberjackcertfile\n      filename: /opt/certs/logstash-forwarder.crt\n    name: logstashinputlumberjackkeyfile\n      filename: /opt/certs/logstash-forwarder.key\n    env_vars:\n    name: AWSACCESSKEY_ID\n      staticval: '{{repl ConfigOption \"logstashinputsqsawsaccesskey\" }}'\n    name: AWSSECRETACCESS_KEY\n      staticval: '{{repl ConfigOption \"logstashinputsqsawssecretkey\" }}'\n    ports:\n    privateport: '{{repl ConfigOption \"logstashinputcollectdport\" }}'\n      publicport: '{{repl ConfigOption \"logstashinputcollectdport\" }}'\n      port_type: tcp\n      when: '{{repl ConfigOptionEquals \"logstashinputcollectd_enabled\" \"1\" }}'\n    privateport: '{{repl ConfigOption \"logstashinputtcpport\" }}'\n      publicport: '{{repl ConfigOption \"logstashinputtcpport\" }}'\n      port_type: tcp\n      when: '{{repl ConfigOptionEquals \"logstashinputtcp_enabled\" \"1\" }}'\n    privateport: '{{repl ConfigOption \"logstashinputudpport\" }}'\n      publicport: '{{repl ConfigOption \"logstashinputudpport\" }}'\n      port_type: tcp\n      when: '{{repl ConfigOptionEquals \"logstashinputudp_enabled\" \"1\" }}'\n    privateport: '{{repl ConfigOption \"logstashinputsnmpport\" }}'\n      publicport: '{{repl ConfigOption \"logstashinputsnmpport\" }}'\n      port_type: tcp\n      when: '{{repl ConfigOptionEquals \"logstashinputsnmp_enabled\" \"1\" }}'\n    privateport: '{{repl ConfigOption \"logstashinputsyslogport\" }}'\n      publicport: '{{repl ConfigOption \"logstashinputsyslogport\" }}'\n      port_type: tcp\n      when: '{{repl ConfigOptionEquals \"logstashinputsyslog_enabled\" \"1\" }}'\n    private_port: \"25826\"\n      public_port: \"25826\"\n      port_type: tcp\n      when: '{{repl ConfigOptionEquals \"logstashinputcollectd_enabled\" \"1\" }}'\n    privateport: '{{repl ConfigOption \"logstashinputgangliaport\" }}'\n      publicport: '{{repl ConfigOption \"logstashinputgangliaport\" }}'\n      port_type: tcp\n      when: '{{repl ConfigOptionEquals \"logstashinputganglia_enabled\" \"1\" }}'\n    privateport: '{{repl ConfigOption \"logstashinputlumberjackport\" }}'\n      publicport: '{{repl ConfigOption \"logstashinputlumberjackport\" }}'\n      port_type: tcp\n      when: '{{repl ConfigOptionEquals \"logstashinputlumberjack_enabled\" \"1\" }}'\n  source: public\n    image_name: getelk/kibana\n    version: 4.0.1-6\n    publish_events:\n    name: Container getelk/kibana started\n      trigger: container-start\n      subscriptions:\n      component: SSL/Authentication\n        container: getelk/auth\n        action: start\n    config_files:\n    filename: /opt/kibana/config/kibana.yml\n      source: github\n      owner: getelk\n      repo: kibana\n      path: files/kibana.yml\n      ref: af0c9cc784c78d6b4b1a53e4656e612014ae1aa9\nname: SSL/Authentication\n  containers:\n  source: public\n    image_name: nginx\n    version: 1.7.10\n    publish_events:\n    name: Container nginx started\n      trigger: container-start\n    config_files:\n    filename: /etc/nginx/conf.d/default.conf\n      source: github\n      owner: getelk\n      repo: replicated\n      path: files/nginx_default.conf\n      ref: 3bac048801f32001095d9d372688803e24f41cce\n    filename: /etc/nginx/conf.d/elasticsearch_head.conf\n      source: github\n      owner: getelk\n      repo: elasticsearch-head\n      path: files/nginx.conf\n      ref: 46809d3c90c9d6f847634a715e999af99d7fc9e9\n    customer_files:\n    name: sslcertfile\n      filename: /opt/certs/server.crt\n    name: sslkeyfile\n      filename: /opt/certs/server.key\n    ports:\n    private_port: \"80\"\n      public_port: \"80\"\n      port_type: tcp\n      when: '{{repl ConfigOptionEquals \"http_enabled\" \"1\" }}'\n    private_port: \"443\"\n      public_port: \"443\"\n      port_type: tcp\n      when: '{{repl ConfigOptionEquals \"https_enabled\" \"1\" }}'\n    private_port: \"9100\"\n      public_port: \"9100\"\n      port_type: tcp\n  source: public\n    image_name: getelk/auth\n    version: 0.4.0\n    publish_events:\n    name: Container getelk/auth started\n      trigger: container-start\n      subscriptions:\n      component: SSL/Authentication\n        container: nginx\n        action: start\n    config_files:\n    filename: /root/config.yaml\n      source: github\n      owner: getelk\n      repo: replicated\n      path: files/auth_config.yaml\n      ref: 46658db4b3464d37c1aab5dffe7a1aadc24abe7a\n    env_vars:\n    name: REPLICATEDAUTHPASSWORD\n      staticval: '{{repl ConfigOption \"authenticationtypepasswordpassword\" }}'\n    name: REPLICATEDAUTHHASHKEY\n      staticval: '{{repl ConfigOption \"authenticationtypepasswordhashkey\" }}'\n    name: REPLICATEDAUTHBLOCKKEY\n      staticval: '{{repl ConfigOption \"authenticationtypepasswordblockkey\" }}'\ncmds:\nname: hashkey\n  cmd: random\n  args:\n  \"64\"\n  _0-9a-zA-Z-\nname: blockkey\n  cmd: random\n  args:\n  \"32\"\n  _0-9a-zA-Z-\nname: publicip\n  cmd: publicip\n  args: []\nname: ssl_cert\n  cmd: cert\n  args:\n  \"4096\"\nname: lumberjack_cert\n  cmd: cert\n  args:\n  \"4096\"\nname: autogenerated_text\n  cmd: echo\n  args:\n  auto-generated\nconfig:\nname: hostname\n  title: Hostname\n  description: Ensure this domain name is routable on your network.\n  items:\n  name: hostname\n    title: Hostname\n    value_cmd:\n      name: publicip\n      value_at: 0\n    type: text\n    required: true\n    test_proc:\n      display_name: Test Hostname Resolution\n      command: resolve_host\nname: privacy\n  items:\n  name: http_enabled\n    title: HTTP Enabled\n    help_text: When enabled, Kibana will listen for and respond to requests on the HTTP protocol (port 80).\n    default: \"0\"\n    type: bool\n  name: https_enabled\n    title: HTTPS Enabled\n    help_text: A valid x509 SSL certificate and private key files are required to use this option. The certificate and key must be in PEM format. The key must be unencrypted.\n    recommended: true\n    default: \"1\"\n    type: bool\nname: ssl\n  description: \"\"\n  when: https_enabled=1\n  test_proc:\n    display_name: Verify TLS settings\n    command: certificate_verify\n    timeout: 5\n    arg_fields:\n    sslcertfile\n    sslkeyfile\n    hostname\n  items:\n  name: sslkeyfile\n    title: Private Key File\n    value_cmd:\n      name: autogenerated_text\n      value_at: 0\n    data_cmd:\n      name: ssl_cert\n      value_at: 0\n    when: https_enabled=1\n    type: file\n    required: true\n    affix: left\n  name: sslcertfile\n    title: Certificate File\n    value_cmd:\n      name: autogenerated_text\n      value_at: 0\n    data_cmd:\n      name: ssl_cert\n      value_at: 1\n    when: https_enabled=1\n    type: file\n    required: true\n    affix: right\n  name: spacer\n    type: label\n    title: \"\"\nname: authentication\n  title: Authentication\n  description: When enabled, Kibana will prevent anonymous connections and prompt for a users to log in.\n  items:\n  name: authentication_type\n    title: \"\"\n    default: authenticationtypeanonymous\n    type: select_one\n    items:\n    name: authenticationtypeanonymous\n      title: Anonymous\n      type: text\n    name: authenticationtypepassword\n      title: Password\n      recommended: true\n      type: text\n  name: authenticationtypepassword_password\n    title: Password\n    help_text: A shared password which will by used to grant access to Kibana\n    when: authenticationtype=authenticationtype_password\n    type: password\n    required: true\n  name: authenticationsupportemail\n    title: Support Email Address\n    help_text: An email address to display on the login form\n    when: authenticationtype!=authenticationtype_anonymous\n    type: text\n    required: true\n  name: authenticationtypepassword_hashkey\n    title: Hash Key\n    value_cmd:\n      name: hashkey\n      value_at: 0\n    when: authenticationtype=authenticationtype_password\n    type: text\n    hidden: true\n    required: true\n  name: authenticationtypepassword_blockkey\n    title: Block Key\n    value_cmd:\n      name: blockkey\n      value_at: 0\n    when: authenticationtype=authenticationtype_password\n    type: text\n    hidden: true\n    required: true\nname: inputs\n  title: Inputs\n  description: Define inputs for logstash to create\n  test_proc: null\n  items:\n  name: inputs_enabled\n    title: Choose which inputs to enable for Logstash.\n    help_text: Inputs are how data gets into your Logstash system.  You can enable as many or as few as is relevant to your requirements.\n    type: select_many\n    items:\n    name: logstashinputcollectd_enabled\n      title: collectd\n      recommended: true\n      default: \"1\"\n      type: bool\n    name: logstashinputfile_enabled\n      title: file\n      default: \"0\"\n      type: bool\n    name: logstashinputganglia_enabled\n      title: ganglia\n      default: \"0\"\n      type: bool\n    name: logstashinputgenerator_enabled\n      title: generator\n      default: \"0\"\n      type: bool\n    name: logstashinputlog4j_enabled\n      title: log4j\n      default: \"0\"\n      type: bool\n    name: logstashinputlumberjack_enabled\n      title: lumberjack\n      default: \"1\"\n      type: bool\n    name: logstashinputrabbitmq_enabled\n      title: rabbitmq\n      default: \"0\"\n      type: bool\n    name: logstashinputsnmp_enabled\n      title: snmp\n      default: \"0\"\n      type: bool\n    name: logstashinputsqs_enabled\n      title: sqs\n      default: \"0\"\n      type: bool\n    name: logstashinputsyslog_enabled\n      title: syslog\n      recommended: true\n      default: \"1\"\n      type: bool\n    name: logstashinputtcp_enabled\n      title: tcp\n      default: \"0\"\n      type: bool\n    name: logstashinputudp_enabled\n      title: udp\n      default: \"0\"\n      type: bool\n    name: logstashinputunix_enabled\n      title: unix socket\n      default: \"0\"\n      type: bool\n    name: logstashinputwebsocket_enabled\n      title: websocket\n      default: \"0\"\n      type: bool\n    name: logstashinputzeromq_enabled\n      title: zeromq\n      default: \"0\"\n      type: bool\n  name: logstashinputcollectd_port\n    title: Collectd listen port\n    default: \"25826\"\n    when: logstashinputcollectd_enabled=1\n    type: text\n    required: true\n  name: logstashinputganglia_port\n    title: Ganglia listen port\n    default: \"8649\"\n    when: logstashinputganglia_enabled=1\n    type: text\n    required: true\n  name: logstashinputfile_path\n    title: File input path\n    when: logstashinputfile_enabled=1\n    type: text\n    required: true\n  name: logstashinputgenerator_count\n    title: Generator count\n    default: \"1000\"\n    when: logstashinputgenerator_enabled=1\n    type: text\n    required: true\n  name: logstashinputlog4j_port\n    title: Log4j listen port\n    default: \"4560\"\n    when: logstashinputlog4j_enabled=1\n    type: text\n    required: true\n  name: logstashinputlumberjack_port\n    title: Lumberjack port\n    when: logstashinputlumberjack_enabled=1\n    type: text\n    required: true\n  name: logstashinputlumberjackkeyfile\n    title: Lumberjack private key file\n    value_cmd:\n      name: autogenerated_text\n      value_at: 0\n    data_cmd:\n      name: lumberjack_cert\n      value_at: 0\n    when: logstashinputlumberjack_enabled=1\n    type: file\n    affix: left\n    required: true\n  name: logstashinputlumberjackcertfile\n    title: Lumberjack cert file\n    value_cmd:\n      name: autogenerated_text\n      value_at: 0\n    data_cmd:\n      name: lumberjack_cert\n      value_at: 1\n    when: logstashinputlumberjack_enabled=1\n    type: file\n    affix: right\n    required: true\n  name: logstashinputrabbitmq_host\n    title: RabbitMQ host\n    when: logstashinputrabbitmq_enabled=1\n    type: text\n    required: true\n  name: logstashinputrabbitmq_port\n    title: RabbitMQ port\n    default: \"5672\"\n    when: logstashinputrabbitmq_enabled=1\n    type: text\n    required: true\n  name: logstashinputsnmp_port\n    title: SNMP listen port\n    default: \"1062\"\n    when: logstashinputsnmp_enabled=1\n    type: text\n    required: true\n  name: logstashinputsqs_queue\n    title: SQS queue\n    when: logstashinputsqs_enabled=1\n    type: text\n    required: true\n  name: logstashinputsqsawsaccess_key\n    title: SQS AWS access key id and secret access key\n    when: logstashinputsqs_enabled=1\n    type: text\n    required: true\n  name: logstashinputsqsawssecret_key\n    title: SQS AWS secret access key\n    when: logstashinputsqs_enabled=1\n    type: text\n    required: true\n  name: logstashinputsyslog_port\n    title: Syslog listen port\n    default: \"514\"\n    when: logstashinputsyslog_enabled=1\n    type: text\n    required: true\n  name: logstashinputtcp_port\n    title: TCP listen port\n    when: logstashinputtcp_enabled=1\n    type: text\n    required: true\n  name: logstashinputudp_port\n    title: UDP listen port\n    when: logstashinputudp_enabled=1\n    type: text\n    required: true\n  name: logstashinputunix_path\n    title: Path of unix socket\n    when: logstashinputunix_enabled=1\n    type: text\n    required: true\n  name: logstashinputwebsocket_url\n    title: Websocket URL\n    default: 0.0.0.0\n    when: logstashinputwebsocket_enabled=1\n    type: text\n    required: true\n  name: logstashinputzeromq_address\n    title: ZeroMQ address\n    default: tcp://*:2120\n    when: logstashinputzeromq_enabled=1\n    type: text\n    required: true\n  name: logstashinputzeromq_topology\n    title: ZeroMQ topology\n    help_text: one of [\"pushpull\", \"pubsub\", \"pair\"]\n    when: logstashinputzeromq_enabled=1\n    type: text\n    required: true\nname: outputs\n  title: Outputs\n  description: Define additional outputs for Logstash to create.  By default, your data will be sent to your new, local Elasticsearch cluster.  If you want any additional output, select it here.\n  items:\n  name: outputs_enabled\n    title: Choose which outputs to enable for Logstash.\n    help_text: You can enable as many or as few as is relevant to your requirements. Elasticsearch is enabled by default.\n    type: select_many\n    items:\n    name: logstashoutputrollbar_enabled\n      title: rollbar\n      default: \"0\"\n      type: bool\n  name: logstashoutputrollbaraccesstoken\n    title: Rollbar access token\n    when: logstashoutputrollbar_enabled=1\n    type: text\n    required: true\n  name: logstashoutputrollbar_environment\n    title: Rollbar project environment\n    default: production\n    when: logstashoutputrollbar_enabled=1\n    type: text\n    required: true\n  name: logstashoutputrollbar_level\n    title: Rollbar log level\n    default: info\n    when: logstashoutputrollbar_enabled=1\n    type: text\n    required: true\n  name: logstashoutputrollbar_format\n    title: Rollbar format string\n    default: '%{message}'\n    when: logstashoutputrollbar_enabled=1\n    type: text\n    required: true\n`",
        "title": "GetELK",
        "description": "An advanced example of the ELK Stack deployed through Replicated with a complex and complete configuration section.",
        "weight": "404",
        "categories": [
            "Examples"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/examples/index.md",
        "content": "\nA series of sample Replicated YAML files that display how to leverage various features of Replicated.",
        "title": "Replicated Examples",
        "description": "A series of sample Replicated YAML files that display how to leverage various features of Replicated.",
        "weight": "401",
        "type": "section",
        "categories": [
            "Examples"
        ],
        "hideSection": true
    },
    {
        "date": "2017-03-17T00:00:00Z",
        "uri": "/content/docs/examples/kubernetes-guestbook.md",
        "content": "\nGuestbook\nWe've taken the standard Kubernetes Guestbook example application and wrapped it in a Replicated YAML to show you how this would look.\n\nkind: replicated\nreplicatedapiversion: \"2.3.5\"\nversion: \"alpha\"\nname: \"Guestbook\"\nproperties:\n  app_url: '{{repl ServiceAddress \"frontend\" 80 }}'\n  logo_url: http://www.replicated.com/images/logo.png\n  console_title: Guestbook Console\n\nadmin_commands:\nalias: redis-cli\n  command: [redis-cli]\n  run_type: exec\n  selector:\n    app: redis\n    tier: backend\n    role: master\n  container: master # optional, will choose first in pod\n\nconfig:\nname: frontend\n  title: Frontend\n  items:\n  name: frontend_replicas\n    title: App Replicas\n    type: text\n    default: 2\nname: db\n  title: DB\n  items:\n  name: redisslavereplicas\n    title: Redis Slave Replicas\n    type: text\n    default: 2\nname: advanced\n  title: Advanced\n  items:\n  name: redispvstorage_class\n    title: Redis PV Storage Class\n    type: text\n    default: slow\n\nkind: scheduler-kubernetes\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: redis-pvc\n  labels:\n    app: redis\n  annotations:\n    volume.alpha.kubernetes.io/storage-class: {{repl ConfigOption \"redispvstorage_class\" }}\nspec:\n  accessModes:\n  ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n\nkind: scheduler-kubernetes\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\n    tier: backend\n    role: master\nspec:\n  ports:\nthe port that this service should serve on\n  port: 6379\n    targetPort: 6379\n  selector:\n    app: redis\n    tier: backend\n    role: master\n\nkind: scheduler-kubernetes\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: redis-master\nthese labels can be applied automatically\nfrom the labels in the pod template if not set\nlabels:\napp: redis\nrole: master\ntier: backend\nspec:\nthis replicas value is default\nmodify it according to your case\n  replicas: 1\nselector can be applied automatically\nfrom the labels in the pod template if not set\nselector:\nmatchLabels:\napp: guestbook\nrole: master\ntier: backend\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n        tier: backend\n    spec:\n      containers:\n      name: master\n        image: gcr.io/google_containers/redis:e2e  # or just image: redis\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        containerPort: 6379\n        volumeMounts:\n        mountPath: /redis-master-data\n          name: data\n      volumes:\n      name: data\n        persistentVolumeClaim:\n          claimName: redis-pvc\n\nkind: scheduler-kubernetes\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-slave\n  labels:\n    app: redis\n    tier: backend\n    role: slave\nspec:\n  ports:\nthe port that this service should serve on\n  port: 6379\n  selector:\n    app: redis\n    tier: backend\n    role: slave\n\nkind: scheduler-kubernetes\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: redis-slave\nthese labels can be applied automatically\nfrom the labels in the pod template if not set\nlabels:\napp: redis\nrole: slave\ntier: backend\nspec:\nthis replicas value is default\nmodify it according to your case\n  replicas: {{repl ConfigOption \"redisslavereplicas\" }}\nselector can be applied automatically\nfrom the labels in the pod template if not set\nselector:\nmatchLabels:\napp: guestbook\nrole: slave\ntier: backend\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: slave\n        tier: backend\n    spec:\n      containers:\n      name: slave\n        image: gcr.io/google_samples/gb-redisslave:v1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        name: GETHOSTSFROM\n          value: dns\nIf your cluster config does not include a dns service, then to\ninstead access an environment variable to find the master\nservice's host, comment out the 'value: dns' line above, and\nuncomment the line below.\nvalue: env\n        ports:\n        containerPort: 6379\n\nkind: scheduler-kubernetes\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\n  labels:\n    app: guestbook\n    tier: frontend\nspec:\nif your cluster supports it, uncomment the following to automatically create\nan external load-balanced IP for the frontend service.\n  type: LoadBalancer\n  ports:\nthe port that this service should serve on\n  port: 80\n  selector:\n    app: guestbook\n    tier: frontend\n\nkind: scheduler-kubernetes\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: frontend\nthese labels can be applied automatically\nfrom the labels in the pod template if not set\nlabels:\napp: guestbook\ntier: frontend\nspec:\nthis replicas value is default\nmodify it according to your case\n  replicas: {{repl ConfigOption \"frontend_replicas\" }}\nselector can be applied automatically\nfrom the labels in the pod template if not set\nselector:\nmatchLabels:\napp: guestbook\ntier: frontend\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      name: php-redis\n        image: gcr.io/google-samples/gb-frontend:v4\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        name: GETHOSTSFROM\n          value: dns\nIf your cluster config does not include a dns service, then to\ninstead access environment variables to find service host\ninfo, comment out the 'value: dns' line above, and uncomment the\nline below.\nvalue: env\n        ports:\n        containerPort: 80\n`",
        "title": "Kubernetes Guestbook",
        "description": "The Kubernetes Guestbook Application, in a Replicated YAML.",
        "weight": "405",
        "categories": [
            "Examples"
        ]
    },
    {
        "date": "2017-04-11T00:00:00Z",
        "uri": "/content/docs/examples/swarm-votingapp.md",
        "content": "\nVotingApp\nWe've taken the standard Swarm voting example application and wrapped it in a Replicated YAML to show you how this would look.\n\nkind: replicated\n\nreplicatedapiversion: 2.4.2\nname: \"Compose Sample\"\n\nhttps://www.replicated.com/docs/packaging-an-application/application-properties\n\nproperties:\n  app_url: http://{{repl ConfigOption \"hostname\" }}:5000\n  console_title: \"Compose Sample\"\n\nSettings screen\nhttps://www.replicated.com/docs/packaging-an-application/config-screen\n\nconfig:\nname: hostname\n  title: Hostname\n  description: Ensure this domain name is routable on your network.\n  items:\n  name: hostname\n    title: Hostname\n    value: '{{repl ConsoleSetting \"tls.hostname\"}}'\n    type: text\n    test_proc:\n      display_name: Check DNS\n      command: resolve_host\n\nkind: scheduler-swarm\nversion: \"3\"\n\nservices:\n\n  redis:\n    image: redis:3.2-alpine\n    ports:\n      \"6379\"\n    networks:\n      voteapp\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n\n  db:\n    image: postgres:9.4\n    volumes:\n      db-data:/var/lib/postgresql/data\n    networks:\n      voteapp\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n\n  voting-app:\n    image: gaiadocker/example-voting-app-vote:good\n    ports:\n      5000:80\n    networks:\n      voteapp\n    depends_on:\n      redis\n    deploy:\n      mode: replicated\n      replicas: 2\n      labels: [APP=VOTING]\n      placement:\n        constraints: [node.role == worker]\n\n  result-app:\n    image: gaiadocker/example-voting-app-result:latest\n    ports:\n      5001:80\n    networks:\n      voteapp\n    depends_on:\n      db\n\n  worker:\n    image: gaiadocker/example-voting-app-worker:latest\n    networks:\n      voteapp:\n        aliases:\n          workers\n    depends_on:\n      db\n      redis\nservice deployment\n    deploy:\n      mode: replicated\n      replicas: 2\n      labels: [APP=VOTING]\nservice resource management\n      resources:\nHard limit - Docker does not allow to allocate more\n        limits:\n          cpus: '0.25'\n          memory: 512M\nSoft limit - Docker makes best effort to return to it\n        reservations:\n          cpus: '0.25'\n          memory: 256M\nservice restart policy\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n        window: 120s\nservice update configuration\n      update_config:\n        parallelism: 1\n        delay: 10s\n        failure_action: continue\n        monitor: 60s\n        maxfailureratio: 0.3\nplacement constraint - in this case on 'worker' nodes only\n      placement:\n        constraints: [node.role == worker]\n\nnetworks:\n    voteapp:\n\nvolumes:\n  db-data:\n`",
        "title": "Docker Swarm Voting Application",
        "description": "The Docker Swarm Voting Application, in a Replicated YAML.",
        "weight": "406",
        "categories": [
            "Examples"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/getting-started/concepts-and-terms.md",
        "content": "\nBefore shipping your application, there are a few terms to learn, as they are used throughout this guide.\n\nApplication\nAn application (or app) is the software package you are installing onto your customer's servers. It isn't a single binary, rather it's all of the individual components which make your product.\n\nChannel\nChannels are used to stage out releases for customers or customer segments. By default there are Stable, Beta and Unstable channels.\n\nRelease\nA release is a shipped version of the application, complete with release notes & version number.\n\nImage\nAn image is a Docker image that will be used to create a container at runtime.\n\nContainer\nA container is a running instance of an image.",
        "title": "Concepts and Terminology",
        "description": "The core concepts and terms used in these documents to describe the Replicated functionality.",
        "weight": "102",
        "categories": [
            "Getting Started"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/getting-started/index.md",
        "content": "\nReplicated is a platform to deploy containerized SaaS applications behind a firewall (ie private cloud, private\ndata center etc). This guide will walk you through the required steps to start shipping your application using Replicated.\n\nOverview\nThe process to ship your application in Replicated consists of the following steps:\n\nCreate a vendor account on the Replicated Vendor Portal.\nPrepare the images required by your app. You can either:\n    Tag and push your images to the Replicated Private Registry. Or\n    Select images from a public registry ie Docker Hub. Or\n    Push your images to a third party private registry & provide Replicated with access.\nDefine your components on the Replicated Vendor Portal.\nCreate a release of your application.\nInstall your application to test.",
        "title": "Getting Started",
        "description": "A quick overview of how to get started with the Replicated platform.",
        "weight": "101",
        "type": "section",
        "categories": [
            "Getting Started"
        ],
        "hideSection": true
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/getting-started/manage-releases.md",
        "content": "\nCreate Releases\nThe Replicated vendor portal provides you with a location to create and release versions of your application to various release channels.\n\nEditing Releases\nOnce you have created a release you can use the built-in YAML editor to define the release contents. The editor provides various keyboard shortcuts as defined here.\n\nPromoting Releases\nIn order to deploy a release you will need to promote the release to the proper channel(s). More details can be found in our Promote Releases documentation.\n\nManage Releases & Channel\nBy default, there are 3 release channels: Stable, Beta and Unstable. When you first log in to Replicated and select the Channels tab, you'll see these default release channels created:\n\nStable\nFor most of your customers, you will create a license that assigns them to the Stable channel. By doing so, they'll only receive updates when you push a new version to this channel.\n\nBeta\nThe Beta channel is designed to provide a channel to test the upgrade path. You can also choose to license some early-adopting customers against this channel.\n\nUnstable\nThe Unstable channel is designed for you to constantly push releases to, much in the same way that you continuously deploy new versions to your cloud product. This is the channel that your development environment should have a license assigned to. You likely will not deliver any Unstable licenses to your customers.\n\nIn addition to creating additional Release Channels in the Replicated vendor site, you can also use the Vendor API.",
        "title": "Create & Manage Releases",
        "description": "An introduction to the release channel management workflow for development on the Replicated platform.",
        "weight": "103",
        "categories": [
            "Getting Started"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/getting-started/replicated-private-registry.md",
        "content": "\nWhen building your application, you have the option of hosting your private images on the Replicated private registry or using external private and public registries.\n\nTagging Images\n\nThe first thing you will need to do is tag your image. Replicated accepts images in the standard Docker format: registry.replicated.com//:. You can find your application slug on the Images tab of the Replicated Vendor Portal.\n\nAn example of tagging an existing image is:\n\n$ sudo docker tag myapp/worker registry.replicated.com/mycounterapp/worker:1.0.1\n\nLogging In\n\nNext you will need to log into the Replicated private registry with your Vendor account credentials. When prompted, you will use your email address for your username.\n\n$ sudo docker login registry.replicated.com\nUsername: john@replicated.com\nPassword:\nLogin Succeeded\n\nPushing Images\n\nFinally you can push your image to the Replicated private registry.\n\n$ sudo docker push registry.replicated.com/mycounterapp/worker:1.0.1\nThe push refers to a repository [registry.replicated.com/mycounterapp/worker] (len: 1)\nSending image list\nPushing repository registry.replicated.com/mycounterapp/worker (1 tags)\n07595b42e5d5: Image successfully pushed\nf9910c2fd14a: Image successfully pushed\n4f409c5d1046: Image successfully pushed\n8e471642d573: Image successfully pushed\nPushing tag for rev [8e471642d573] on {https://registry.replicated.com/v1/repositories/mycounterapp/worker/tags/1.0.1}\n\nFor additional information on building, tagging and pushing docker images, please refer to the\nDocker CLI Documentation.\n\nDeploying to Kubernetes\n\nWhen deploying an application to a Kubernetes cluster, Replicated will automatically deploy a secret named replicatedregistrykey. This secret can be used as an imagePullSecret to gain read-only access to the images from the on-prem environment.\n\nFor example:\n\n     spec:\n       containers:\n       name: frontend\n         image: registry.replicated.com/guestbook/gb-frontend:v4\n         ...\n       imagePullSecrets:\n       name: replicatedregistrykey\n\nDeploying to Swarm\n\nWhen deploying an application to a swarm cluster, just reference the image in the Replicated registry. Replicated will automatically authenticate with the registry using the customer's license.\n\nFor example:\n\nversion: '3.1'\n\nservices:\n  megaladon:\n    image: registry.replicated.com/guestbook/gb-frontend:v4\n    deploy:\n      replicas: 1\n`",
        "title": "Replicated Private Registry",
        "description": "How to push and access private images in Replicated's hosted private registry.",
        "weight": "105",
        "categories": [
            "Getting Started"
        ]
    },
    {
        "date": "2017-04-11T00:00:00Z",
        "uri": "/content/docs/getting-started/schedulers.md",
        "content": "\nYour application is deployed in Docker containers, and likely needs a scheduler to orchestrate and manage the runtime of your containers. Replicated offers several options when integrating, and it's important to understand how each scheduler works with Replicated. Simply stated, the scheduler you choose is the technology you'll have to learn to orchestrate and start your containers. But there are some small differences in how each is run in a customer environment.\n\nReplicated Scheduler\nThe Replicated scheduler is a mature container orchestration runtime that supports Docker 1.7.1 and newer. It's actively deployed in many large enterprises and supports all of the features of Replicated. If supporting Docker versions older than 1.13.1 or supporting a one-line installation option is important, the Replicated scheduler is the best choice.\n\nDocker Swarm\nThe Docker Swarm scheduler is a great choice if you have existing docker-compose yaml and want to target servers capable of running Docker 1.13.1 or newer. Using the Swarm scheduler, you can use all of the Swarm functionality including overlay networks, DNS service discovery, Docker secrets and more.\n\nKubernetes\nKubernetes is a powerful and popular container orchestration and runtime that's quickly gaining popularity. Replicated supports deploying Kubernetes resources to a Kubernetes cluster, if your customer supplies and manages a cluster. If you have existing Kubernetes specs written and your customer is able to supply a Kubernetes cluster to run your application, this is a good choice to use with Replicated.",
        "title": "Choosing A Scheduler",
        "description": "Schedulers",
        "weight": "104",
        "categories": [
            "Getting Started"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/automate-install.md",
        "content": "\nReplicated has support for automated installation and configuration to facilitate integration\ntesting. This feature is built to configure a new installation and prepare it to run a test\nsuite and not to managing existing installations.\n\nThese steps must be run before installing Replicated. The Replicated daemon only checks for these\nvalues during startup, and only the first time it is started.\n\nConfigure Replicated Automatically\n\nDuring installation, Replicated will create a config file in /etc/replicated.conf. If this file\nexists, the installer will not overwrite it. To configure Replicated, create this file using any\nor all of the following options:\n\n{\n  \"DaemonAuthenticationType\": \"anonymous\",\n  \"TlsBootstrapType\": \"server-path\",\n  \"TlsBootstrapHostname\": \"server.company.com\",\n  \"TlsBootstrapCert\": \"/etc/server.crt\",\n  \"TlsBootstrapKey\": \"/etc/server.key\",\n  \"LogLevel\": \"debug\",\n  \"Channel\": \"stable\",\n  \"LicenseFileLocation\": \"/tmp/license.rli\",\n  \"ImportSettingsFrom\": \"/tmp/settings.conf\",\n  \"BypassPreflightChecks\": true\n}\n\nThese settings are explained in the following table:\n\n| Setting | Acceptable Values | Description |\n|---------|-------------------|-------------|\n| DaemonAuthenticationType | anonymous or password | For test automation Replicated supports anonymous and password protected access. |\n| DaemonAuthenticationPassword | Any string | If DaemonAuthenticationType is set to password this value is required to access the Replicated console. |\n| DaemonToken | Any string | Authentication token used by operators for automating a cluster installation |\n| TlsBootstrapType | ['server-path', 'self-signed'] | The type of TLS cert the Replicated UI will run with. Use self-signed for a fully automated setup, use server-path to provide a static cert and key to bootstrap the console with. |\n| TlsBootstrapHostname | Any string | The hostname to use for the Replicated-UI :8800 console |\n| TlsBootstrapCert | A file location as a string | If TlsBootstrapType is set to server-path, this value should be present and set to the location of a PEM encoded certificate file. |\n| TlsBootstrapKey | A file location as a string | If TlsBootstrapType is set to server-path, this value should be present and set to the location of a PEM encoded key file. |\n| LogLevel | ['debug', 'info, 'error'] | If present, this will set the log level of the Replicated daemon. |\n| Channel | ['stable', 'beta'] | The release channel you are using to install Replicated from. This is the Replicated channel, not the app channel. |\n| LicenseFileLocation | A file location as a string | This should be set to the location of an installable .rli license file. Note that you should not enable activation on this license. |\n| ImportSettingsFrom | A file location as a string | If your application has any required config settings, you can supply custom values here. Replicated will read these and set them as if the user manually configured it. (see below) |\n| BypassPreflightChecks\t | Boolean true or false | Skips preflight checks |\n\nConfigure App Settings Automatically\n\nWhen your application has a Settings page, you can configure required options automatically by setting\nthe ImportSettingsFrom field in /etc/replicated.conf. This file is in JSON format and uses the\nconfig_item names from the YAML as keys. For example, if your YAML contained the following config section:\n\nconfig:\nname: general\n  title: General Settings\n  items:\n  name: hostname\n    title: The hostname of this server.\n    type: text\n    required: true\n  name: port_number\n    title: The port to listen on.\n    type: text\n    required: true\n  name: a_file\n    title: Pick a file.\n    type: file\n  name: a_boolean\n    title: Yea or Nay\n    type: bool\n  name: manyafiles\n    title: Pick many files.\n    multiple: true\n\nYou could then create the settings.conf in this format:\n\n{\n \"hostname\": {\n   \"value\": \"http://app.domain.com\"\n },\n \"port_number\": {\n   \"value\": \"1123123\"\n },\n \"a_file\": {\n   \"value\": \"/some/fake/filepath\",\n   \"data\": \"\"\n },\n \"a_boolean\": {\n   \"value\": \"true\"\n },\n \"manyafiles\": {\n   \"multi_value\": [\n     \"/some/fake/file1\",\n     \"/some/fake/file2\"\n   ],\n   \"multi_data\": [\n     \"\",\n     \"\"\n   ]\n }\n}\n",
        "lastmod": "2016-07-03T00:00:00Z",
        "title": "Automate Install for Testing",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/branding.md",
        "content": "\nWith the release of Replicated 1.2.73 you can now customize the look & feel of the\ndashboard of your on-prem customer experience.\n\nTo get started visit the vendor portal app settings page.\n\nBranding is staged by channel, so that you can test your branding before shipping it to\ncustomers. If you remove the branding & leave a comment in the CSS\n/* show minimal */ you’ll get a minimalist version of Replicated UI:\n\nBy default, existing vendors will have the traditional purple, organge, teal colors set as\ntheir default CSS for all channels. Existing client installations will fallback to the\ntraditional colors if no CSS is available.\n\nUpdates to the CSS are applied at installation (new install), when a customer updates to a\nReplicated brandable version (1.2.73 or newer) and then anytime an app update is applied.\nCSS updates are not applied during a standard Replicated daemon update or during a\nlicense sync.\n\nCurrently the supported items are labeled below:\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Branding Your Configuration Screen",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/certs-and-keys.md",
        "content": "\nMost customers will set up a subdomain & DNS for their instance of your application. During\nsetup you can allow them to identify their hostname and provide custom SSL certs for their\ninstance (as shown below). The YAML below will allow the user to determine if they’d like\nto proceed with the SSL certs provided during the initial setup of the management console,\nprovide their own, or use self-signed certs that are generated with the cmd in the cmds\nsection. The YAML also writes the active file to the nginx container as a customer file.\nfor these fields (and ‘test’ buttons for testing\nhostname resolution\n& cert verification\nis also provided below.\n\nThe meta-data about your application\nreplicatedapiversion: 1.3.0\nproperties:\n  app_url: http://{{repl ConfigOption \"hostname\" }}\n  console_title: Flask App\n\nSetting's screen markup\nconfig:\nname: hostname\n  title: Hostname\n  description: Ensure this domain name is routable on your network.\n  items:\n  name: hostname\n    title: Hostname\n    value: '{{repl ConsoleSetting \"tls.hostname\"}}'\n    type: text\n    test_proc:\n      display_name: Check DNS\n      command: resolve_host\n      arg_fields: []\n  name: mgmt_certs\n    type: bool\n    title: Use same management console TLS cert/key\n    default: \"1\"\nname: override_certs\n  when: mgmt_certs=0\n  test_proc:\n    display_name: Verify TLS settings\n    command: certificate_verify\n    timeout: 5\n    arg_fields:\n    sslcertfile\n    sslkeyfile\n    hostname\n  items:\n  name: sslkeyfile\n    title: SSL Private Key\n    when: mgmt_certs=0\n    value: Use self-signed cert\n    data_cmd:\n      name: ssl_cert\n      value_at: 0\n    type: file\n    affix: left\n  name: sslcertfile\n    title: SSL Certificate\n    when: mgmt_certs=0\n    value: Use self-signed cert\n    data_cmd:\n      name: ssl_cert\n      value_at: 1\n    type: file\n    affix: right\n\nThis section uses a flask image from google and nginx image to create a hello world website.\n\ncomponents:\nname: webserver\n  containers:\n  source: public\n    image_name: google/python-hello\n    version: latest\n    ports:\n    private_port: \"8080\"\n      public_port: \"8080\"\n      port_type: tcp\nname: loadbalancer\n  containers:\n  source: public\n    image_name: nginx\n    version: latest\n    ports:\n    private_port: \"443\"\n      public_port: \"443\"\n      port_type: tcp\n    private_port: \"80\"\n      public_port: \"80\"\n      port_type: tcp\n    config_files:\n    filename: /opt/certs/server.key\n      contents: '{{repl if ConfigOptionEquals \"mgmtcerts\" \"0\" }}{{repl ConfigOptionData \"sslkey_file\"}}{{repl else}}{{repl ConsoleSetting \"tls.key.data\"}}{{repl end}}'\n    filename: /opt/certs/server.cert\n      contents: '{{repl if ConfigOptionEquals \"mgmtcerts\" \"0\" }}{{repl ConfigOptionData \"sslcert_file\"}}{{repl else}}{{repl ConsoleSetting \"tls.cert.data\"}}{{repl end}}'\n    filename: /etc/nginx/conf.d/default.conf\n      contents: |\n        upstream web-server {\nReplace this line with your webserver\n          server {{repl NodePrivateIPAddress \"webserver\" \"google/python-hello\"}}:8080 fail_timeout=0;\n        }\n\n        server {\n          listen 0.0.0.0:80;\n          return 301 https://$host$request_uri;\n        }\n\n        server {\n          listen 0.0.0.0:443;\n          server_name {{repl ConfigOption \"hostname\" }};\n\n          ssl on;\n          ssl_certificate /opt/certs/server.cert;\n          sslcertificatekey /opt/certs/server.key;\n\n          location / {\n            proxysetheader        Host $host;\n            proxysetheader        X-Real-IP $remote_addr;\n            proxysetheader        X-Forwarded-For $proxyaddxforwardedfor;\n            proxysetheader        X-Forwarded-Proto $scheme;\n            proxy_pass              http://web-server;\n          }\n        }\n\nThis cmd is used to generate the cert & key.\n\ncmds:\nname: ssl_cert\n  cmd: cert\n  args:\n  \"2048\"\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Custom Supplied TLS Certs and Keys",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/config-screen-overview.md",
        "content": "\nOne of the core features of Replicated is the ability for any application to present users\nwith a configuration/settings page on which they can input values to be used to customize\ntheir instance. Generally, these settings include things like hostname, SSL certs, SMTP\nsettings. Technical documentation config YAML.\n\nPreview with YAML\n\nThis screen can be previewed in the vendor portal while viewing the YAML each release. You\ncan also use the Replicated Atom Preview Plugin.\n\nReconfigure anytime.\n\nSaved anytime to restart with new values.\n\nConditional Inputs\n\nConditional inputs allow you to determine when an item or group is shown.\n(technical documentation)\n\nTest Commands\n\nUsers to validate information like SMTP Auth, SSL Certs, Hostname resolution, GitHub Auth Creds,\nand AWS Creds with the use of Replicated test commands.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Configuration Screen Overview",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/create-promote-release.md",
        "content": "\nOne common use case of the Vendor API is to connect it into your CI/CD workflow and create new unstable releases\non Replicated whenever a new build is run. This is especially helpful when you pair it with the ability to\nscript the installation of Replicated for automated testing.\nThis article will show you how to use the Vendor API to automatically create a new release and promote it to\nthe unstable channel.\n\nPrerequisites\n\nYou should already have an API Token from the vendor site, and you should know the target App ID and Channel ID.\nFor details on how to get the App ID and Channel ID programmatically, check out the\nVendor API documentation. These values will not change and should be supplied as static\nvalues to your deployment script.\n\nNext, you should have the YML that you want to promote. One common method to generate this is to store the YML with\ntemplate values indicating the image tags to use. When your build server creates a new Docker image, it’s easy to\ngenerate a new YAML programmatically.\n\nCreate a new release\n\nFirst, we will create a new release with the current YAML. This release will not be\nconnected to your Channel ID at all; we are simply creating a release in your app that can be promoted later. T\nhis can be done with a single POST:\n\ncurl -X POST \\\n     -H 'Authorization: ' \\\n     -H 'Content-Type: application/json' \\\n     -d '{\"source\":\"latest\"}' \\\n     https://api.replicated.com/vendor/v1/app//release\n\nNote, in the result, you will receive a JSON object that contains a key named Sequence. Save this value. This is\nthe unique ID for the release you are creating. It will be required in the next steps.\n\nUpdate the release\n\nNext, we want to put our new YML into this release. This is accomplished with a single PUT:\n\ncurl -X PUT \\\n     -H 'Authorization: ' \\\n     -H 'Content-Type: application/yaml' \\\n     -d  \\\n     https://api.replicated.com/vendor/v1/app///raw\n\nThere will be a 204 No Content status code returned if this is successful. If there are parsing or validation errors\nin the YML, a detailed message will be returned.\n\nPromote the release\n\nFinally, we want to promote this release to the channel. This will make it immediately\navailable to any installation of a license from that channel. We recommend doing this for the Unstable or dev/test\nchannels only at this time. Promoting the release is a single POST:\n\ncurl -X POST \\\n     -H 'Authorization: ' \\\n     -H 'Content-Type: application/json' \\\n     -d '{\"channels\":[\"\"],\\\n          \"release_notes\":\"This is an auto generated release\",\\\n          \"label\":\"AUTO\",\"required\":false}' \\\n     https://api.replicated.com/vendor/v1/app///promote\nThis release will be generated with static release notes and version label, but these fields can be edited manually\nat any time, including when you promote to the beta channel.\n\nTest Your App YAML Changes\n\nWhen creating new releases during an automated process we recommend testing your app YAML first. This will allow you to validate, and if there are errors during validation exit your build process prior to creating a new release sequence.\n\nTo validate your app YAML we use the same call as updating but with a dry_run flag. This will allow you find any errors without updating your application. On success the response will be HTTP 200 OK. If there is a problem with your app YAML the service will return a HTTP 400 response with a JSON payload indicating the error. A HTTP 403 response indicates that the sequence you're trying to update has already been promoted in which case you would create a new release and retry the dry run.\n\ncurl -X PUT \\\n     -H 'Authorization: ' \\\n     -H 'Content-Type: application/yaml' \\\n     -d  \\\n     https://api.replicated.com/vendor/v1/app///raw?dry_run=1\n\nNext Steps\n\nOnce you have this integrated into your CI/CD process, the next step is to set up\nautomated installation for testing and you will be close to\na fully automated on-prem deployment process.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Create and Promote a Release",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-12-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/custom-license-fields.md",
        "content": "\nCustom license fields are key value pairs that can be created in the vendor portal, securely delivered to each on-prem instance and kept in sync through automated update checks. This is useful if specific application level information might change from customer to customer.\n\nExamples of custom license fields are \"seats\" to limit the number of active users or \"hostname\" in order to specify the domain that the application can be run on.\n\nCreating Custom License Fields\nCustom license fields are generally created and managed from the License Fields section of the vendor portal. Any new Required field will need to be set for all existing Customers (either with custom data or by using the Default value) as well as all future Customers. Fields that are marked as Hidden will not be displayed on the :8800/license page of the on-prem admin console. Title will be the name that is displayed on the :8800/license page of the on-prem console. Field is the unique id. Type is either an 'integer', 'string' or 'text' type.\n\nSetting Custom License Fields\nThe values can then be set for each Customer in the manage customer screen of the vendor portal in the same way that standard customer fields are managed.\n\nAccessing the Field Values\nThese values are available in the on-prem instance and can be used to configure or alter the application. During configuration, these fields can be read from the template functions and then used to overwrite config files or be injected as environment variables. Once the application is running, these values are synced automatically on the update check interval. Changes can be detected in the running instance by polling the Licensing Integration API.\n\nThese fields are cryptographically signed by Replicated using PKI and the Replicated components will not install or update the license fields if the data does not match the signature.\n",
        "lastmod": "2016-12-01T00:00:00Z",
        "title": "Manage Custom License Fields",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/default-or-value.md",
        "content": "\nCommands are an important part of the\nReplicated ecosystem for bootstrapping a configuration screen to help streamline the\ninstallation of your application on-premise.\n\nImportant concepts to understand when using cmds:\n\ncmds ONLY RUNS AT YAML IMPORT TIME (during app installation & during app updates).\ncmds are computed, meaning they can not be templated with the {{repl }} template functions.\ncmds need to be “defined” in your yaml to be used by configuration options.\ncmds are called in a configuration option in one of 3 ways:\n  value_cmd\n  default_cmd\n  data_cmd\nIf called via valuecmd or datacmd the initial value/data will persist through updates, restarts etc (just as it would be if the customer input that value into the settings screen themselves).\nIf called from default_cmd this value will be recalculated everytime the app is updated or installed.\n\nExample App:\n\nI want a user to be able to specify a hostname but would like it to default to its public ip address.\nI want to generate a default random admin password for my application.\n\nStep 1: Define my cmds.\n\ncmds:\nname: host_ip\n  cmd: getmypublicipaddress\n  args: []\nname: generaterandompassword32char\n  cmd: random\n  args:\n  \"32\"\n\nStep 2: Call my cmds from config section.\n\nconfig:\nname: App\n  title: App Configuration\n  description: Set default values for my App to Use\n  items:\n  name: hostname\n    title: Hostname\n    type: text\n    value_cmd:\n      name: getmypublicipaddress\n      value_at: 0\n    required: true\n  name: admin_pw\n    title: Admin User Password\n    type: text\n    value_cmd:\n      name: generaterandompassword32char\n      value_at: 0\n    required: true\n\nDownload Full Replicated YAML Example.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Using cmd To Populate Values & Defaults",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2017-03-17T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/devicemapper-warning.md",
        "content": "\nRunning devicemapper in loopback mode is discouraged for production. It has known performance problems and a different storage driver should be used.  See devicemapper performance considerations and selecting a storage driver to understand the available storage drivers and limitations.\n\n| Linux Kernel Version | Docker Version Constraints | Recommended Storage Driver |\n|----------------------|----------------|----------------------------|\n|  install\ncat install | sudo bash -s bypass-storagedriver-warnings\n",
        "lastmod": "2017-03-17T00:00:00Z",
        "title": "Devicemapper Warning",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-10-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/docker-file-copy.md",
        "content": "\nOne of the challenges of running containerized application is limited access to host OS information and resources.  This article intends to demonstrate one way to access files on the host OS that are not available to an application running inside of a container.\n\nThis article shows how to read /etc/hosts file from the host OS.   This approach can be used to read entire directories.\n\nMethod Summary\n\nOne way to read a file from inside a container is to start another container with the file mounted and then use the Docker copy function.  There are two versions\n\narchive - added in docker 1.8 (API v1.20)\ncopy - removed in docker 1.12 (API v1.24)\n\nThis example will use the archive function.\n\nContainer Requirements\n\nCommunicating with Docker daemon\n\nIn order for application to be able to issue docker commands from inside a container, the container will need to have the docker socket file mounted. Custom configurations can run Docker listening on a TCP socket. In those rare cases this mount is not necessary. However the host IP address needs to be routable from inside the container.\n\nThis example will use the socket file.  So the basic run command that will launch the initial application container will look like this:\n\ndocker run \\\n  -v /var/run/docker.sock:/host/var/run/docker.sock \\\n  dockercp\n\nStarting the secondary container\n\ndockercp is the image name that will be used in the example.  However, the secondary container started from the main application container can be based on any public or private image.\n\nExample Source Code\n\npackage main\n\nimport (\n\t\"archive/tar\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"github.com/fsouza/go-dockerclient\"\n)\n\nfunc mountedPath(path string) string {\n\treturn filepath.Join(\"/host\", path)\n}\n\nfunc main() {\n\tdockerCli, err := docker.NewClient(\"unix:///host/var/run/docker.sock\")\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfileToDownload := \"/etc/hosts\"\n\ttarStream := getTarStream(dockerCli, fileToDownload)\n\tdefer tarStream.Close()\n\n\tfmt.Println(\"Downloading\", fileToDownload, \"as tar\")\n\n\ttarReader := tar.NewReader(tarStream)\n\tfor {\n\t\theader, err := tarReader.Next()\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\n\t\tfileInfo := header.FileInfo()\n\t\t// Should also check if file is a link in real life\n\t\tif fileInfo.IsDir() {\n\t\t\tfmt.Println(\"Got dir\", header.Name)\n\t\t\tcontinue\n\t\t}\n\n\t\tfmt.Println(\"Got file\", header.Name, \"with size\", header.Size)\n\t\tfmt.Println(\"=========Contents=========\")\n\t\tif _, err = io.CopyN(os.Stdout, tarReader, header.Size); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tfmt.Println(\"==========================\")\n\t}\n\tfmt.Println(\"Download complete\")\n}\n\nfunc getTarStream(cli *docker.Client, filename string) io.ReadCloser {\n\tcontainer := createContainer(cli, filename)\n\tfmt.Println(\"Created container\", container.ID)\n\tdefer removeContainer(cli, container)\n\n\tstartContainer(cli, container)\n\tfmt.Println(\"Started container\")\n\n\tpreader, pwriter := io.Pipe()\n\topts := docker.DownloadFromContainerOptions{\n\t\tPath:         mountedPath(filename),\n\t\tOutputStream: pwriter,\n\t}\n\n\t// Let docker asynchronously write into the pipe while we are reading it on the other end\n\tgo func() {\n\t\tdefer pwriter.Close()\n\t\tfmt.Println(\"Requesting file\", opts.Path)\n\t\tif err := cli.DownloadFromContainer(container.ID, opts); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}()\n\n\treturn preader\n}\n\nfunc createContainer(cli docker.Client, filename string) docker.Container {\n\tcreateOpts := docker.CreateContainerOptions{\n\t\tConfig: &docker.Config{\n\t\t\t// any image that has the command specified below can be used\n\t\t\tImage: \"dockercp\",\n\t\t\t// \"cat\" with no arguments will simply block indefinitely ensuring that the container does not terminate.\n\t\t\tCmd:        []string{\"cat\"},\n\t\t\tEntrypoint: []string{},\n\t\t\tOpenStdin:  true,\n\t\t},\n\t\tHostConfig: &docker.HostConfig{\n\t\t\tBinds: []string{fmt.Sprintf(\"%s:%s\", filename, mountedPath(filename))},\n\t\t},\n\t}\n\n\tcontainer, err := cli.CreateContainer(createOpts)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn container\n}\n\nfunc startContainer(cli docker.Client, container docker.Container) {\n\tif err := cli.StartContainer(container.ID, nil); err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc removeContainer(cli docker.Client, container docker.Container) {\n\tremoveOpts := docker.RemoveContainerOptions{\n\t\tID:            container.ID,\n\t\tRemoveVolumes: true,\n\t\tForce:         true,\n\t}\n\n\terr := cli.RemoveContainer(removeOpts)\n\tif err != nil {\n\t\tif _, ok := err.(*docker.NoSuchContainer); ok {\n\t\t\treturn\n\t\t}\n\t\tpanic(err)\n\t}\n}\n\nRunnable project is available in this public repository.\n",
        "lastmod": "2016-10-01T00:00:00Z",
        "title": "Docker File Copy",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-10-05T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/docker-iptables.md",
        "content": "\nWhen Docker starts it registers a DOCKER chain into iptables to allow communication between ports exposed on the\ncontainers it manages.\n\nThe iptables chain can be verified by running iptables -L (Replicated 2.0 install shown here)\n\niptables -L\n...\nChain DOCKER (1 references)\ntarget     prot opt source               destination\nACCEPT     tcp  --  anywhere             172.17.0.2           tcp dpt:9879\nACCEPT     tcp  --  anywhere             172.17.0.2           tcp dpt:9878\nACCEPT     tcp  --  anywhere             172.17.0.2           tcp dpt:9877\nACCEPT     tcp  --  anywhere             172.17.0.2           tcp dpt:9876\nACCEPT     tcp  --  anywhere             172.17.0.2           tcp dpt:9875\nACCEPT     tcp  --  anywhere             172.17.0.2           tcp dpt:9874\nACCEPT     tcp  --  anywhere             172.17.0.3           tcp dpt:8800\n\nChain DOCKER-ISOLATION (1 references)\ntarget     prot opt source               destination\nRETURN     all  --  anywhere             anywhere\n\nDuring development if you delete the DOCKER chain or the iptables rules get dropped (e.g. by a restart of\nfirewalld) Docker will start logging iptables errors such as \"failed programming external connectivity ...\niptables: No chain/target/match by that name\".\n\nTo resolve simply restart Docker and the correct iptables rules will be created.\n",
        "lastmod": "2016-10-05T00:00:00Z",
        "title": "Docker IPTables and the No-Chain Error",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-22T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/docker-links.md",
        "content": "\nIn the earlier releases of Docker, there was a concept of container linking. This has been deprecated in recent versions of Docker, but some Docker Compose YAMLs may still reference them. If you are dependent on the legacy Docker links to deploy your application, it's possible to use the extra_hosts option in Replicated to simulate this and deploy your containers without any changes.\n\nFor example, let's take a basic Docker compose snippet that has an API container and Redis container, and is using links to connect the two. The docker-compose might look like:\n\nredis:\n  image: redis:latest\napi:\n  image: 'mycompany/api:latest'\n  links:\n    redis\n\nConverting this to Replicated YAML:\n\ncomponents:\n  name: Redis\n    containers:\n      source: public\n        image_name: redis\n        version: \"latest\"\n        ports:\n          private_port: \"6379\"\n            public_port: \"6379\"\n            interface: \"docker0\"\n        publish_events:\n          name: Redis started\n            trigger: container-start\n            subscriptions:\n              component: API\n                container: mycompany/api\n                action: start\n\n  name: API\n    containers:\n      source: dockerhub\n        image_name: mycompany/api\n        extra_hosts:\n          hostname: redis\n            address: '{{repl ThisNodeDockerAddress }}'\n",
        "lastmod": "2016-07-22T00:00:00Z",
        "title": "Docker Links",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-08-05T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/domains-required-by-replicated.md",
        "content": "\nBelow is a list of domains that Replicated On-Prem will communicate with in non-airgap mode. All connections are made over TLS.\n\nget.replicated.com\napi.replicated.com\nregistry.replicated.com\nquay.io\nget.docker.com\n.docker.io, .docker.com 1\n\n[1] index.docker.io is the main URL for the Docker registry, however, we cannot provide an exact list of URLs as it has not been published by Docker.\n\nThe following is a link to a repository hosting the IPs for all domains that Replicated controls.\nhttps://github.com/replicatedhq/ips\n",
        "lastmod": "2016-08-05T00:00:00Z",
        "title": "Domains Used by Replicated",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/ephemeral-containers.md",
        "content": "\nDatabase migrations are an important part of any software upgrade, and having a strategy to manage\ndata migrations in enterprise deployments is important. One common pattern to manage database migrations\nis to run an ephemeral container\nalong with container events\nto run migration tasks.\n\nIn this example, I will take a very simply python stack based on the Django framework and postgres, and show how to run database migrations. No knowledge of python is necessary to understand this.\n\nDatabase Component\n\nLet's begin by creating a container in the Replicated based on the public postgres Docker image from\nDockerhub. This snippet will expose Postgre on port 5432 on the host, and define an event that\ninstructs Replicated to poll port 5432 on the host until a connection can be established. We are\ngoing to use this event as a trigger to run the migration.\n\nname: db\n  containers:\n  source: public\n    image_name: postgres\n    version: 9.5\n    env_vars:\n    name: POSTGRES_DB\n      static_val: pythonapp\n    name: POSTGRES_USER\n      staticval: 'someuser'\n    name: POSTGRES_PASSWORD\n      staticval: 'somepassword'\n    volumes:\n    host_path: /data/postgresql/data\n      container_path: /var/lib/postgresql/data\n    ports:\n    private_port: \"5432\"\n      public_port: \"5432\"\n      port_type: tcp\n    publish_events:\n    name: Postgres started and waiting for connections\n      trigger: port-listen\n      data: \"5432\"\n      subscriptions:\n      component: db-migration\n        container: pythonapp\n        action: start\n\nMigration Component\n\nNext, we will run our \"app\" component, but overriding the CMD in with python manage.py db upgrade,\nwhich is how a migration is performed in a Django app. This should be replaced with rake db:migrate\nor anything appropriate for your stack.\n\nWe define this container with ephemeral: true to indicate that this container is expected to exit.\nWithout this flag, Replicated will detect that the container exited, and will flag the entire app\nas \"Stopped\" in the dashboard.\n\nFinally, we listen for this container to stop, and fire an event to start the app container.\n\nname: db-migration\n  containers:\n  source: replicated\n    image_name: pythonapp\n    version: 1.4.2\n    ephemeral: true\n    env_vars:\n      name: DB_URL\n        staticval: \"postgresql://pythonapp:{{repl ConfigOption \\\"postgrespw\\\"}}@{{repl NodePrivateIPAddress \\\"db\\\" \\\"postgres\\\" }}:5432/pythonapp\"\n    cmd: '[\"python\", \"manage.py\", \"db\", \"upgrade\"]'\n    publish_events:\n    name: db migration complete\n      trigger: container-stop\n      subscriptions:\n      component: python-app\n        container: pythonapp\n        action: start\n\nApp Component\n\nThe app component is a standard Django app. We assume that it can start normally using a built in\nCMD or ENTRYPOINT in the Dockerfile. We aren't overriding the CMD this time, so the container\nwill start serving the web site.\n\nname: python-app\n  containers:\n  source: replicated\n    image_name: pythonapp\n    version: 1.4.2\n    env_vars:\n      name: DB_URL\n        staticval: \"postgresql://pythonapp:{{repl ConfigOption \\\"postgrespw\\\"}}@{{repl NodePrivateIPAddress \\\"db\\\" \\\"postgres\\\" }}:5432/pythonapp\"\n\nIf this release contains a required migration and the migration is not going to be present in future\nreleases, you can mark this release as required\nwhen promoting it.\n\nFor more on sequencing the startup of your application take a look at this article.\n\nDownload entire Replicated YML.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Migrating Your Application With Ephemeral Containers",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2017-03-13T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/finding-your-api-token-and-app-id.md",
        "content": "\nIn order to utilize many of the developer API endpoints you will need to know your API Token as well as other items such as your App ID or Channel ID. This document will provide examples of how those values can be located using the cURL utility.\n\nAPI Token\nYou can create and show your API Token inside your Vendor Web portal by clicking on the Teams & Tokens link.\n\nApp ID\nTo list your apps:\n\ncurl -X GET \\\n    -H 'Authorization: ' \\\n    https://api.replicated.com/vendor/v1/apps \\\n    | python -m json.tool\n\nYou will receive a json response that includes the \"App:Id\" for each of your Apps along with the \"Channel:Id\" for each release channel.\n\nChannel ID\nTo look up a Channel ID for a specific App:\n\ncurl -X GET \\\n    -H 'Authorization: ' \\\n    https://api.replicated.com/vendor/v1/app//channels \\\n    | python -m json.tool\n",
        "lastmod": "2017-03-13T00:00:00Z",
        "title": "Finding Your API Token and App ID",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/generate-api-token.md",
        "content": "\nTo interact with the vendor API\n(anything that is available in the vendor web portal is available in the API) you’ll need to create API tokens.  API tokens identify your team and depending on your needs the API token can be read or read/write.  Using the API you can automate most of your development and license issuing workflows.\n\nAfter you have created your API token use it in the Authorization header for vendor API calls.\n\ncurl --header \"Authorization: API-TOKEN\" https://api.replicated.com/vendor/v2/licenses\n\nTo create an API token you use your http://vendor.replicated.com/team page.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Generate an API Token",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/github-integration-config-files.md",
        "content": "\nYou can connect to GitHub to store large template files such as config files. This\ncan be done from https://vendor.replicated.com/settings\n\nFor detailed technical documentation on how to\nreference GitHub files in the Replicated YAML.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Github Integration For Config File Versioning And Storage",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/github-integration.md",
        "content": "\nIf your application leverages the GitHub API, each customer instance of your application will require a unique Client Id and Client Secret during setup . Additionally, most enterprises will prefer to hook this into their instance of GitHub Enterprise (you’ll need to allow for the GH endpoint to be determined by the customer as well… sometimes GH:E instances are setup over HTTP instead of HTTPS so you’ll need to know that as well). Below is an example of the on-prem settings screen that a customer would see when configuring their GitHub Integration info (as well as the YAML that drives it & a ‘test’ button).\n\nname: github\n  title: GitHub Integration\n  description: Provide the location of your GitHub account\n  test_proc:\n    display_name: Verify GitHub Auth\n    command: githubappauth\n    arg_fields:\n    github_type\n    githubenterprisehost\n    githubenterpriseprotocol\n    githubclientid\n    githubclientsecret\n  items:\n  name: github_type\n    default: githubtypepublic\n    type: select_one\n    items:\n    name: githubtypepublic\n      title: GitHub.com\n      type: text\n    name: githubtypeenterprise\n      title: GitHub Enterprise\n      type: text\n  name: githubenterprisehost\n    title: GitHub Enterprise Host\n    description: The hostname of your GitHub Enterprise server\n    when: githubtype=githubtype_enterprise\n    type: text\n    required: true\n  name: githubenterpriseprotocol\n    title: GitHub Enterprise Host\n    description: The hostname of your GitHub Enterprise server\n    default: githubenterpriseprotocol_https\n    when: githubtype=githubtype_enterprise\n    type: select_one\n    required: true\n    items:\n    name: githubenterpriseprotocol_http\n      title: Insecure (http)\n      type: text\n    name: githubenterpriseprotocol_https\n      title: Secure (https)\n      recommended: true\n      type: text\n  name: githubclientid\n    title: Client ID\n    description: Your GitHub Application Client ID\n    type: text\n    required: true\n  name: githubclientsecret\n    title: Client Secret\n    description: Your GitHub Application Client Secret\n    type: text\n    required: true\n`",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Integrating GitHub",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/image-tagging.md",
        "content": "\nFor your Replicated YAML, we highly encourage you to push images with a specific tag & avoid the\n\"latest\" tag. This rule can be ignored when you're in early development stages and constantly\nupdating your images. However, once you're becoming more production ready, we suggest tagging\nimages with some type of version (we generally just use the sha of the GitHub commit). This is\nvaluable when you have multiple release channels & want to be able to maintain a constant version\nin the stable channel, but experiment with the unstable channel.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Image Tagging Best Practices",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/ldap.md",
        "content": "\nINTRODUCING V1 OF REPLICATED DIRECTORY SERVICE SYNC\nYou can now offer your Replicated deployed enterprise customers user management through\nLDAP & Active Directory by integrating with Replicated’s Directory Service module.\n\nThe first version of this feature supports LDAP 389 DS,\nFree IPA, Open LDAP and\nMicrosoft Active Directory 2008 & 2012.\n\nWe’ve done our best to simplify the integration pain points from these legacy legendary\ntechnologies. Instead of writing C connectors, your team can keep building features that\ntruly differentiate your application.\n\nSo, how does it work? There are 2 parts to this… the first piece will do the authentication\n& should only take about 3-5 lines of new code. The second piece is optional, takes a bit\nmore effort but will provide you with full LDAP/AD Sync to your user table (so you can do\nthings like show a list of users etc). This is the ‘producty’ description...\ndeveloper docs are available)\n\nPART 1: AUTH\nIf you have any type of federated login, then you likely have an if block that determines\nwhich method of authentication is being used. For Replicated DS Auth, you’ll want to check\nan envar to determine if DS auth is enabled. If it is, you’ll simply collect the username\n& password from the user as normal:\n\nYou then post that data to the replicated integration api login endpoint (which we register\nwith every container as an environment variable running in a Replicated install).\n\nAnd we’ll provide you with a success or failure response (200 OK or 401 Unauthorized)\n\nIf the response is success, you’ll also also get a response body with the username, DN\nand an array of attributes. The attributes array is very powerful as you could instruct\nyour enterprise customers to include custom attributes in their DS that you can use in\nyour app. (Quick note: unless you require that your customers include a specific field\nin the attributes section, you should not expect it. Every IT runs these systems a bit\ndifferently, so if you need something to be there, require it for your integration.)\n\nPART 2: SYNC\n\nIf you manage a user table in your SaaS product you might want to consider going beyond\nsimple DS auth and use the Replicated DS Sync functionality\n(see sync developer docs). This will\nallow you to access your user table for product features like user profiles, sharing,\nattribution etc. Sync is designed to work in conjunction with auth, so make sure you\nenable Part 1 before moving on to Part 2.\n\nTo do this you’ll need to create a REST provisioning service with specific endpoints.\nWhen you invoke the Replicated Sync we’ll use this endpoint to create, delete & manage\nusers in your user table, in sync with your customer’s activities in AD or LDAP.\n\nWe suggest deploying this as an additional microservice that is only available in your\nReplicated installed version when a customer enables DS Sync. You can do this by\nidentifying this service as an optional container in your Replicated YAML.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "LDAP Integration",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-08-29T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/license-controlled-settings.md",
        "content": "\nLicense fields can be used to control the settings page to show or hide advanced settings.  This technique does not need a new release and the settings page can be updated by a simple license sync.\n\nTo start add the custom license field on Vendor Web.  The field will be visible to the customer unless it is marked as hidden.  Set a default value to ensure existing licenses behave correctly as customers update their app version.\n\nUpdate the application YAML with a hidden item that represents the license flag.  Hidden items are very useful to bind template functions and command output to your settings.  Here the item should be read only with a default field that uses the template LicenseFieldValue function that refers to the new license field.  See the advanced_opt item shown below.\n\nFind the config item to be controlled by the license field.  The item may represent a single setting or a collection of settings.  Set the when option check the value for the hidden field and on the next application publish you can control the settings page with license flags.\n\nconfig:\nname: authentication\n  title: Authentication\n  description: Service credentials.\n  items:\n  name: advanced_opt\n    title: Allow Advanced UI Settings\n    type: text\n    default: '{{repl LicenseFieldValue \"advanced\" }}'\n    readonly: true\n    hidden: true\n  name: password\n    title: Password\n    type: text\n    default: password\nname: advanced\n  title: Advanced\n  description: Advanced settings to control the authentication endpoint.\n  when: advanced_opt=1\n  items:\n  name: endpoint\n    title: Override API Endpoint\n    type: text\n    default: api.services.io\n",
        "lastmod": "2016-08-29T00:00:00Z",
        "title": "License Controlled Settings",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/local-images.md",
        "content": "\nShorten the development workflow of iterating on your container images. Here's how:\n\nDuring development use a static tag for your image & YAML reference to the image (just don't use \"latest\" as we always pull latest).\nStop the application in Replicated and do a docker build of the image on the machine running the Replicated daemon.\nFor public images tag the image with the name as it appears in your app YAML (for instance \"image_name: alpine\" and \"version: 3.4\" should be tagged as alpine:3.4)\nFor third party private registry images tag the image with the full name including the registry path. For example: registry.replicated.com/example/counter:1.0.1 (you can find the name using docker images).\nUse the Start button on the Replicated dashboard and your newly built images will be used without the need for you to push and pull.\n\nTo ensure reproducible installs once you are done with development increment the tag, push it to your repository and update the app YAML to use the new tag. Do not overwrite images used in prior releases.\n\nSince Replicated is not installing the new images but using what is already available on the host it will not clean up the old images so you should occasionally delete older versions of your images.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Speed Up Development By Using Local Images",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/local-testing.md",
        "content": "\nOnce you have written some YAML describing your application, you’ll need to begin actually\ntesting the application on a test server. You can do so by setting up a vagrant machine\nor by starting a virtual machine in your favorite IaaS (recommended).\n\nThe process for testing a Replicated application requires a bit of iteration. The key\ncomponents are:\n\nPromote a build to the unstable channel from the releases page\n\n*Hint: During this iteration process we recommend marking all releases as optional in\norder to avoid getting stuck.*\n\nCreate a developer license for the unstable channel from the licenses page\nDownload the license to your local machine\nThen run curl -sSL https://get.replicated.com/docker | sudo bash to install Replicated\non your server.\n\nUpload your license and see if your application works as you expected. If not, you might\nneed to reset the server by running:\n\nCentOS/RHEL\nyum remove -y replicated replicated-ui replicated-agent replicated-updater\nrm -rf /var/lib/replicated\ncurl -sSL https://get.replicated.com/ | sudo sh\n\nUbuntu/Debian\ndpkg --purge replicated replicated-ui replicated-agent replicated-updater\nrm -rf /var/lib/replicated\ncurl -sSL https://get.replicated.com/ | sudo sh\n\nFor this reason we often recommend working with IaaS machines that are disposable during\nthe early development/testing phase.\n\nIf you need to make changes to your application, you can simply create a new release on\nthe releases page, update the YAML and then\npromote it to the top of the unstable channel.\n\nFrom there you return to your server & click the check for updates button on the dashboard.\nIf your new release doesn’t show up, there might be an error in the YAML. Then just\npromote a new release to the top of unstable and check for updates again on your test\nserver. If you’re iterating through a lot of new containers, check out our guide for a\nsimple development workflow.",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Local Testing Setup",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/multi-node-cassandra.md",
        "content": "\nIt is critical for some of our vendor's applications to offer a Highly Available (HA) solution to their\ncustomers. Hardware fails, plugs get disconnected but your application must do its best to stay running!\n\nIn this article I am going to set up a Cassandra cluster that runs on\n3 physical hosts with replication. With this setup a host can be completely lost and a new host can\nreplace it with 0 data loss.\n\nStep 1: Create Cassandra Component with a Tag and Host Count\n\ncomponents:\nname: Cassandra\n  tags:\n  cassandra\n  cluster: true\n  clusterhostcount:\n    min: 2\n    max: 3\n\nFirst we create our tags, these will be used in the Replicated on-prem UI to assign this component to the desired hosts. Each host that gets tagged with a cassandra tag will run this component.\n\nWe then set cluster to true to tell Replicated that we will be running in a clustered fashion and set min and max nodes.\n\nStep 2: Set up the container\n\n  containers:\n  source: public\n    image_name: cassandra\n    display_name: cassandra\n    ephemeral: false\n    version: 2.1.12\n    restart:\n      policy: always\n    volumes:\n    host_path: /opt/cassandra-data-volume/data\n      container_path: /var/lib/cassandra/data\n    host_path: /opt/cassandra-data-volume/commitlog\n      container_path: /var/lib/cassandra/commitlog\n    host_path: /var/log/testapp/cassandra\n      container_path: /var/log/cassandra\n    env_vars:\n    name: CASSANDRABROADCASTADDRESS\n      static_val: '{{repl ThisNodePrivateIPAddress }}'\n    name: CASSANDRA_SEEDS\n      static_val: '{{repl range $index, $host := NodePrivateIPAddressAll \"Cassandra\" \"cassandra\" }}{{repl if eq $index 1}},{{repl end}}{{repl if lt $index 2}}{{repl $host}}{{repl end}}{{repl end}}'\n    ports:\n    private_port: \"9042\"\n      public_port: \"9042\"\n      port_type: tcp\n    private_port: \"7000\"\n      public_port: \"7000\"\n      port_type: tcp\n    private_port: \"7001\"\n      public_port: \"7001\"\n      port_type: tcp\n    private_port: \"7199\"\n      public_port: \"7199\"\n      port_type: tcp\n    private_port: \"9160\"\n      public_port: \"9160\"\n      port_type: tcp\n\n*Cassandra implements the concept of Seed Nodes to\naccomplish its ability to cluster. At least one Seed node should exist, it enables new nodes to\njoin the cluster. Seed Nodes also act as gossip hot spots and have the most current information\non them. All nodes in a cluster should have the same list of seed nodes in a cluster but not all\nnodes should be seeds.*\n\nWe set the envvar CASSANDRABROADCAST_ADDRESS using the ThisNodePrivateIPAddress template function to get the unique private IP of the containers host. This tells the local cassandra instance its own ip.\nLastly we generate a list of seed nodes using a little Replicated magic to set the envvar CASSANDRASEEDS using Go Templates and NodePrivateIPAddressAll. I only want to have 2 cassandra seed nodes but you can implement it however you like by slightly altering the template below:\n\n//Loop through all Hosts that have a cassandra container on it\n{{repl range $index, $host := NodePrivateIPAddressAll \"Cassandra\" \"cassandra\" }}\n//Only make the first 2 containers Seed Nodes\n\t{{repl if eq $index 1}},{{repl end}}\n\t{{repl if lt $index 2}}{{repl $host}}{{repl end}}\n{{repl end}}'\n\nWhen testing don't forget initialize your Cassandra DB with appropriate keyspace\nWe also make sure to set up our keyspace in\nCassandra correctly to achieve the correct redundancy and replication associated with a 3 node setup. I did\nthis by going directly into cqlsh shell and creating my keyspace like so:\n\nCREATE KEYSPACE replicated_test\n  WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };\n\nCheck out Ephemeral Containers\nfor running migrations!\n\nAnd there you have it, your very own Cassandra Cluster!\n\nDownload Full Replicated YAML Example.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Creating a Multi-Node Cassandra Cluster",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/one-click-updates.md",
        "content": "\nOverview\nReplicated makes the process of providing your on-prem customers with your regular\napplication updates as simple as possible. Whenever a release is promoted to a channel\nwhere existing customers are licensed, your customers will be able to update their\ninstance with 1-click.\n\nFor example, if you promote a release to the Stable channel (from the vendor portal)\nyou'll be prompted to input release notes (supports markdown), a version number and\nspecify if the release is required or not (required releases are useful if there are\nmigration scripts that need to be run sequentially).\n\nIf your customers do not have automatic updates enabled in their Replicated license,\nthey'll be able to apply the update manually. To do so, your customers will need to\ncheck their on-prem admin console (https://server:8800/dashboard) to see the\navailable update.\n\nBy clicking to view the update button they'll be taken the release history page\n(https://server:8800/releases) where they'll see the most recent release notes\ndisplayed prominently.\n\nBy clicking the “Install Update” button, they'll see some quick feedback on the progress\nof the installation & then the “Status” of the release in the release history table\nwill change from “New” to “Current”. If there are multiple updates available,\nReplicated will step through the installation of all “required” releases sequentially\n(to ensure that required migrations are carried out). Any release marked as optional\n(other than the most recent) will not be applied.\n\nFrom this page they'll also be able to view their previous release history & click\n\"read more\" to read the release notes in full markdown. Release notes under 100\ncharacters is just displayed in-line, in plain text).\n\nIf there is no update currently detected, the dashboard provides a button to \"Check for\nUpdates\" as well as a link to this release history page.\n\nScheduling Update Checks\n\nYour customers have the ability to change the frequency that Replicated will check for\napplication updates with either prechosen time increments or a crontab format enabled\ncustom field setting.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "One-Click Updates",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/optional-required.md",
        "content": "\nAs continuous integration and continuous deployment (CI/CD) processes have become common\nplace, rapidly iterating on your software has become a necessity to enable your development\nteam to deliver quickly and reliably. When shipping enterprise versions through Replicated,\nthe Vendor API provides\nmethods to connect an existing CI/CD workflow to your on-prem installations.\n\nIf you plan to ship frequent releases to on-prem installations, you should make sure your\nrelease process works well for customers who don't update frequently. Most enterprises\nwon't apply each update right away, they will need some time to plan the upgrade. Often this\nmeans a customer might be several releases behind and need to “catch up” to the most current\nrelease to get a new feature or fix. Replicated allows releases to be marked as optional or\nrequired to help manage this process.\n\nOptional Releases\n\nAn optional release is a release that can be skipped, if there is a newer release available.\nFor example, if a customer is on release 101, and you've promoted 102 and 103, both as\noptional releases, the customer will never install 102. When this customer clicks the\nupdate button, it will skip over the 102 optional release and install 103.\n\nIf an optional release is the current release in a channel, it will be installed when a\ncustomer clicks the Update button in the dashboard.\n\nRequired Releases\n\nA required release is a release that should always be installed when applying updates,\neven if its an intermediate release. A common reason this is required is when a critical\ndatabase migration only exists in a single release and you don't want your customer to\nskip over this release. In the same example as above, if a customer is on release 101,\nand you've promoted 102 as an optional release, 103 as a required release, and 104 as an\noptional release, when this customer clicks the update button, Replicated will install\n103 and then immediately install 104 only after 103 has started successfully.\n\nSpecifying \"Optional\" or \"Required\" when promoting a release\n\nSpecifying or changing after a release is promoted\n\nIt’s also possible to to\nchange an existing release to\nmodify whether it's optional or required.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Understanding Optional/Required Releases",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-08-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/running-commands-on-the-host-os.md",
        "content": "\nThe current version of Replicted is delivered as Docker images and run in containers on every supported operating system. The Replicated containers are built on top of a small Alpine Linux base image. Sometimes, it's desirable to access the host operating system and execute commands or change the environment in some way.\n\n{{}}\nRemember that Replicated supports many different operating systems that will be running a variety of kernels, package managers and more. Any script you choose to run on the host OS should be portable and run across any supported system.\n{{}}\n\nReplicated can leverage the Docker socket file to create a container that can connect to the host over SSH and run commands over that SSH connection. Anything you execute on the host should be tested thoroughly across all operating systems. It is possible to make breaking changes to the host that cannot be recovered remotely.\n\nFinally, the below example uses a curl | bash command in a script. You should carefully consider any security and supportability concerns that come from changing the host itself.\n\nreplicatedapiversion: \"2.3.5\"\n\nname: \"Application\"\n\nproperties:\n  logo_url: http://www.replicated.com/images/logo.png\n  console_title: Application\n\ncomponents:\n  name: SSH\n    containers:\nExecutes a bash script to generate keys\nWrites the public key to the host's authorized_keys\n      source: public\n        image_name: debian\n        version: jessie\n        ephemeral: true\n        publish_events:\n          name: SSH Key Created\n            trigger: container-stop\n            subscriptions:\n              component: Bootstrap\n                container: debian\n                action: start\n        volumes:\n          host_path: /root/.ssh\n            container_path: /.ssh\n            is_ephemeral: true\n          host_path: /keys\n            container_path: /keys\n            is_ephemeral: true\n\n        cmd: '[\"/createsshkey.sh\"]'\n        config_files:\n          filename: /createsshkey.sh\n            file_mode: 0700\n            contents: |\n              #!/bin/bash\n\n              echo \"Check if the public key exists in the authorized keys.\"\n              if [ -f /keys/id_rsa.pub ]; then\n                grep -f /keys/idrsa.pub /.ssh/authorizedkeys\n                if [ \"$?\" -eq \"0\" ]; then\nThe key exists and is in the authorized keys.\n                  exit 0\n                fi\n              fi\n\n              echo \"Generate a new key, if it does not exist\"\n              if [ ! -f /keys/id_rsa.pub ]; then\n                echo \"Install the ssh-keygen command\"\n                apt-get update && apt-get install -y keychain && rm -rf /var/lib/apt/lists/*\n\n                echo \"Create an ssh key\"\n                ssh-keygen -f /keys/id_rsa -t rsa -N ''\n              fi\n\n              echo \"Write the public key to the authorized keys file\"\n              cat /keys/idrsa.pub  /.ssh/authorizedkeys\n\nConnects to host over SSH as root and run a command\n  name: Bootstrap\n    containers:\n      source: public\n        image_name: debian\n        version: jessie\n        ephemeral: true\n        publish_events:\n          name: Install Complete\n            trigger: container-stop\n            subscriptions:\n              component: SSH\n                container: alpine\n                action: start\n        volumes:\n          host_path: /keys\n            container_path: /keys\n            is_ephemeral: true\n        cmd: '[\"/runincontainer.sh\"]'\n        config_files:\n          filename: /runincontainer.sh\n            file_mode: 0700\n            contents: |\n              #!/bin/bash\n\nIt would be preferrable to create a custom image with this next line already installed.\n              apt-get update && apt-get install -y ssh && rm -rf rm -rf /var/lib/apt/lists/*\n\nSSH to the host and then execute the \"runonhost.sh\" script on the host\n              ssh -i /keys/idrsa -o StrictHostKeyChecking=no root@{{repl ThisNodePrivateIPAddress}} 'bash -s' < /runon_host.sh\n          filename: /runonhost.sh\n            file_mode: 0700\n            contents: |\n              #!/bin/bash\n\nThis script will be executed on the host operating system and can use replicated template functions\n\nIt's possible to access license properties here\n              mylicenseval='{{repl LicenseFieldValue \"mycustomfield\"}}'\n\nAlso, config options\n              myconfigval='{{repl ConfigOption \"myconfigoption\"}}'\n\nComplete example:\n              if [ '{{repl ConfigOption \"updatemanagementconsole\" }}' = \"true\" ]; then\n                nohup sh -c \"curl -sSL https://get.replicated.com/docker | sudo bash && curl -sSL https://get.docker.com | sudo bash\" &\n              fi\n  name: Application\n    containers:\nReplace this container with a container in your application that never terminates\n      source: public\n        image_name: alpine\n        version: 3.3\n        cmd: '[\"tail\", \"-f\", \"/dev/null\"]'\nconfig: []\n",
        "lastmod": "2016-08-01T00:00:00Z",
        "title": "Running Commands on the Host OS",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/sequencing-startup.md",
        "content": "\nOrchestrating your containers to “turn on” in the correct order is one of the more challenging issues\nour vendors face when taking their product from “SaaS to On-Prem”. To help you avoid frustrating\nrace conditions when deploying your app on-premise Replicated has added a few powerful tools to\nsimplify container sequencing. Note that containers will not be guaranteed to start in a specific\norder if no events are present in your yaml.\n\n**I want to ensure that my DB (Redis) is started before I turn on my app and eventually my load balancer.\nWe are going to use Publish Events\nfunctionality to accomplish this.**\n\nIm going to add container-start triggers to our containers, I will also specify a subscription to this\ntrigger and the action that the subscribed container will take when the event happens.\n\nFirst in the redis container I declare:\n\npublish_events:\nname: Container redis started\n  trigger: container-start\n  data: \"\"\n  subscriptions:\n  component: App\n    container: freighter/counter\n    action: start\n\nThen in the freighter/counter container I declare:\n\npublish_events:\nname: Container freighter/counter started\n  trigger: container-start\n  data: \"\"\n  subscriptions:\n  component: LB\n    container: nginx\n    action: start\n\n**Bonus: I also want to make sure that my app is actually started before my on-premise dashboard\nindicates that it is ready. We are going to use Health Check\nfunctionality to accomplish this.**\n\nBy adding the following snippet I am able to tell Replicated that my app is ready when my load balancer\nstarts serving pages.\n*Note: Replicated attempts the command for 10 minutes, after that time if the command has not succeeded\nthe app will be in a failed to start state*\n\nstate:\n  ready:\n    command: httpstatuscode\n    args:\n    http://{{repl ConfigOption \"hostname\"}}/assets/bootstrap.min.css\n    \"200\"\n\nThats it! I now have a app that will initialize in the correct order and knows when my app is ready so that my end customer knows when to start using it.\n\nDownload Full Replicated YAML Example.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Sequence Your App Startup and Know When it's Ready",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-06-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/smtp.md",
        "content": "\nMany SaaS applications rely on external email services such as SendGrid or Amazon SES to deliver\nemails. This isn't always portable to enterprise installations because these services might not\nbe reachable. It's preferable to provide configuration options to let your customer configure\ntheir own SMTP gateway, and use that to deliver emails for on-prem installations.\n\nThere are many settings that are needed to completely configure an SMTP connection. To help,\nwe've created a config snippet you can copy and paste into your YAML. This YAML snippet is at\nthe bottom of this page. We also have created and included in the snippet an\nSMTP Auth test command.\nThis will create a test button next to the settings so that the customer can validate they've\nentered valid information.\n\nFollowing this example, you'll have a settings section that looks like this:\n\nThe YAML to create this is:\n\nname: smtp_on\n  title: Email Server Settings\n  description: Configure your outgoing email server settings\n  items:\n  name: smtp_enabled\n    default: smtpenabledno\n    type: select_one\n    items:\n    name: smtpenabledyes\n      title: Enable SMTP\n      recommended: true\n      type: text\n    name: smtpenabledno\n      title: Disable SMTP\n      type: text\nname: smtp\n  when: smtpenabled=smtpemabled_yes\n  test_proc:\n    display_name: Test SMTP Authentication\n    command: smtp_auth\n    arg_fields:\n    smtphostaddress\n    smtp_starttls\n    smtpauthtype\n    smtp_username\n    smtp_password\n  items:\n  name: smtphostaddress\n    title: SMTP Server Address\n    default: smtp.gmail.com:587\n    help_text: \"please note you must include the port like so: smtp.gmail.com:587\"\n    type: text\n    required: true\n  name: smtp_username\n    title: SMTP Username\n    help_text: A valid user account to log in to your SMTP server\n    type: text\n    affix: left\n  name: smtp_password\n    title: SMTP Password\n    help_text: The password for the user\n    type: password\n    affix: right\n  name: smtpfromaddress\n    title: From Address\n    help_text: The from address that will be used in outgoing emails\n    type: text\n  name: smtp_starttls\n    title: Encryption Type\n    default: 1\n    type: select_one\n    items:\n    name: 1\n      title: Enable STARTTLS\n      recommended: true\n      type: text\n    name: 0\n      title: Disable STARTTLS\n      type: text\n  name: smtpauthtype\n    default: Login\n    title: SMTP Authentication Type\n    type: select_one\n    items:\n    name: Login\n      title: Login\n      type: text\n    name: CRAM-MD5\n      title: CRAM-MD5\n      type: text\n    name: Plain\n      title: Plain\n      type: text\n\nDownload Full Replicated YAML Example.\n",
        "lastmod": "2016-06-01T00:00:00Z",
        "title": "Adding SMTP Support On-Premises",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/third-party-registries.md",
        "content": "\nReplicated can integrate with your third party private registry (ie Docker Trusted Registry, Quay.io etc). To connect to these external registries\nyou'll need to connect your vendor account to these accounts on the app settings page.\n\nYou'll need to provide us with a reference name, endpoint, username, password and email address (we recommend creating a specific account for\nReplicated with read-only access to use).\n\nYour credentials will never be shared or used by the customer to pull your images, instead your images will be proxied by us for each\ninstallation.\n\nTo access these images in your YAML you'll need to use the reference name as the source & then the image name will need to provide the image\nname location, along with the version tag.\n\ncomponents:\nname: App\n  containers:\n  source: mythirdpartyprivateregistry\n    image_name: namespace/imagename\n    version: 2.0.0\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Third Party Registries",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2017-03-13T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/updating-expiration-on-licenses.md",
        "content": "\nAn example use case for using the Vendor API is license maintenance. This example will show how to extend the expiration dates of all licenses in a specific channels.\n\nPrerequisites\nAn API Token with Read/Write access\nThe target App ID\nThe target Channel ID\n\nFor details on how to get the App ID and Channel ID, refer to Finding Your API Token and App ID. These values will not change and should be supplied as static values to your license updating script.\n\n1) Get all outstanding licenses\nTo start, get all the licenses that have been issued for a specific app:\n\ncurl -X GET \\\n    -H 'Authorization: ' \\\n    https://api.replicated.com/vendor/v2/app//licenses\n\n2) Find expiring licenses\nIterate through the returned array from step 1 to create a new array that contains all licenses where \"ChannelId\" fields match your target channel ID. Select the licenses you want to extend from this array.\n\n3) Update expiring licenses\nIssue a single API call to the Vendor API for each license in the array to update the expiration date. An example of this (using cURL) is:\n\ncurl -X PUT \\\n    -H 'Authorization: ' \\\n    -H 'Content-Type: application/json' \\\n    https://api.replicated.com/vendor/v2/license/ \\\n    -d '{\"license_type\":\"dev\",\n        \"activation_email\": false,\n        \"airgapdownloadenabled\": false,\n        \"assignee\":\"Original Assignee Name\",\n        \"update_policy\":\"manual\",\n        \"channel_id\":\"\",\n        \"expiration_date\":\"2018-03-13\",\n        \"expiration_policy\":\"ignore\",\n        \"require_activation\":false}'\n",
        "lastmod": "2017-03-13T00:00:00Z",
        "title": "Updating Expiration on Licenses",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/developer-resources/zero-downtime-backup.md",
        "content": "\nReplicated provides a way to achieve zero downtime backups by combining two of our more powerful replicated features: Admin Commands and Snapshots.\n\nIn this example we will demonstrate how to backup redis without having to stop the redis process itself:\n\nStep 1: Create Redis Container with 2 volumes.\n\nNote: This YAML creates 2 Docker volumes: 1) A primary volume (“data”) 2) a secondary volume (“backup”) that will be used to store the redis dump. Make sure that your primary volume (ie “data”) has isexcludedfrom_backup: true to ensure that it isn't paused during the backup process.\n\ncomponents:\nname: DB\n  containers:\n  source: public\n    image_name: redis\n    version: latest\n    cmd: \"[\\\"redis-server\\\", \\\"--appendonly\\\", \\\"yes\\\"]\"\n    volumes:\n    host_path: /data\n      container_path: /data:rw\n      isexcludedfrom_backup: true\n    host_path: /backup\n      container_path: /backup\n    support_files: []\n\nStep 2: Create your admin Commands to backup and move data\n\nadmin_commands:\nalias: backup-redis-to-rdb\n  command: [redis-cli, bgsave]\n  run_type: exec\n  component: DB\n  container: redis\nalias: mv-backup-rdb-to-safe-place\n  command: [mv, /data/dump.rdb, /backup/dump.rdb]\n  run_type: exec\n  component: DB\n  container: redis\n\nStep 3: Enable backups\n\nWe enable backups and inline a script that calls our admin commands (notice that we use the --no-tty flag). Note: pause_all is set to false thus enabling 0 downtime backups!\n\nbackup:\n  enabled: true\n  pause_all: false\n  script: |\n    #!/bin/sh\n    replicated admin --no-tty backup-redis-to-rdb\n    replicated admin --no-tty mv-backup-rdb-to-safe-place\n\nAnd there you have it, zero downtime backup!\n\nDownload Full Replicated YAML Example.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Zero Downtime Backups with Replicated (Redis)",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Developer Resources"
        ]
    },
    {
        "type": "page",
        "uri": "/content/docs/kb/index.md",
        "content": "",
        "layout": "knowledgebase"
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/automated-snapshot-configuration.md",
        "content": "\nIn Replicated, snapshots can be run any time by clicking the “Run” button on the dashboard.\nSometimes, it may be preferable to configure this to run automatically, on a set schedule.\nThis can be accomplished by editing (or creating) a /etc/replicated.conf and manually restarting\nthe Replicated service.\n\nSample conf file\n\n{\n  \"SnapshotsPath\": \"/var/lib/replicated/snapshots\",\n  \"SnapshotsSchedule\": \"0 0 * * *\",\n  \"SnapshotsMaxBackups\": 4,\n  \"DisableScheduledSnapshots\": false\n}\n\n| Setting | Acceptable Values | Description |\n|---------|-------------------|-------------|\n| SnapshotsPath | A path location as a string | The location where your snapshots are stored. Default Value: /var/lib/replicated/snapshots |\n| SnapshotsSchedule | CRON expression as a string | A time interval as represented by a CRON Expression. (This is parsed and interpreted in GMT/UTC time zone). Default Value: \"0 0 * * *\" |\n| SnapshotsMaxBackups | int | Number of snapshots that will be kept (FIFO). Default Value: 3 |\n| DisableScheduledSnapshots | boolean | A boolean that represents if automatic scheduled snapshots are running. Default Value: true |\n\nNOTE: You have to restart the replicated service for these changes to take effect\n\nUbuntu/Debian\nservice replicated restart\n\nCentOS/RHEL/Fedora\nsystemctl restart replicated\n\nTake a look at our restoring from a snapshot article\nfor more on this subject.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Automated Snapshot Configuration",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2017-06-13T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/cli-alias.md",
        "content": "\nThe Replicated easy install script will automatically set up the replicated CLI and replicatedctl CLI aliases. However, if a server is automatically upgraded from a version before 2.9.0, then the replicatedctl alias will not be automatically installed. Likewise, if your customer performed a manual installation, then neither Replicated CLI versions will be available as an alias.\n\nDepending on the scheduler Replicated is running with, run the following shell scripts as sudo to set up the replicated and replicatedctl aliases:\n\nReplicated Scheduler\n\ncat  /usr/local/bin/replicated  /usr/local/bin/replicatedctl  /usr/local/bin/replicated  /usr/local/bin/replicatedctl  /usr/local/bin/replicated  /usr/local/bin/replicatedctl ",
        "lastmod": "2017-06-14T00:00:00Z",
        "title": "Replicated CLI Alias",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/creating-an-ami.md",
        "content": "\nAt times, it can be desirable to ship an Amazon Machine Image (AMI) to allow customers a familiar installation path.\nReplicated does not provide an official machine image, but it's simple to prepare one and publish it in the AMI\nmarketplace. These instructions will set up and prepare a Replicated 2.0 installation on an Ubuntu 14.04 AMI. You\ncould extend these to support any other operating system by using the concepts outlined here.\n\nLaunch an EC2 Instance\n\nTo get started, launch a base image of Ubuntu 14.04 with an EBS root device.\n\nPrepare and create the AMI\n\nStep 1: Install Replicated\n\nInstall Replicated using our installation script.\n\ncurl -sSL https://get.replicated.com/docker | sudo bash\n\nYou don't need to set up a proxy server and can choose the default settings. We will be erasing all settings before\nshipping this image.\n\nAfter installing Replicated, all of the containers should be running. You can verify this by running docker ps and\nthere will be several Replicated containers.\n\nStep 2: Stop and remove the Replicated containers\n\nWe want to clean the auto-generated and auto-detected data from this server. This way, when a customer turns on a new\ninstance of this server, it will be calculated again.\n\nsudo service replicated stop\nsudo service replicated-ui stop\nsudo service replicated-operator stop\nsudo docker rm -f replicated replicated-ui replicated-operator\n\nStep 3: Remove config files\n\nReplicated stores configuration in several locations. Removing all of this will force Replicated to rebuild it the next\ntime it starts.\n\nsudo rm -rf /etc/replicated*.conf\nsudo rm -rf /etc/default/replicated*\nsudo rm -rf /etc/sysconfig/replicated*\nsudo rm -rf /var/log/upstart/replicated*\nsudo rm -rf /var/lib/replicated\n\nStep 4\n\nWe've just deleted a lot of the work the initial installation script did. We need to create a little helper script that will do\nthe following tasks:\n\nUse the Amazon Metadata API to detect the\nprivate/public addresses of the instances\nGenerate a new, random daemon secret token\nCreate a custom /etc/replicated.conf file and write it to the server\nCreate a custom /etc/replicated-operator.conf file and write it to the server\nTo help with this, we've created a shell script and hosted it on get.replicated.com. You can download this and make it an init script. This\nwill cause the script to run when your customer boots the instance. We've also added a command at the end to delete the script to ensure\nit only runs once.\n\nThe files used in the next script are:\n\nbootstrap script to start replicated and install\ninit file to launch bootstrap on startup\n\nsudo mkdir /etc/replicated-bootstrap\nsudo mv /etc/init/replicated* /etc/replicated-bootstrap\nsudo curl -o /etc/replicated-bootstrap/init-defaults.sh https://get.replicated.com/utils/aws/ubuntu1404/replicated-init\nsudo chmod a+rx /etc/replicated-bootstrap/init-defaults.sh\nsudo curl -o /etc/init/replicated-init.conf https://get.replicated.com/utils/aws/ubuntu1404/replicated-init.conf\n\nTo learn more about using the /etc/replicated.conf file to provide the license for an automated install, and\nhow you can set your settings during an automated install process, please read\nAutomate Install for Testing.\n\nStep 5\n\nDo whatever other cleanup you want. Maybe you want to delete the bash history. Or add any other services to this machine. This is your AMI.\n\nStep 6\n\nShip the AMI. Using Amazon's instructions, you can now create an distribute\nyour own AMI with Replicated installed. When a customer turns it on, port 8800 will be listening and ready to accept a Replicated license file.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Creating An AMI",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/customer-install-instructions.md",
        "content": "\nWhen you are creating documentation for your customers to install your application via\nReplicated, we always recommend starting with our [official installation instructions]\n(/distributing-an-application/installing/). However, it can be helpful to\nsee what the installation instructions of other customers look like today:\n\nGetElk is an example app that we created with open sourced YAML and components.\n\nSysDig provides detailed instructions on installation for both a single host version and a multi-host distributed version of their application.\nSysDig Enterprise Installation Guide.\n\nCodeClimate provides a great example of how to communicate the entire process from start.\nto finish: CodeClimate Enterprise Installation\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Customer Facing Installation Instructions",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/data-policy.md",
        "content": "\nA Replicated installation connects to a Replicated-hosted endpoint periodically to\nperform various tasks including checking for updates and syncing the installed\nlicense properties. During this time, some data is transmitted from an installed\ninstance to the Replicated API. This data is limited to:\n\nThe IP address of the primary Replicated instance.\nThe ID of the installation.\nThe state of the installation (running, stopped, etc).\nThe current version of the installation.\nThe current version of the Replicated components.\n\nThis data is required to provide the expected update and license services. No additional\ndata is collected and transmitted by default from the instance to external servers.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Replicated Data Policy",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/firewalls.md",
        "content": "\nOften, customers will need to have a complete list of expected internal and outbound network traffic so they can open ports in firewalls and whitelist hosts and IP addresses for outbound connectivity. This document provides the list of all known connections that Replicated requires to run. Any external services required are not listed here.\n\nNote: Airgap installations can run completely offline, and all tasks can be performed without outbound internet access. Additionally, no installations of Replicated ever require inbound access.\n\nDepending on the current activity, the needs can be different. This document is broken into the tasks that the customer is attempting to perform, and then broken down by the type of installation they are running.\n\nFor IP based firewalls rules you can get the needed IPs from the Replicated Services and IPs.\n\nPort Requirements\n\nTo use the Replicated management console you are required to allow inbound/outbound traffic on TCP port :8800 to the subnet with which an IT administrator would be accessing the console.\n\nFor Replicated communication you must also allow TCP ports :9870-:9880 to accept both inbound/outbound traffic on the installed subnet. These ports are for internal communication and should not be exposed externally. Please note that if you are running a multi-host setup communication on these ports will be required between hosts as well as on the primary host.\n\nInitial Installation of Replicated\nWhen Replicated is installed, it can be downloaded from the Internet or packaged up and delivered in an airgap pacakge.\n\n| Host | Online Installation | Airgap Installation | Description |\n|---|---|---|---|\n| get.replicated.com |  Required |  Not Required | This endpoint hosts the install script that used in the Replicated easy install script. |\n| quay.io |  Required |  Not Required | The current Replicated images are hosted as public images in the Quay.io registry. |\n| Docker Hub |  Required |  Not Required | Some dependencies of Replicated are hosted as public images in Docker Hub.|\n\nApplication Installation and Upgrade\n\nTo install your application and perform updates, some external connections are required. All connections are initiated from inside the network, and can vary depending on the installation method and the application update.\n\n| Host | Online Installation | Airgap Installation | Description |\n|---|---|---|---|\n| api.replicated.com |  Required |  Not Required | This endpoint services the license sync check and used to pull down yaml for app upgrades. |\n| registry.replicated.com |  Required |  Not Required | This endpoint services pull requests for all private images. |\n| quay.io |  Required |  Not Required | The current Replicated images are hosted as public images in the Quay.io registry and may be upgraded during app upgrade time. |\n| Docker Hub |  Required |  Not Required | Some dependencies of Replicated are hosted as public images in Docker Hub.|\n| Third Party Registries |  Required |  Not Required | Replicated will pull public images hosted on third party registries directly so those should be identified and white listed. |\n\nOngoing Access\nWhen the application is up and running, and not being updated, the requirements for outbound internet access are greatly reduced. It's possible to even run a server completely disconnected from the Internet, and only connect when you want to check for updates.\n\nOnce the application is installed, your customer can continue to run it, and stop and start the application without any outbound access.\n\nIn order to perform basic maintence, some outbound access is required, as documented in the table below:\n\n| Task | Host | Online Installation | Airgap Installation | Description |\n|---|---|---|---|---|\n| Check for updates | api.replicated.com (port 443) |  Required |  Not Required | This endpoint is the only endpoint required to check for application updates. |\n| License sync | api.replicated.com (port 443) |  Required |  Not Required | This endpoint is the only endpoint required to sync the license. |\n",
        "lastmod": "2017-03-22T00:00:00Z",
        "title": "Firewalls",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/install-known-versions.md",
        "content": "\nWe recognize that developers depend on Replicated to provide a known experience each and every installation.\n\nStarting with Replicated 2.3.0 we support specifying the version of Replicated you want to use via the app YAML and Replicated can auto update itself during app updates. By using the channel install script the installer will automatically select the right version of Replicated for you. When updating your application, if the version of Replicated is lower than you specified, Replicated will auto update prior to installing your software. The auto update happens automatically and does not require any effort on your customers behalf.\n\nTo specify the version to use, add replicated_version into your preflight checks:\n\nreplicatedapiversion: 2.4.2\n\nhost_requirements:\n  replicated_version: \"=2.4.2 < 2.9.0\"\n\nFor the installer to use the version of Replicated you specify you should use the channel release install script url. This version of the script is similar to the easy install url but is customized based on your app YAML. To get the install link, login to your vendor.replicated.com account, select your app and click \"build history\" for your channel and click \"Copy install script url\".\n\nInstalling earlier versions\n\nTo install older versions of Replicated after 2.0 we support specifying the version in the install url.\n\ncurl -sSL \"https://get.replicated.com/docker?replicatedtag=2.3.2&replicateduitag=2.3.2&replicatedoperator_tag=2.3.2\" | sudo bash\n\nBest Practices\n\nHere are some best practices for installing your application with Replicated.\n\n1. Always install versions of the Replicated components that are known to work together\n\nIf you are just starting, we recommend using the host requirements to set the Replicated version. When moving to a newer version of Replicated always test your application prior to release. On a regular basis test the latest Replicated version by updating your replicated_version range and when ready publish it to your customers. Subscribe to and read our release notes.\n\n2. Proxy this to give out a branded and standard install url\n\nFor example, if you want to host the installation script on https://get.company.com/docker, we recommend setting up an nginx proxy. Proxy /docker externally to https://get.replicated.com/docker/app-id/channel-name.\n\n3. Install latest for support sometimes\n\nOccasionally, when supporting a customer running an older version of Replicated, we may ask that they upgrade it. We ship new versions constantly, and often will have a fix in a newer version. For this reason, we encourage you to keep as close to latest as possible.\n\nWe definitely encourage and support any installation that is running a production release of Replicated. We encourage you to install our latest release if possible but use host requirements to control when your customers start using the new version.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Installing Known Versions of Replicated",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/installation-script.md",
        "content": "\nToday we released an enhanced installation script to allow your customers to more easily configure\nfor proxy, Docker version requirements and interface selection. Each prompt is on a reasonable timer;\nif no selection is made the default is accepted and the installation proceeds.\n\nHTTP PROXY IDENTIFICATION\nPrompted during 100% of installations:\n\nDoes this machine require a proxy to access the Internet? (y/N)\n\nThis will always accept the default answer after 20 seconds, if not supplied. The default is N, unless\nthe environment variable (one of httpproxy | httpsproxy | HTTPPROXY | HTTPSPROXY) is set, then the\ndefault is Y.\n\nDOCKER VERSION REQUIREMENT\nIf Docker is not installed or at the correct version, no prompt or action is visible.\nIf Docker is detected and older than the current supported version the following prompt is displayed:\n\nThis installer will upgrade your current version of Docker (1.8.2) to the minimum required version: 1.9.1\nDo you want to allow this? (Y/n)\n\nSelecting Y will upgrade Docker. Selecting N will exit the script with instructions indicating that\nyou should upgrade your Docker installation. Making no selection will default to Y after 20 seconds.\n\nIf Docker is detected and newer than the currently supported version the following prompt is displayed and\nthe installer is aborted:\n\nThe installed version of Docker (1.9.4) may not be compatible with this installer.\nThe recommended version is 1.9.1\nDo you want to proceed anyway? (y/N)\n\nINTERFACE SELECTION\nIf the server is in EC2 or GCE with common configuration settings, no prompts will be visible. The Cloud Provider\nMetadata API will provide the private/public interfaces and the installation will continue.\n\nIf the server has eth0 and only eth0 (ipv4 assignment, excluding docker0 and lo interfaces), it will be\nassumed and the installation will continue.\n\nIf the server has no eth0 or multiple valid interfaces found, a prompt similar to the following will be presented:\n\nAnalyzing network configuration...\nThe installer was unable to automatically detect the private IP address of this machine.\nPlease choose one of the following network interfaces:\n[0] default: unspecified\n[1] lo   \t127.0.0.1\n[2] eth0 \t192.168.196.130\n[3] eth1 \t192.168.100.200\n[4] docker0\t172.17.42.1\n\nMaking no selection will default to unspecified after 60 seconds… for which Replicated will attempt to pick the best\noption.",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Installation Script",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2017-06-29T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/installing-docker-ee.md",
        "content": "\nThis document describes the general process of installing Docker Enterprise Edition (EE) and Replicated onto a RedHat Enterprise Linux (RHEL) server.\n\nStep 1: Install Docker Enterprise Edition onto RHEL\n\nFor RHEL servers you will need to install Docker EE. This will require that you have a paid Docker EE license or a 30-day trial license. Follow the Installing Docker EE on RHEL document and ensure you have a running Docker EE service before continuing to the next section.\n\nStep 2: Configure DeviceMapper + direct-lvm\nRedHat recommends a non-default storage driver for production environments. Follow the Configure direct-lvm mode for production documentation to place your Docker engine into DeviceMapper direct-lvm mode.\n\nStep 3: Install Replicated on RHEL\n\nOnce you have your RHEL server running Docker EE you will want to install Replicated using the no-docker flag.\n\nFor example, installing Replicated into a Docker EE/RHEL using the easy install method you will run the following command:\n\ncurl -sSL https://get.replicated.com/docker | sudo bash -s no-docker\n",
        "lastmod": "2017-06-29T00:00:00Z",
        "title": "Docker Enterprise Edition and Replicated on RHEL",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/installing-docker-in-airgapped.md",
        "content": "\nInstalling a supported version of Docker on a server that does not have any internet access is a simple process,\nbut it can require you to install a few dependencies. Most airgapped environments will still have access to yum\nand apt, but they will be pointing to local mirrors.\n\nFirst we must locate and download the desired package of Docker from one of the official repositories:\n\n*Note: Some distributions do not support newer versions of Docker, Replicated will continue to support a minimum\n1.7.1 version of Docker. We recommend that your customers install the latest\nReplicated supported version of\nDocker, see the table below.*\n\n| OS | Highest Docker Version Supported in Replicated |\n|---|---|\n| CentOS 7 / RHEL 7 | {{}} |\n| Ubuntu 12.04 (precise) / 14.04 (trusty ) / 15.10 (wily) / 16.04 | {{}} |\n| Fedora 22 | {{}} |\n| Debian 7 (Wheezy) / 8 (Jessie) | {{}} |\n| Ubuntu 15.04 (vivid) | 1.9.1 |\n| Fedora 21 | 1.9.1 |\n| CentOS 6 / RHEL 6 | 1.7.1 |\n| Ubuntu 14.10 (utopic) | 1.7.1 |\n\nOnce the correct package has been downloaded and transferred to the airgapped machine they need to install it using\none of the following commands:\n\nrpm (CentOS/RHEL/Fedora)\nrpm -ivh .rpm\n\ndpkg (Ubuntu/Debian)\ndpkg --install .deb\n\nDifferent versions of Docker require different dependencies that may have to be manually downloaded/transferred/installed\nto the airgapped machine. Your customers will have to follow the same procedure for each one of those dependencies.\n\nMake sure you take a look at our Docs for the rest of the Airgapped\ninstallation instructions.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Installing Docker in an Airgapped Environment",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/installing-licenses.md",
        "content": "\nIt's possible your customers might run into problems while uploading their license after\ninstalling Replicated.\n\nIf your customer's encounter this screen please check if the license has expired!\n\nYou can do this by visiting:\nhttps://vendor.replicated.com - app - licenses page.\n\nIf customers see the following screen:\n\nTheir license file has been corrupted or modified, you should download a new copy and resend it to\nthe customer.\n\nIf you utilize Replicated's license activation feature your customers might see the following screen:\n\nWhen this error screen appears you should have your customers check the following:\n\nHas the customer cut and copied the exact code from the LATEST “Replicated License Activation Code”\nemail? (Please note that Replicated can send multiple activation emails)\nDoes this host still have connectivity to Replicated's market API? You can test this by having the\ncustomer curl -i https://api.replicated.com/market/v1/echo/ip and checking that they get a response.\nIf all else feels please contact Replicated\nand let us help!\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Installing Licenses",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/instance-reporting.md",
        "content": "\nReplicated recently introduced a way to track the usage & updates for each customer you issue a\nlicense to (available for customer running Replicated versions 1.2.48 & higher). This is helpful when\nyou're supporting a customer or what to just have some insight as to how they're using the license\nyou provided to them.\n\nYou can view the number of times a license has been installed by navigating to the\nCustomers page in the vendor portal. By clicking on the \"Customer reporting\" button under Actions you’ll then be able to view the active and inactive instances of\nthat license.\n\nYou can click into a specific instances details by clicking the \"View Details\" icon. (Instances are\nmarked as inactive after 24 hours has passed from their last checkin).\n\nOn the instance page you’ll find the records for when each update was first applied (specifically when\nit first reported back to Replicated that it was applied). You’ll also find the active versions of\nthe Replicated daemons.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Instance Reporting",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/license-screen.md",
        "content": "\nThe license screen within the Replicated Vendor Portal was recently updated. These changes were designed to make it easier to create and manage your licenses.\n\nCreating a new license\nYou can create a new license by clicking “New License” in the top right of the main license screen.\n\nSearch\nThe license screen search bar searches the content inside of the Customer and Channel columns. You can also filter your searches by Active, Inactive and Archived Licenses.\n\nLicense Toolbar\n\nThe license toolbar is a new interface to manage your license. While on the main license screen, you’ll see the toolbar appear when you hover over an individual license.\n\nThis toolbar is also the primary navigation when viewing and individual license, it can be found in the top right corner within a license.\n\nSettings\n\nThe settings tab on the toolbar takes you to the manage license screen. This screen allows you to manage General, Policy and Custom Field Properties for that specific license.\n\nReporting\n\nThe reporting tab on the toolbar takes you to the license reporting screen. This screen will show you instance reporting information for Active and Inactive licenses. For each instance you can hover and click details for more information on a specific instance.\n\nBilling\n\nThe billing tab on the toolbar takes you to the license billing screen. If your license type is paid you’ll be able to create billing events below the license type input.\n\nArchive\n\nThe archive tab on the toolbar will archive an individual license. This can be undone. To find an archived license go to the main license screen, select the filter “Archived Licenses” and search for a specific license you’d like to unarchive.\n\nUnarchive\n\nWhen a license is archived the toolbar will show an unarchive tab. Simply click this tab to unarchive an individual license.\n\nDownload\n\nThe download tab on the toolbar will download the license key for the individual license.\n\nCustom License Fields\nYou can edit the values of a custom field within the settings of an individual license. If you are looking to add a new Custom Field we’ve moved this screen to a new tab in the side nav under Licenses as show below.\n\nIf you have any questions or concerns please contact Replicated\nand let us help!\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "New License UX",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2017-06-12T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/multichannel-licenses.md",
        "content": "\nInstalling Multi-Channel License\nWhen multiple release channels are assigned to a license, the customer will be able to select the channel during installation time.\n\nChannel drop-down will show all channels assigned to the license.\nChannel description, located under the drop-down, can be edited on the Channels screen in the Replicated Vendor Portal.\nCurrent version tag and release notes can be modified on the Channel History screen in the Replicated Vendor Portal.\n\nAirgapped Installations\nFor airgapped installations, the license channel has to be selected at the time when the airgap bundle is downloaded.\n\nChanging Channel\nChannel can be changed on the License page in Replicated management console.  Once channel is changed, Replicated will sync license and check for release updates.  If newer releases are available, the customer will be able to upgrade the application.\n\n{{}}\nSwitching to a channel with an older application release than the one currently installed will not result in a downgrade of the application.\n{{}}\n\nAirgapped Installations\nSwitching channels in airgapped installations is not supported.\n",
        "lastmod": "2017-06-12T00:00:00Z",
        "title": "Multi-channel Licenses",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/networking-setup.md",
        "content": "\nEnterprise customers frequently run iptables on their servers. If you need a quick\nrefresher on iptables, take a look at this quick tutorial.\nMost installations of iptables have a default rule to deny traffic unless otherwise\nallowed. This will often prevent your containers from connecting to each other. You\nobviously don't want this, but the customer environment is defaulting to deny everything.\n\nAvoiding trouble with iptables\n\nWe want to help you avoid this problem on the next installation by clearly communicating\nthis requirement to your customers. Most\ninstallation documentation\nlists the ports that must be opened between the users and the server network, but not\ninternally. A common approach is to list the installation requirements like this:\n\nat least 8 cores and 16 GB of RAM\ninbound access to ports 8800, 80 and 443\n250GB of SSD storage\n\nTo avoid iptables troubles, add a new bullet point to your requirements:\n\ninternal communication on ports 3306, 6379 and 11211 on the server\n\nThis will help prepare your customer for the installation.\n\nYour customer won't want to open a Postgres or Redis port to everything and it's a good\npractice to allow them to write the most restrictive iptables rules possible. If all of\nyour containers run on a single host, the Replicated YML has the ability to define some\nservices to bind only to the docker0 interface, which generally isn't routable off of\nthe single server. But this interface is still subjected to the iptables rules, and still\nmust be configured. Often, customers will be more willing to change iptables rules on\nthe docker0 interface than they would be to change them on the eth0 interface. When\nyou bind only to the docker0 interface, the ports won't be available to port scan and\nother vulnerability scans that will inevitably be run against your installation.\n\nDetecting iptables trouble\n\nSometimes when your customer installs your software, things don't work as well as they\nhave in all of your tests. Knowing how to troubleshoot to diagnose and quickly resolve\nthe problem is important. At Replicated, our goal is to make all of your installations\npainless and successful, but there's one piece that we can't control – the customer's\nenvironment.\n\nMost networking problems present themselves early during an installation. If your\ncontainers appear to start and die quickly or Replicated won't successfully start your\napp but instead shows a message like this on the dashboard:\n\nit's very possibly a networking problem. This is one of the easiest things to troubleshoot\nand resolve, once you have some tools ready.\n\nBasic Troubleshooting\n\nSupport Bundle\n\nAs always, the first place to start is by requesting a support bundle and examining it. You'll\nwant to look at the docker stdout/stderr for your containers, and see if you are having a\ncommunication problem between them. For example, if postgres is part of your stack, you\nmight find a message like this in your api container:\n\npsql: could not connect to server: Connection refused Is the server running on host host.domain.com and accepting TCP/IP connections on port 5432?\n\nBut it's weird, because you see postgres running. The container is there. You can even see port 5432\nlistening in the netstat output of the host in the support bundle. Why can't this other container\nestablish a connection?\n\niptables\n\nIt's almost always iptables.\n\nAd-hoc commands to confirm it's iptables\n\nOn one recent troubleshooting session, the customer didn't know if iptables might be causing the\nproblem. So we decided to test connectivity on docker0. It went something like this:\n\nCustomer: The app won't start. It says “Error waiting for ports to enter listening state”.\nVendor: Can you send us a support bundle?\n[pause for time to examine support bundle]\nVendor: Ok, I see that our rails app can't connect to our MongoDB container. What's the ip address of the docker0 interface on this server?\nCustomer: echo $(ip addr | grep docker0 -A5 | grep \"inet \" | head -n1 | cut -d \"\" -f3,2 | awk '{ print $2}' | cut -d \"/\" -f1,1) says it's 172.17.0.5.\nVendor: Uh, great? Can you try telnet 172.17.0.5 27017?\nCustomer: It connects. MongoDB is definitely listening.\nVendor: Ok, can you try to connect from inside a container too? Run docker run -it ubuntu /bin/bash and you'll get a # prompt. Run the same telnet command in there.\nCustomer: It cannot connect from there. Ah. Let me edit my iptables ruleset.\n\nNow the customer has a very simple, reproducible environment that doesn't have the complexity of your\nconfig files and envvars and everything else. They can simply change iptables and run the failing telnet\ncommand until iptables is allowing the traffic.\n\nKeep it updated\n\nThis problem will creep back up any time a new port is opened, or a new cache/database is used, or any\narchitecture change is deployed. The final step is to make sure you have a process to keep this\ndocumentation current.\n\nOne idea we really like is to test your Replicated deployments on a server with iptables enabled. When\nyou add a new component or connection, it will fail until you edit the iptables configuration. This\nwill remind you to keep your docs updated and communicate the new port to your customers.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Networking Setup",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-22T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/registries.md",
        "content": "\nWhen your customer is installing or updating your application, Replicated is responsible for pulling all required Docker images into the environment. These images can come from a variety of sources and each customer environment is a little different. Replicated makes use of various methods to securely deliver the Docker images to all of your customer's nodes, and understanding how this works can be useful with troubleshooting.\n\nReplicated Images\nTo use the private Docker Registry that Replicated hosts for each application please see the Replicated registry integration guide.\n\nWhen you use the Replicated private registry, Replicated will always use the Docker client to pull your images. The Replicated registry is a secure, private-only registry that is closely integrated with our licensing feature. Your developers and API tokens can push and pull images to and from this registry. A customer's license is used for authentication during installation and update on-prem. The customer's license credentials is only granted pull permissions to your images.\n\nPublic Images\nWhen your application makes use of public images, Replicated uses the Docker Remote API to pull the image. Nothing special is needed to support this, and Replicated supports this automatically.\n\nPrivate Images\nTo use images in a private (non-Replicated) registry, see the guide for integrating a third party registry.\n\nOur servers provide a proxy to securely deliver these external images only to licenses that you create and authorize (without sharing the registry credentials you provided). When the on-prem component attempts to pull a private image, Replicated will open an HTTPS connection to a service we host. Once the connection is authenticated and verified, the Replicated proxy will stream the Docker layers and manifest to the on-prem server. The on-prem server then uses the Docker Remote API to docker load this image into Docker.\n\nWe currently support the Docker registry protocol version 2 and version 2.2. In current versions of Replicated, we've removed support for importing private images that only support the Docker registry protocol version 1.\n",
        "lastmod": "2016-07-22T00:00:00Z",
        "title": "Registries",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/resetting-console-password.md",
        "content": "\nIf the customer chooses to secure their on-prem Replicated admin console with a\npassword they can reset that password (or switch the authentication type to\nLDAP/unsecured) by visiting https://``/create-password.\n\nIf they are unable to remember the password (and hence unable to access the\n/create-password page), they can also reset the password if they have sudo access\nto the machine.\n\nTo do this they need to:\n\nUse the following command: replicated auth reset to remove the password\nVisit https://``:8800/create-password to create a new password or connect LDAP.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Resetting the Console Password",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/restoring-from-a-snapshot.md",
        "content": "\nThe snapshots functionality in Replicated is for disaster recovery purposes.\n\nIf snapshots are enabled in the application YAML, the user will have the ability\nto perform manual and scheduled snapshots.  More information on configuring snapshots can be\nfound in the Snapshots section.\n\nTo restore you need to create a fresh install of replicated which you can find instructions\nfor here. Before\nrunning the web console at https://:8800 place a copy of the full snapshot directory\non the host. Proceed through the https setup screen and on the upload your license page click the\n\"restore from a snapshot\" link.\n\nEnter the path on the host where you have copied the snapshots folder,\nClick “Browse snapshots”:\nLocate the latest version you would like to backup from and click the “Restore” button.\n\nYou will be given options for restoring, downloading the volumes, or deleting from the prior\ninstall, in this case we will restore to the local host by clicking the “restore” button.\n\nNext you will be prompted to specify which host you would like to restore to (for this example\nI am going to restore to local).\n\nThe last step is to set the correct interface for the localhost, in this case it is “eth0” and one last time hit the “restore” button.\n\nYou have now restored your snapshot! Take yourself to the console.\n\nFor example of advanced snapshot setups make sure to check out our\nZero Downtime Backups With Replicated article.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Restoring From a Snapshot",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/set-up-2fa.md",
        "content": "\nWith two-factor authentication (2fa) enabled, you'll be asked to provide an authentication code as\nwell as your password, when you access the Replicated vendor portal. Replicated utilizes the open\nalgorithm known as the Time-based One-time Password\n(TOTP) which is specified by\nthe Internet Engineering Task Force (IETF) under RFC 6238.\n\nTo enable two-factor authentication :\n\nLog in to Replicated and visit the 2fa setup page.\nClick on the “Set up two-factor authentication” button\nEnter your password to get to the two-factor configuration screen.\nOpen a supported two-factor authentication app on your device (we suggest Google Authenticator).\nScan the QR code or enter the alphanumeric code into the app.\nEnter the generated code in the form.\n\nSave the recovery codes\n\nYou will be presented with an option to download a set of recovery codes. These codes can be used any\ntime (one time per code), in case you lose your device. It's important to save these codes in a secure\nlocation.\n\nLog out\n\nTry it out. Log out of your account and log back in. You will be prompted to enter a one-time code\ngenerated by the supported application.",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Setting Up Two Factor Authentication",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/support-bundle-from-cli.md",
        "content": "\nIf your customer cannot access the Replicated UI, you can still have them\ndownload the support bundle from the Replicated CLI by running the following\ncommands on the server:\n\n*Note: If your on-premise console has a password you will need to authenticate\non the CLI*\n\nreplicated apps\nreplicated support-bundle\n\nWhere  is the id of your application taken from the output of the first command.\n\n(Available in versions of Replicated 1.2.73 and higher)",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Generate a Support Bundle from the CLI",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/two-factor-licenses.md",
        "content": "\nIn order to ensure that your application and its images are only being used by the companies\nthat you have authorized to do so, Replicated introduced a feature to tie a license to a\nspecific email address.\n\nBy so doing, the license file is no longer the single factor to access & run your\napplication. Instead, every time a new instance installation is attempted, Replicated\nwill require that your customer provide an activation code that has been emailed to\nthe associated email address of that license.\n\nThis feature is activated at the license level. During license creation (you can add/remove\nthis at anytime... previous installations will not be impacted in anyway, except on\nreinstallation).\n\nFrom the Customers page, select a customer or\ncreate a new customer:\n\nCheck “Require Activation” & include the email address of your customer.\n\nDuring the installation process, immediately after uploading their license file (before any\nof your images are delivered to the customer’s installation) they’ll be prompted to input\nthe activation code:\n\nIf they check their email they should have an activation code sent from Replicated:\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Two Factor Activation for Customers",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/update-certs.md",
        "content": "\nEach Replicated installation requires customers to either use a locally generated\nself-signed TLS/SSL cert or provide their own certs during setup.\n\nIf after initial setup this needs to be changed, it can be done so from the command\nline by using the\nSSL cert set CLI command\nor via the UI at https://:8800/console/settings\n\n![Console Settings)(/static/console-settings-v1.png)\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Updating SSL/TLS Certificates for On-Prem Admin",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/update-live-release.md",
        "content": "\nIf you promote a release to a channel but later realize there is a issue you can use the\nvendor web to fix the problem quickly.\n\nPossible issues would be:\n\nRelease had been marked as “required” and now should be skipped.\nMisspelling in the release notes.\nVersion number was mislabeled.\n\nStep 1: Click on the Channel of the release that you want to fix.\n\nStep 2: Click the link of the release in question.\n\nStep 3: Update the values you would like to change and click “Save Release”\n\nThere you have it!",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Updating a Live Release",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-01T00:00:00Z",
        "uri": "/content/docs/kb/supporting-your-customers/updating-replicated.md",
        "content": "\n{{}}\nThe content in this document is for a previous version of Replicated. If you are looking\nfor the current version, it is available at\n}}distributing-an-application/upgrading/\"{{}}distributing-an-application/upgrading/\n{{}}\n\nFor your customers to be able to take advantage of the latest features available in\nReplicated, they will occasionally need to update the version of Replicated they are\nrunning in their server.\n\nThis is done differently based on which OS they are running.\n\nUbuntu/Debian\napt-get update && apt-get install -y replicated replicated-ui replicated-agent replicated-updater\n\nCentOS/RHEL\nyum makecache && yum -y update replicated replicated-ui replicated-agent replicated-updater\n\nIf they are running a distributed version of your application across multiple hosts, they’ll also need to update the replicated-agent on those machines. To do so, they’ll need to run the following command on each additional host:\n\ncurl -sSL https://get.replicated.com/agent | sudo sh\n\nYou should make sure that they also update Docker\nto the Replicated supported version as well.\n",
        "lastmod": "2016-07-01T00:00:00Z",
        "title": "Updating Replicated",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-07T00:09:10Z",
        "uri": "/content/docs/kb/supporting-your-customers/upgrading-to-replicated-2.md",
        "content": "\n{{}}\nTo prevent loss of data, backing up your server is highly recommended before performing a migration.\n{{}}\n\nReplicated provides a one line migration script to upgrade your v1 installation to v2. The script will first stop\nyour app and backup all Replicated data in case there is a need for a restore. To invoke the migration script all\nyou have to do is run the script below and follow the prompts.\n\ncurl -sSL https://get.replicated.com/migrate-v2 | sudo bash\n",
        "lastmod": "2016-07-07T00:09:10Z",
        "title": "Upgrading to Replicated 2.0",
        "weight": "999999",
        "categories": [
            "Knowledgebase",
            "Supporting Your Customers"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/admin-commands.md",
        "content": "\nThe admin_commands section allows you to define ad-hoc commands that can be executed inside a running container from the shell.\n\nNote: If you are calling admin commands from a script use the --no-tty flag.\n\nExecuting\n\nReplicated\n$\nor\n$ replicated admin\nor\n$ docker exec -it replicated replicated admin\n\nSwarm\n$\nor\n$ replicated admin\nor\n$ docker exec -it \"$(docker inspect --format \"{{.Status.ContainerStatus.ContainerID}}\" \"$(docker service ps \"$(docker service inspect --format \"{{.ID}}\" replicated_replicated | awk \"NR==1\")\" -q)\")\" replicated admin\n\nKubernetes\n$ kubectl exec -it \"$(kubectl get pods -l=app=replicated -l=tier=master -o=jsonpath='{.items..metadata.name}')\" -c replicated -- replicated admin\n\nExamples\n\nnginx-reload\n\nThis example admin command will create a shell alias to allow mycli nginx-reload to execute the command service nginx reload inside the running nginx container. This same admin command will also be available to run with replicated admin nginx-reload, or simply as nginx-reload\n\nproperties:\n  shell_alias: mycli\nadmin_commands:\nalias: nginx-reload\n  command: [service, nginx, reload]\n  run_type: exec\n  component: MyComponent\n  container: nginx\n\nredis-sadd\n\nThis example admin command will create a shell alias to allow mycli redis-sadd mykey myvalue to execute the command redis-cli sadd mykey myvalue inside the redis container. This same admin command will also be available to run with replicated admin redis-sadd mykey myvalue or redis-sadd mykey myvalue\n\nproperties:\n  shell_alias: mycli\nadmin_commands:\nalias: redis-sadd\n  command: [redis-cli, sadd]\n  run_type: exec\n  component: MyComponent\n  container: redis\n\nSwarm\n\nproperties:\n  shell_alias: mycli\nadmin_commands:\nalias: redis-sadd\n  command: [redis-cli, sadd]\n  run_type: exec\n  service: redis\n\nKubernetes\n\nadmin_commands:\nalias: redis-sadd\n  command: [redis-cli, sadd]\n  run_type: exec\n  selector:\n    app: redis\n    tier: backend\n    role: master\n  container: master # optional, will choose first in pod\n\nConfiguration\n\nshell_alias\nThis is the shell alias that will be created during installation when using the Replicated scheduler.  Commands can be invoked using this alias or using the replicated CLI directly. Note that this is defined in Application Properties, not in the admin command. This alias is not available for Kubernetes applications.\n\nalias\nThis is the command that the user will specify on the command line.  When shell_alias is defined, shell aliases will also be created for each individual admin command.\n\ncommand\nThis is the actual command that will be executed inside the container when the alias is invoked through the replicated CLI.\n\nrun_type\nSpecify exec to execute the command in the currently running container. This is currently the only option.\n\ncomponent\nReplicated (required): This identifies the component under which to run the command.\nSwarm: unavailable\nKubernetes: unavailable\n\nservice\nReplicated: unavailable\nSwarm (required): This identifies the service under which to run the command. A container will be chosen at random to run the command in.\nKubernetes: unavailable\n\nselector\nReplicated: unavailable\nSwarm: unavailable\nKubernetes (required): This is a Kubernetes map of selectors to identify the pod that the admin command should be run in.\n\ncontainer\nReplicated (required): This specifies the container in which to run the admin command.\nSwarm: unavailable\nKubernetes (optional): This specifies the container in the pod in which to run the admin command. If not supplied the first container will be chosen.\n",
        "title": "Admin Commands",
        "description": "Implementation guide for application vendors to provide customers with aliased CLI commands that can be performed in the containers across a cluster.",
        "weight": "211",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/application-properties.md",
        "content": "\nThe properties section of the YAML allows you to configure properties of the admin console.\n\nBelow is an example of the properties section of an application config YAML.\n\nproperties:\n  app_url: http://{{repl ConfigOption \"hostname\" }}\n  logo_url: http://replicated.com/logo.png\n  console_title: My Enterprise Application\n  shell_alias: mycli\n\nAvailable Properties\napp_url\nThe URL of your application. A link to this URL will de displayed to the user on the dashboard of the admin console. This\nfield supports template functions and often uses one to determine the\nhostname or IP address to link to.\n\nlogo_url\nThe admin console header logo.  This image will be proxied by the Replicated license server during installation time. The\non-prem admin console will not load this image from the Internet; it becomes a local resource. This image will automatically\nbe included on airgapped installations without external access.\n\nconsole_title\nThe admin console header title (in the navbar) and HTML page title.\n\nshell_alias\nShell alias that can be used to run admin commands. See Admin Commands\nfor more information.\n\nconsolesupportmarkdown\nAdditional markdown content that will be displayed on the Support page of the admin console.\n",
        "title": "Application Properties",
        "description": "The Replicated YAML section `properties` allows several high level items to be defined. ",
        "weight": "203",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/clustering.md",
        "content": "\nBy default Replicated will start one instance per component and container in your application on a single host. With the addition of clustering your application can optionally leverage multiple hosts as well as multiple instances per host.\n\nSee an example of setting up a Cassandra Cluster with Replicated here\n\nHost Count\nThe application can be scaled horizontally at the component level by specifying host counts using the clusterhostcount property. The cluster property is required to enable this feature. When clustering is enabled, all containers that are members of the respective component will be allocated across the cluster to a minimum of min nodes and a maximum of max or 0 for unlimited.\ncomponents:\nname: App\n  cluster: true\n  clusterhostcount:\n    min: 2\n    max: 4\n  ...\n\nIn the example above, a minimum of 2 hosts will be required for the application to start. Replicated will start a single instance for each container that is a member of the App component on a minimum of 2 (and up to 4 hosts) as nodes are added to the cluster.\n\nTags\nIn addition to specifying counts, components can be tagged for more powerful orchestration on multi-host installations. Nodes within the cluster must be tagged at runtime, allowing the customer to specify where each component of the application will be deployed across the cluster. Components can also be tagged with conflicting tags. Replicated will prevent conflicting components from being allocated on the same node.\n\ncomponents:\nname: App\n  tags:\n  app\n  conflicts:\n  lb\n  cluster: true\n  ...\n\nInstance Count\nReplicated can also vertically scale the application on a single host at the container level by specifying instance counts using the clusterinstancecount property. The cluster property is required to enable this feature.\n\ncomponents:\nname: App\n  ...\n  clusterhostcount:\n    min: 2\n  containers:\n  source: public\n    image_name: freighter/worker\n    ...\n    cluster: true\n    clusterinstancecount:\n      initial: 3\n\nIn the example above at least 2 hosts are required. 3 instances of the freighter/worker container will be started on each of these 2 hosts.\n\nTemplates\n\nThe cluster property for both components and containers can be templatized. The template function must be parsable as a boolean. In addition, each of the properties of clusterhostcount and clusterinstancecount can be templatized. In this case the template function must be parsable as an unsigned integer. This was introduced in Replicated {{}}.\n",
        "title": "Clustering",
        "description": "An implementation guide for using the Replicated built in clustering functionality.",
        "weight": "208",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/commands.md",
        "content": "\nThe cmds section of the YAML allows you to leverage the power of external commands within your configuration. The sole\npurpose of these cmds is to generate data for input in the configuration screen.\n\nA command takes a variable number of string arguments and returns an array of strings. We have created an API with some\nuseful commands. There is also the option to run the command raw. This command will take any raw string input and run\nthe command in an Ubuntu Trusty container.\n\nThe command is run at YAML import time only (during app installation & during app updates).\n\nBelow is an example of a command that will generate a private key, a x509 certificate, and a random admin password that\nare used as configuration for our app.\n\ncmds:\nname: certcmdresult\n  cmd: cert\n  args:\n  \"1024\"\nname: gensecretresult\n  cmd: random\n  args:\n  \"32\"\n\nconfig:\n  ...\n  items:\n  name: serverprivatekey\n    title: Private key file\n    type: file\n    hidden: true\n    data_cmd:\n      name: certcmdresult\n      value_at: 0\n  name: server_cert\n    title: Certificate file\n    type: file\n    hidden: true\n    data_cmd:\n      name: certcmdresult\n      value_at: 1\n  name: server_authority\n    title: X.509 Signing Authority\n    type: file\n    hidden: true\n    data_cmd:\n      name: certcmdresult\n      value_at: 2\n  name: admin_password\n    title: Generated Admin Password\n    type: text\n    value_cmd:\n      name: gensecretresult\n      value_at: 0\n\nAvailable Commands\ncert\nGenerates a private key and x509 certificate pair. The resulting certificate is signed by the master authority belonging to the local instance of the Replicated management container. The authority certificate is also returned by this command so that clients can verify the full X.509 chain.\n\nNote 1: The authority certificate is generated the first time the management container is run on a system.\n\nNote 2: If you do not pass-in your own domain or IP address values to this command, you'll need to make sure that your server is set up to ignore bad certificate chains.\n\nNote 2 (long explanation): To properly verify the resulting certificate chain, the server using this new certificate must be accessible via the domains or the IP addresses encoded into the cert. You can specify these values as arguments to the cert command. If you don't, default values will be used, and the defaults will almost certainly not match the domain or IP address of the actual machine using the cert.\n\nArguments\nKey length: The length in bits of the private key. Default: 1024\nDomains: Comma-separated list of domains for which this certificate is valid. Default: \"example.com\"\nIP addresses: Comma-separated list of IP addresses for which this certificate is valid. Default: the IP address bound to eth0 on the machine running the Replicated management container.\n\nReturn value\n0: The new PEM-encoded private key.\n1: The new PEM-encoded certificate.\n2: The PEM-encoded cert authority used to sign the new certificate.\n\ncmds:\nname: certcmdresult\n  cmd: cert\n  args:\n  \"4096\"\n  mycounterapp.com\n\npublicip\nGets the public IP address of the agent server.\n\nThis function reaches out to an external service to acquire the ip and the result can then be written to a config item.\n\nReturn value\n0: Public IP address\n\nname: host_ip\n  cmd: publicip\n  args: []\n\nrandom\nGenerates a random string with the default charset [_A-Za-z0-9].\n\nArguments\nLength - The length of the string (default 16 characters).\nCharset\n\nReturn value\n0: Random string\n\nname: hash_key\n  cmd: random\n  args:\n  \"64\"\n  \"[A-Za-z0-9]\"\n\necho\nEchos the first argument.\n\nArguments\nString to echo\n\nReturn value\n0: String\n\nname: hello_world\n  cmd: echo\n  args:\n  Hello World!\n\nsystem\n{{}} Runs command directly on the machine on which Replicated is running. Be careful as Replicated supports many\nlinux distributions.\n\nArguments\nVariable\n\nReturn value\n0: String\n\nname: hello_world\n  cmd: system\n  args:\n  echo\n  Hello World!\n\nraw\nRuns command from a bash shell inside an \"ubuntu:trusty\" docker container. The docker image is hosted on dockerhub at https://hub.docker.com/r/freighter/cmd/\n\nArguments\nVariable\n\nReturn value\n0: String\n\nname: hello_world\n  cmd: raw\n  args:\n  echo\n  Hello World!\n",
        "title": "Commands",
        "description": "The `cmds` section of the Replicated YAML allows you to leverage the power of external commands within your application configuration.",
        "weight": "209",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/components-and-containers.md",
        "content": "\nThe components section of the YAML defines how the containers will be created and started. A component is\na group of one or more containers that are guaranteed to run on the same node.\n\nImage Registry Location\nFor each container we must supply some basic information. First the source from which we will pull this image. This can be either a third\nparty private or public registry (ie Docker Hub Registry, Quay.io, or your own public registry) or Replicated's private vendor registry\n(replicated). We must then specify the name of the image (image_name) and the tag (version).\n\ncomponents:\nname: DB\n  containers:\n  source: public\n    image_name: redis\n    version: latest\n    when: '{{repl ConfigOptionEquals \"useexternaldb\" \"0\" }}'\n    ...\n\nWhen including a public image please use a valid image name. This can be any format that the Docker client is able to pull, including:\n\nimage (trusted image from Docker Hub)\nnamespace/image (public image from Docker Hub)\nhost/namespace/image (public image from a different host)\n\nFor Replicated private images (when source = replicated, do not include the namespace in the imagename. The imagename field should only\ninclude the image name, not the hostname or namespace. The when option allows container to be started conditionally.\n\nPrivileged\nIn some advanced scenarios, we may need to run our container in privileged mode or specify a hostname to be set in the container. This is possible in the YAML by adding a couple of optional tags.\n\n  source: public\n    image_name: redis\n    ...\n    privileged: true\n    hostname: host01\n\nEphemeral\nSometimes we may want a container that is not meant to continue running for the lifetime of the application. In this case we can mark that container as ephemeral.\n\n  source: replicated\n    image_name: startup\n    ...\n    ephemeral: true\n\nContainer Resource Constraints\nReplicated supports the following constraint parameters:\n\ncpu_shares How much CPU time is allotted to the container details.\nmemory_limit How much memory is allotted to the container details.\nmemoryswaplimit How large a memory swap is allocated to the container details.\n\nnote: Replicated requires you to specify entire byte count as shown below\n\n  source: replicated\n    image_name: redis\n    ...\n    cpu_shares: \"1024\"\n    memory_limit: \"8000000000\" #8GB\n    memoryswaplimit: \"1000000000\" #1GB\n\nCMD\nNext we can optionally define a container CMD to execute when running our container.\n\n  source: public\n    image_name: redis\n    ...\n    cmd: '[\"redis-server\", \"--appendonly\", \"yes\"]'\n\nConfig Files\nThe next section contains inline configuration files that we can supply to our container. Replicated will create a file within the\ncontainer with the specified path (filename) and contents. You may optionally specify an octal permissions mode as a string (file_mode),\nand/or the numeric uid of a user & group (file_owner) to be applied to the resulting file in the container.\n\n    config_files:\n    filename: /elasticsearch/config/elasticsearch.yml\n      file_mode: \"0644\"\n      file_owner: \"100\"\n      contents: |\n        path:\n          data: /data/data\n          logs: /data/log\n          plugins: /elasticsearch/plugins\n          work: /data/work\n        http.cors.enabled: true\n        http.cors.allow-origin: /https?:\\/\\/{{repl ConfigOption \"hostname\" }}(:[0-9]+)?/\n\nGitHub Reference\nIt is also possible to specify a file as a Github Reference, where the ref is the SHA of the commit. This ref will need to be updated any\ntime the file changes (we cache the remote file to remove this external dependency from the install time processes). The repository will\neither need to be public or you will need to connect your Github account via the App Settings link of the Replicated Vendor Portal.\n(Supports config files only)\n\n    config_files:\n    filename: /elasticsearch/config/elasticsearch.yml\n      source: github\n      owner: getelk\n      repo: elasticsearch\n      path: files/elasticsearch.yml\n      ref: c3636b396b2df172926816be5660c9cabc8c5355\n      file_mode: \"0644\"\n      file_owner: \"100\"\n\nCustomer Files\nIt can also be helpful to request a customer supplied file. This file can be referenced by the name parameter and will be created within\nthe container at the specified path (filename).\n\n    customer_files:\n    name: logstashinputlumberjackcertfile\n      filename: /opt/certs/logstash-forwarder.crt\n      file_mode: \"0600\"\n      file_owner: \"0\"\n    name: logstashinputlumberjackkeyfile\n      filename: /opt/certs/logstash-forwarder.key\n      file_mode: \"0600\"\n      file_owner: \"0\"\n\nEnvironment Variables\nNext we have the option of specifying environment variables. There is also a flag provided to exclude anything secret from the support bundle.\n\n  env_vars:\n    name: AWSACCESSKEY_ID\n      staticval: '{{repl ConfigOption \"logstashinputsqsawsaccesskey\" }}'\n      isexcludedfrom_support: true\n    name: AWSSECRETACCESS_KEY\n      staticval: '{{repl ConfigOption \"logstashinputsqsawssecretkey\" }}'\n      isexcludedfrom_support: true\n\n{{}}\nHaving environment variables in Support Bundles can be invaluable for troubleshooting.   However, environment variables can contain sensitive data.  Setting isexcludedfrom_support to true will exclude them from Support Bundles.\n{{}}\n\nPorts\nWe can use the ports property to expose a container's port (privateport) and bind it to the host (publicport). The when property allows us to conditionally expose and bind that port when some prior condition is satisfied. Use the interface property to force the public port to be bound on a specific network interface. The public_port property is optional as of {{}} allowing a port to be exposed but not bound.\n\n    ports:\n    private_port: \"80\"\n      public_port: \"80\"\n      port_type: tcp\n      when: '{{repl ConfigOptionEquals \"http_enabled\" \"1\" }}'\n    private_port: \"443\"\n      public_port: \"443\"\n      interface: eth0\n      port_type: tcp\n      when: '{{repl ConfigOptionEquals \"https_enabled\" \"1\" }}'\n\nVolumes\nWe can also specify volumes that will be mounted.\n\nVolumes are required for any persistent data created by your application. If you have data in a container that needs to be available to new versions of your app, or data that should be backed up then you will define a volume to store it. Volumes are also useful for services that require a fast filesystem such as database or cache applications.\n\nYou need to specify only the hostpath and containerpath of the volume. When new versions of your container are deployed, the volume will be mounted in the updated container.\n\nNamed Volumes: You may create a \"named\" volume by providing a hostpath without a leading \"/\" (ex. hostpath: dbdata) which becomes the name of the volume. On creation, named volumes will link the information inside the containerpath into the hostpath location and will act as a shared folder between your host and your docker container. Only folders can be named volumes.\n\nHost Volumes: If you would like to have the volume mounted at a specific location on the host then you will provide a hostpath value with a leading \"/\" (ex. hostpath: /dbdata). Host volumes will bind-mount the hostpath contents into the containerpath location and will act as a shared mount between your host and your docker container. Folders or files can be bind-mounted host volumes.\n\nRequired properties:\n\nhost_path For named volumes, this is the volume name (ex. dbdata). For host volumes, this is the absolute host location for the volume (ex. /dbdata).\ncontainer_path The absolute location inside the container the volume will bind to (ex. /var/lib/mysql).\n\nOptional properties:\n\npermission should be a octal permission string.\nowner should be the uid of the user inside the container.\noptions {{}} optional volume settings in an array of strings, a \"ro\" entry puts the volume into read-only mode.\nis_ephemeral {{}} Ephemeral volumes do not prevent containers from being re-allocated across nodes. Ephemeral volumes will also be excluded from snapshots.\nisexcludedfrom_backup exclude this volume from backup if Snapshots enabled.\n\n    volumes:\n    host_path: /dbdata\n      container_path: /var/lib/mysql\n      permission: \"0755\"\n      owner: \"100\"\n      is_ephemeral: false\n      isexcludedfrom_backup: true\n      options: [\"rw\"]\n\nReplicated supports volumes_from to attach several mounts from a colocated container.\n\n  source: public\n    ...\n    name: datastore\n    publish_events:\n    name: Datastore started\n      trigger: port-listen\n      data: '6379'\n      subscriptions:\n      component: DBs\n        container: alpine\n        action: start\n  source: public\n    image_name: alpine\n    version: 3\n    ephemeral: true\n    cmd: '[\"migrate_data.sh\"]'\n    volumes_from: [\"datastore\"]\n\nThe container using \"volumesfrom\" must start after any containers it mounts from.  Property \"volumesfrom\" takes an array of strings where each string identifies a named container running on the same server.\n\nLogs\nWe can configure logs for containers by specifying the max number of logs files and the max size of the log files. The max size string should include\nthe size, k for kilobytes, m for megabytes or g for gigabytes. Log settings at the component level are inherited by the container and will be\nused unless overwritten.\n\ncomponents:\n  name: sample-agent\n    logs:\n      max_size: 200k\n      max_files: 2\n    containers:\n      source: replicated\n        logs:\n          max_size: 500k\n          max_files: 5\n\nRestart Policies\nOptionally, containers can be configured to be restarted automatically. Currently supported restart policies match those supported natively by Docker.\nIf the policy is not specified, the container will never be restarted. This behavior is equivalent to this setting:\n\nNever restart\n  restart:\n    policy: never\nSpecifying the following policy will always restart the container regardless of the exit code.\n\nAlways restart\n  restart:\n    policy: always\nSpecifying the following policy will cause the container to be restarted with it terminates with an error. The max parameter is optional. If omitted, the\ncontainer will be restarted indefinitely.\n\nRestart on error only\n  restart:\n    policy: on-failure\n    max: 1000\nPlease refer to our Examples page for additional component configuration examples.\n\nConfig Files\nYour application may have config files that require dynamic values. These values may be input by the person installing the software, values\nspecific to the environment your application is running in, values created by other containers or read from the embedded license via the\nLicense API. To accomplish this, Replicated allows templatizing of its config values using the Go template language with a repl escape\nsequence. When your application runs, Replicated will process the templates, and you have full access to the the Replicated template library.\n\nCustomer Files\nSometimes it can be helpful to allow a customer to supply a file to your app. A good example of this is when your customer should supply an\nSSL certificate and private key. To add customer supplied files to your container, you must first define the file as a config option, and\nthen create a link to it in any container that needs that file. Replicated will prompt for the file to be uploaded and will ensure that the\nfile is at the correct location in the container when it is started.\n\nEnvironment Variables\nThe 12-factor app encourages the use of environment variables for configuration, and Replicated supports this design pattern. You can specify\nenvironment variables, which will be injected into a container when it's created.\n\nEnvironment variables can be created with static values or customer supplied values.\n\nEnvironment variables support the Replicated template library.\n\nExposed Ports\nAll ports listed in the Dockerfile with the EXPOSE directive will be automatically exposed when started. The Docker runtime will choose a\nrandom port, ensuring that there are no conflicts. If you need to specify a specific public (host) port, you can list it here.\n\nCommon examples of when it is necessary to list an exposed port are for web server containers, or servers which have clients that are incapable\nof discovering dynamic port mappings.\n\nPort mappings support the Replicated template library.\n\nStartup\nThe startup section of a container allows you to specify the CMD value that will be passed to your container when it's started. It's generally\ngood to end your Dockerfile with an ENTRYPOINT command. If you specify a value for the CMD, it will be passed as parameters to the your ENTRYPOINT.\n\nAs with all inputs to containers, you have full access to the Replicated template library when creating a CMD value.\n\nDocker Options\nYou may also limit the resources used by your containers with the memory, cpushares and network modes and further secure your containers with security options\n\nMemory and Swap Limit\nThe amount of memory or swap for your container.  The format is number|unit where unit may be one of b, k, m or g.  By default there is no memory or swap limit and your container can use as much as needed.  You can learn more at User Memory Constraints documentation.\n  memory_limit: 500m\n  memoryswaplimit: 1g\n\nCPU Shares\nUsing CPU shares you can change the access to the servers CPU at busy times.  When non CPU-intensive processes are other containers may use extra CPU time.  The default is 1024 and by increasing or decreasing the value on a container you change how the weighted CPU access is granted across all running containers.  You can learn more at CPU Share Constraints documentation.\n  cpu_shares: 2048\n\nNetwork Mode\nNetwork mode supports bridge, host, container or none.  Learn more about Docker's network modes at Network Settings.\n  network_mode: host\n\nSecurity Options\nWith security options you can use Docker security with existing well know systems such as apparmor.\n  security_options:\n  apparmor=unconfined\n\nWhen specifying your security options you can use template functions and any blank security option is allowed and will be filtered out by Replicated.\n  security_options:\n    '{{repl if ConfigOptionEquals \"enableunconfinedapparmor_profile\" \"1\"}}apparmor=unconfined{{repl end}}'\n\nLearn more about Docker's security configuration.\n\nPrivileged Mode and Security Capability\nSecurity capabilities and access to devices are limited for containers by default, however you can add security capabilities with the privileged and securitycapadd option.\n    privileged: true\n    securitycapadd:\n    SYS_MODULE\n\nLearn more about Security Capabilities.\n\nAllocate TTY\nFor interactive processes you can allocate a TTYL with allocate_tty.  Learn more by reading about container process Foreground.\n  allocate_tty: true\n\nHostname\nSets the hostname inside of the container.  See the network host section under Network settings.\n  hostname: anxiety-closet\n\nExtra Hosts\nAdd extra hostname mappings with hostname, address and an optional when field.  See extra_hosts.\n  extra_hosts:\n  hostname: mysql\n    address: 10.0.1.16\n  hostname: redis\n    address: 10.0.1.32\n\nNamed Containers\nThe name argument sets the name of your running container. It is provided as a convenience method during development when you may want to connect to your containers and view logs. References to the container in template functions should continue to the use image name.  Do not use on containers which run concurrently as the second container will fail to start due to a name conflict.\n\n  name: redis\n\nFor more information see named containers.\n\nEntrypoint\nWhen working with third party containers you may want to override the default entry point using the\nentrypoint option.\nLearn more about overriding entrypoints and how the\ncmd and entrypoint options work together.  Entrypoint takes an array of strings.\n\n    entrypoint: [\"redis\", \"-p\", \"6380\"]\n\nUlimits\n{{}} Since setting ulimit settings in a container requires extra privileges not available in the default container, you can set these using the ulimits property of the container. Learn more about ulimits here.\n\n    ulimits:\n    name: nofile\n      soft: 1024\n      hard: 1024\n\nPid Mode\n\n{{}} Pid mode lets you specify the process namespace for your container. By default each container has its own space and by declaring a pid_mode you can see the processes of another container or host. See PID settings to learn more.\n\n    pid_mode: host\n",
        "title": "Components And Containers",
        "description": "The `components` section of the Replicated YAML defines how the containers will be created and started.",
        "weight": "206",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/config-screen.md",
        "content": "\nThe admin console settings screen configuration is specified as an array configuration\ngroups and items.\n\nGroups\nGroups map to links within the left sidebar navigation. Groups are comprised of a name,\ntitle, description and an array of items.\n\nconfig:\nname: authentication\n  title: Authentication\n  description: Configure application authentication below.\n\nItems\nItems map to input fields and belong to a single group. All items should have name, title\nand type properties. Specific item types can including new types.\n\nAvailable Item Types\n label\n text\n password\n file\n bool\n select_one\n select_many\n textarea\n select\n\nExamples\n\nbool\nThe bool input type should use a \"0\" or \"1\" to set the value\nname: toggles\n  items:\n  name: http_enabled\n    title: HTTP Enabled\n    help_text: When enabled we will listen to http\n    type: bool\n    default: \"0\"\n\nlabel\nThe label input type allows you to display an input label.\nname: Email\n  items:\n  name: email-address\n    title: Email Address\n    type: text\n  name: description\n    type: label\n    title: \"Note: The system will send you an email every hour.\"\n\nselect\nTypes selectone and selectmany are special cases. These types must have nested items\nthat act as options. These types will be displayed as radio buttons (select_one) or\ncheckboxes (select_many) in the admin console.\n\nAt this time these two control types do not support the title field.\n\nname: inputs\n  title: Inputs\n  description: \"\"\n  items:\n  name: logstashinputenabled\n    default: \"\"\n    type: select_many\n    items:\n    name: logstashinputfile_enabled\n      title: File\n      default: \"0\"\n    name: logstashinputlumberjack_enabled\n      title: Lumberjack\n      default: \"0\"\nname: authentication\n  title: Authentication\n  description: \"\"\n  items:\n  name: authentication_type\n    default: authenticationtypeanonymous\n    type: select_one\n    items:\n    name: authenticationtypeanonymous\n      title: Anonymous\n    name: authenticationtypepassword\n      title: Password\n\ntextarea\nA textarea can specify a props that will map into the HTML element directly.\n\nname: custom_key\n  title: Set your secret key for your app\n  description: Paste in your Custom Key\n  items:\n  name: key\n    title: Key\n    type: textarea\n    props:\n      rows: 8\n  name: hostname\n    title: Hostname\n    type: text\n\nProperties\n\ndefault and value\nA default value will be applied to the ConfigOption template function when no value is\nspecified. A default value provided via a command (default_cmd) is treated as ephemeral\ndata that will get overwritten each time commands are executed. It will appear as\nplaceholder text in the settings section of the On-Prem Console.\n\nA value is data that will be overwritten by user input on non-readonly fields. A value\nprovided as the result of a command (valuecmd or datacmd) will persist and will never\nchange after the first execution of the command. It will appear as the HTML input value\nin the settings section of the On-Prem Console.\n\nname: custom_key\n  title: Set your secret key for your app\n  description: Paste in your Custom Key\n  items:\n  name: key\n    title: Key\n    type: text\n    value: \"\"\n    default: localhost\n\nrequired\nA required field will prevent the application from starting until it has a value.\n    required: true\n\nwhen\nThe when value is used to denote conditional inputs that will only be visible (or required) when the condition evaluates to true. The when item can be used on groups, items and selectone or selectmany options.\n\nThe settings UI will update right away when a field used in a when clause is updated (no need to save) and can be used to used to show optional config sections. The equality check should match exactly without quotes.\n\nThe when property can be configured in two different formats. The legacy format is in form configitemname=value or configitemname!=value. As of Replicated {{}} template functions that evaluate to a parsable boolean can be used as a value to the when property.\n\nname: databasesettingsgroup\n  items:\n  name: db_type\n    type: select_one\n    default: embedded\n    items:\n    name: external\n      title: External\n    name: embedded\n      title: Embedded DB\n  name: database_host\n    title: Database Hostname\n    type: text\n    when: db_type=external\n  name: database_password\n    title: Database Password\n    type: password\n    when: '{{repl (ConfigOptionEquals \"selectone\" \"external\") or (ConfigOptionEquals \"selectone\" \"embedded\")}}'\n\nrecommended\nAn item can be recommended. This item will bear the tag \"recommended\" in the admin console.\n    recommended: true\n\nhidden\nItems can be hidden. They will not be visible if hidden.\n    hidden: true\n\nreadonly\nItems can be readonly.\n    readonly: true\n\naffix\nItems can be affixed left or right. These items will appear in the admin console on the same line.\n    affix: left\n\nUsing CMD (commands) as input to options\nCommands can be used as defaults, values, or data with defaultcmd, valuecmd and\ndata_cmd respectively. Data is a special property of the file type. The value corresponds\nto the file name while the data corresponds to its contents.\n\n    value_cmd:\n      name: hash_key\n      value_at: 0\n",
        "title": "Config Screen",
        "description": "The `config` section of the Replicated YAML creates a dynamic settings page that customers can use to configure their instance.",
        "weight": "204",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-02T00:00:00Z",
        "uri": "/content/docs/packaging-an-application/custom-metrics.md",
        "content": "\nAll Replicated installations come with a StatsD/Graphite/Carbon container that can be used by the application to report data to StatsD. Application YAML can also include optional custom monitors that will be used to display additional charts in Replicated dashboard. Applications can also query Graphite directly.\n\nDefining Metrics\n\nAdd custom_metrics as a root element to the application YAML. The following elements are supported:\n\ntarget - This is the full key name for your metric. It should use the Graphite naming conventions. The format is restricted to regex rules supported by Carbon.\nretention - The retention period defined using the new Carbon format: frequency:history\naggregation_method - One of the aggregation methods supported by Carbon: average, sum, min, max, and last.\nxfiles_factor - This is a value between 0 and 1 that defines what percentage of samples must be present in order for aggregate to a non-null value.\n\nDetailed information about retention, aggregationmethod, and xfilesfactor can be found in Carbon documentation.\n\nExample\n\ncustom_metrics:\ntarget: stats.gauges.myapp100.disk..\n  retention: \"1s:10m,1m:20m,1h:30d\"\n  aggregation_method: \"average\"\n  xfiles_factor: 0.3\ntarget: stats.gauges.myapp100.ping.*\n  retention: \"1s:7d\"\n  aggregation_method: \"last\"\n  xfiles_factor: 0.6\n\nLimitations\n\nPlease note that changing retention and aggregation settings will have no effect on keys that already exist in Carbon.\n\nStatsD is configured to use the flush interval of 1 second. Using average aggregators with bucket size greater than 1 second may result in inaccurate data.\n\nDefining Monitors\n\nEach custom monitor will be added as a tile to the Replicated dashboard. The following elements are supported\n\nname - String that will appear as the top label for the graph\ntarget - Deprecated.  Use targets.\ntargets - An array of strings that will be used as the target in Graphite query. Wildcards, lists, and functions are all allowed. See Graphite documentation for more details.\nfrom -  Start time for displayed data.  Relative and absolute times are supported.  See Graphite documentation for more details.\nuntil -  End time for displayed data.  Relative and absolute times are supported.  See Graphite documentation for more details.\ndisplay.label_unit - String that will be placed on the Y axis labels.\ndisplay.label_scale - Scale to be used on the Y axis. Possible values are:\n  metric - Y axes will scaled using metric units in 1K increments. The string specified in label_unit will be added as the postfix.\n  none - No scale will be applied.\ndisplay.labelrangeoverride - This value is used to indicate that the minimum and the maximum points on the Y axis will be overridden.\ndisplay.label_min - The minimum value to show on the Y axis.\ndisplay.label_max - The maximum value to show on the Y axis.\ndisplay.label_count - Number of labels to show on the Y axis (not counting the one at the origin).\ndisplay.fill_color - The color to use to fill the area under the graph line.  If this value is omitted, a color will be selected automatically.\ndisplay.stroke_color - The color to use for the graph line.  If this value is omitted, a color will be selected automatically.\ndisplay.cssclassname - The name of the CSS class to use for background colors.\n\nColors can be specified using one of the standard web color formats:\n\nHEX color, for example #10FF60\nRGB color, for example rgb(100, 0, 50)\nRGBA color, for example rgba(100, 0, 50, 0.5)\n\nfrom and until fields are optional.  If omitted, the tile will display the last 15 minutes of data.  If a larger time window is used, retention policies must be configured accordingly.\n\nExample\n\nmonitors:\n  cpuacct:\n  DB,redis\n  memory:\n  DB,redis\n  custom:\n  name: Disk Usage (bytes)\n    targets:\n      stats.gauges.myapp100.disk.*.free\n      stats.gauges.myapp100.disk.*.total\n    from: \"-3days\"\n    until: \"-30minutes\"\n    display:\n      label_unit: B\n      label_scale: metric\n      label_min: 30000000000 # gigabytes\n      label_max: 40000000000\n      labelrangeoverride: true\n      label_count: 2\n      fill_color: rgba(100, 0, 50, 0.5)\n      stroke_color: \"#ff1060\"\n      cssclassname: app1-custom-metrics\n  name: Disk Free (%)\n    targets:\n      scale(divideSeries(stats.gauges.myapp100.disk..free,stats.gauges.myapp100.disk..total),100) # Show values between 0 and 100\n    display:\n      label_unit: \"%\"\n      label_scale: none\n      label_count: 3\n      fill_color: rgba(0, 100, 50, 0.5)\n      stroke_color: \"#10FF60\"\n      cssclassname: app1-custom-metrics\n\nPorts\n\nBy default, Replicated will use dynamic ports for Graphite and StatsD.  These can be discovered by examining the\ngraphite container. Static ports can be defined using the graphite and the statsd sections of app YAML.\nThe graphite port can be accessed via web browser for a full Graphite dashboard.\n\nBecause Graphite uses TCP and StatsD uses UDP, the same port number can be used for both.\n\nExample\n\ngraphite:\n  port: 8899\nstatsd:\n  port: 8899\n\nCustom CSS\n\nCustom CSS can be used to define background colors for each metric's tile and chart background. CSS class name must match the name specified in the cssclassname tag.\n\n/* custom graphs */\n.app1-custom-metrics {\n  background-color: #999910;\n}\n.app1-custom-metrics  .app1-custom-metrics-chart {\n  background-color: #109999;\n}\n\nThe CSS can be added in Vendor Web dashboard in App Settings.\n\nIntegration API\n\nStatsD host can be discovered via Replicated Console Settings. The following Go function will return the current endpoint. Note that this value can change, and applications should periodically query Integration API to obtain the current endpoint.\n\nfunc GetStatsdEndpoint() (string, error) {\n\ttr := &http.Transport{\n\t\tTLSClientConfig: &tls.Config{\n\t\t\tInsecureSkipVerify: true,\n\t\t},\n\t}\n\t// Do not create a new client for every request in production code.\n\thttpClient = &http.Client{Transport: tr}\n\n\tapiEndpoint := os.Getenv(\"REPLICATED_INTEGRATIONAPI\")\n\tif apiEndpoint == \"\" {\n\t\treturn \"\", errors.New(\"REPLICATED_INTEGRATIONAPI is not set\")\n\t}\n\n\turl := fmt.Sprintf(\"%s/console/v1/option?name=statsd.endpoint\", apiEndpoint)\n\tresp, err := httpClient.Get(url)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn \"\", errors.New(\"Integration API returned unexpected status %v\", resp.StatusCode)\n\t}\n\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\treturn string(body), nil\n}\n\nInstalled Configuration\n\nThe Graphite/StatsD/Carbon container managed by Replicated has some baked in configurations. These defaults will apply to all data that matches the specified patterns. If application sends data to this container and the application has no matching custom_metrics, these defaults will be applied permanently.\n\nPlease note that statsd-config.js cannot be customized at this time.\n\nstatsd-config.js\n{\n  graphiteHost: \"127.0.0.1\",\n  graphitePort: 2003,\n  port: 8125,\n  flushInterval: 1000,\n  backends: [ \"./backends/graphite\" ],\n  deleteIdleStats: true,\n  deleteGauges: true\n}\n\nstorage-aggregation.conf\n[default] pattern = .* aggregationMethod = average xFilesFactor = 0.5\nstorage-schemas.conf\n\n[default] pattern = .* retentions = 2s:20m\n",
        "lastmod": "2016-07-02T00:00:00Z",
        "title": "Custom Metrics",
        "weight": "217",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2017-01-26T00:00:00Z",
        "uri": "/content/docs/packaging-an-application/custom-preflight-checks.md",
        "content": "\nThe host requirements section of the yaml gives Replicated the ability to analyze system\nrequirements and warn or prevent the user from proceeding with an installation or upgrade. In\naddition to host requirements, Replicated has the ability to define fully customizable preflight\nrequirements as of version {{}}. These custom requirements provide\nflexibility to the point that an arbitrary command can be executed by a vendor provided image. See\nthe commands section below for a full list of commands that may be run including\nexamples.\n\nThere are three types of custom preflight checks:\n\nRun a preflight check using your own container - see scheduler\nRun a shell script using ubuntu trusty - see raw\nUse a common preflight check - see diskspaceavailable, diskspacetotal, portavailable, tcpdial\n\nCommands\n\nCommands will be run to determine the status of a requirement. They return result messages, a\nstatus code and an error. Next we will look at examples. For details on the fields please see the\nresource specification section at the bottom of the page.\n\nScheduler\n\nThe scheduler command references a container in the components section of the yaml. Standard out\nand standard error will be captured and returned via the result message. Any exit code as a result\nof the container will be returned via the status code of the command. When the container cannot be\nrun due to an error, an error will be returned. The container will be run on the nodes as specified\nby the component section of the container yaml.\n\nId: scheduler\n\nStatus Codes: 1, 22, 62 *\n\n| Name | Type | Required | Description |\n|----------|----------|--------------|-----------------|\n| source | ReplicatedSchedulerSource{component: string, container: string} | yes | A component and container reference |\n| cmd | string | no | Optionally override the container cmd |\n| config_files | array[ConfigFile] | no | {{}} Additional config files to mount as volumes in the container |\n| entrypoint | array[string] | no | Optionally override the container entrypoint |\n| ports | array[ExposedPort] | no | Optionally override the container exposed ports |\n\nExample\n\ncustom_requirements:\nid: check-schema-version\n  message: Database schema is at the correct version\n  details: The database schema must be at version 2\n  when: '{{repl eq AppVersionCurrent 2 }}' # only when upgrading from app version 2\n  results:\n  status: success # error, warn, success\n    message: Schema is at version 2\n    condition: # error, statuscode, boolexpr\n      status_code: 0 # and\n      bool_expr: '{{repl Trim .Result | eq \"2\" }}' # template vars .StatusCode, .Result, .Results, .Error\n  status: error\n    message: # it is possible to localize these messages\n      id: custom_requirements[check-schema-version].results[1] # this is the default message id\n      default_message: Schema is at incorrect version {{.version}}. Please upgrade your schema to version 2.\n      args:\n        version: '{{repl Trim .Result }}'\n    condition: # error, statuscode, boolexpr\n      status_code: 0\n  status: warn\n    message:\n      defaultmessage: Unexpected status {{.statuscode}}\n      args:\n        status_code: '{{repl .StatusCode }}'\nif no error: true condition is specified the check will fallback to the default error message\n  command:\n    id: scheduler\n    timeout: 30 # in seconds, default to 15, -1 == no timeout\n    data:\n      component: DB # the component and container from the components section of the yaml\n      container: mysql\n      cmd: \"[\\\"sh\\\", \\\"-c\\\", \\\"'exec mysql -h {{repl ThisNodePrivateIPAddress }} -u myuser -p {{repl ConfigOption \\\"mysql_pass\\\" }} yourdatabase < /opt/check-schema-version.sql'\\\"]\"\n      config_files:\n      filename: /opt/check-schema-version.sql\n        contents: |\n          select version from schema limit 1;\n      ports: [] # override scheduler container properties\n\nRaw\n\nThe raw command is run inside Replicated's command container.\nThe clustering and tags properties will determine where the command is run. If clustering is\ndisabled the command will run on all nodes in the cluster. Additional properties to the raw command\nare all that can be specified in the container section of the yaml.\n\nId: raw\n\nStatus Codes: 1, 22, 62 *\n\n| Name | Type | Required | Description |\n|----------|----------|--------------|-----------------|\n| cmd | string | yes | The cmd to be run when executing the container |\n| cluster | string | no | Is clustering enabled (evaluated to a boolean value) |\n| tags | array[string] | no | Determines nodes where the check is performed when cluster=true |\n| conflicts | array[string] | no | Skips nodes with the tag when cluster=true |\n| additional... |  | no | all possible container properties |\n\nExample\n\ncustom_requirements:\nid: license-file-exists\n  message: License file exists\n  details: The vendor license file must exist on the host at /etc/vendor-license\n  results:\n  status: success\n    message: File /etc/vendor-license exists.\n    condition:\n      status_code: 0\n  status: error\n    message: File /etc/vendor-license does not exists.\n    condition:\n      status_code: 1\nelse error\n  command:\n    id: raw\n    data:\n      cmd: '[\"test\", \"-e\", \"/host/etc/vendor-license\"]'\n      volumes:\n      host_path: /etc\n        container_path: /host/etc\n        options: [\"ro\"]\n\nDisk Space Available\n\nThe disk space available command will return the disk space available in bytes. Note that the\nresult is always a string and must be parsed (e.g. {{repl .Result | ParseFloat | lt 1e+9 }} or\n{{repl .Result | ParseFloat | HumanSize }}). The clustering and tags properties will determine\nwhere the command is run. If cluster is false the command will run on all nodes in the\ncluster.\n\nId: diskspaceavailable\n\nStatus Codes: 1, 22, 62 *\n\n| Name | Type | Required | Description |\n|----------|----------|--------------|-----------------|\n| dir | string | yes | The directory to check |\n| cluster | string | no | Is clustering enabled (evaluated to a boolean value) |\n| tags | array[string] | no | Determines nodes where the check is performed when cluster=true |\n| conflicts | array[string] | no | Skips nodes with the tag when cluster=true |\n\nExample\n\ncustom_requirements:\nid: disk-space-available-mysql\n  message: Mysql data directory has sufficient disk space\n  details: The /data/mysql directory must have at least 8GB of disk space available.\n  when: '{{repl eq AppVersion AppVersionFirst }}' # initial install only\n  results:\n  status: success\n    message: Directory /data/mysql has enough space available\n    condition:\n      status_code: 0 # and\n      bool_expr: '{{repl Trim .Result | ParseFloat | lt 8e9 }}' # 8GB\n  status: error\n    message:\n      default_message: Directory /data/mysql has {{.bytes}} space available. Please increase disk space to at least 8GB.\n      args:\n        bytes: '{{repl ParseFloat .Result | HumanSize }}'\n    condition:\n      status_code: 0\n  status: warn\n    message:\n      default_message: 'Invalid status code {{.status}}. ERROR: {{.error}}'\n      args:\n        status: '{{repl .StatusCode }}'\n        error: '{{repl .Error }}'\nelse error\n  command:\n    id: diskspaceavailable\n    data:\n      cluster: true\n      tags: [\"db\"]\n      dir: /data/mysql\n\nDisk Space Total\n\nThe disk space total command will return the disk space available in bytes. Note that the result is\nalways a string and must be parsed (e.g. {{repl .Result | ParseFloat | lt 1e+9 }} or\n{{repl .Result | ParseFloat | HumanSize }}). The clustering and tags properties will determine\nwhere the command is run. If cluster is false the command will run on all nodes in the\ncluster.\n\nId: diskspacetotal\n\nStatus Codes: 1, 22, 62 *\n\n| Name | Type | Required | Description |\n|----------|----------|--------------|-----------------|\n| dir | string | yes | The directory to check |\n| cluster | string | no | Is clustering enabled (evaluated to a boolean value) |\n| tags | array[string] | no | Determines nodes where the check is performed when cluster=true |\n| conflicts | array[string] | no | Skips nodes with the tag when cluster=true |\n\nExample\n\ncustom_requirements:\nid: disk-space-total-mysql\n  message: Mysql data directory has sufficient disk space\n  details: The /data/mysql directory must have at least 8GB of disk space total.\n  when: '{{repl ne AppVersion AppVersionFirst }}' # upgrade only\n  results:\n  status: success\n    message: Directory /data/mysql has enough space total\n    condition:\n      status_code: 0 # and\n      bool_expr: '{{repl Trim .Result | ParseFloat | lt 8e9 }}' # 8GB\n  status: error\n    message:\n      default_message: Directory /data/mysql has {{.bytes}} space total. Please increase disk space to at least 8GB.\n      args:\n        bytes: '{{repl ParseFloat .Result | HumanSize }}'\n    condition:\n      status_code: 0\n  status: warn\n    message:\n      default_message: 'Invalid status code {{.status}}. ERROR: {{.error}}'\n      args:\n        status: '{{repl .StatusCode }}'\n        error: '{{repl .Error }}'\nelse error\n  command:\n    id: diskspacetotal\n    data:\n      cluster: true\n      tags: [\"db\"]\n      dir: /data/mysql\n\nPort Available\n\nThe port available command will determine whether the port and ip are available for use. Status\ncode 98 (address already in use) will be returned when unable to bind to the address. The\nclustering and tags properties will determine where the command is run. If cluster is false\nthe command will run on all nodes in the cluster.\n\nId: port_available\n\nStatus Codes: 1, 22, 62, 98 *\n\n| Name | Type | Required | Description |\n|----------|----------|--------------|-----------------|\n| port | string | yes | The port to check |\n| proto | string | no | The protocol, one of tcp (default) or udp |\n| ip | string | no | The ip to bind to, defaults to 0.0.0.0 (will take precedence over interface if set) |\n| interface | string | no | The interface to bind to |\n| cluster | string | no | Is clustering enabled (evaluated to a boolean value) |\n| tags | array[string] | no | Determines nodes where the check is performed when cluster=true |\n| conflicts | array[string] | no | Skips nodes with the tag when cluster=true |\n\nExample\n\ncustom_requirements:\nid: port-available-lb-80\n  message: Load balancer port is available\n  when: '{{repl eq AppVersion AppVersionFirst }}' # only on first install\n  details: Port 80 must be available for the load balancer.\n  results:\n  status: success\n    message: Port 80 is available\n    condition:\n      status_code: 0\n  status: error\n    message: Port 80 is not available\n    condition:\n      status_code: 98\n  status: warn\n    message:\n      default_message: 'Invalid status code {{.status}}. ERROR: {{.error}}'\n      args:\n        status: '{{repl .StatusCode }}'\n        error: '{{repl .Error }}'\nelse error\n  command:\n    id: port_available\n    data:\n      port: '80'\n      ip: '{{repl ThisNodePublicIPAddress }}'\n      cluster: true\n      tags: [\"lb\"]\n\nTCP Dial\n\nThe tcp dial command will determine whether a connection can be made over tcp to the address\nspecified. Status code 111 (connection refused) will be returned when unable to connect to the\naddress. The clustering and tags properties will determine where the command is run. If cluster is\nfalse the command will run only on the local node.\n\nId: tcp_dial\n\nStatus Codes: 1, 22, 62, 111 *\n\n| Name | Type | Required | Description |\n|----------|----------|--------------|-----------------|\n| addr | string | yes | The address to connect to |\n| cluster | string | no | Is clustering enabled (evaluated to a boolean value) |\n| tags | array[string] | no | Determines nodes where the check is performed when cluster=true |\n| conflicts | array[string] | no | Skips nodes with the tag when cluster=true |\n\nExample\n\ncustom_requirements:\nid: tcp-dial-github\n  message: Can access github.com\n  details: Can connect to the address github.com:443.\n  results:\n  status: success\n    message: Successful connection to the address github.com:443.\n    condition:\n      status_code: 0\n  status: error\n    message: Failed to connect to the address github.com:443.\n    condition:\n      status_code: 111\n  status: warn\n    message:\n      default_message: 'Invalid status code {{.status}}. ERROR: {{.error}}'\n      args:\n        status: '{{repl .StatusCode }}'\n        error: '{{repl .Error }}'\nelse error\n  command:\n    id: tcp_dial\n    data:\n      addr: 'github.com:443'\n      cluster: false\n\nResource Specification\n\nCustom requirements are represented with the followings and properties.\n\nRequirement\n\nThe requirement resource is the primary resource for custom preflight checks. A requirement\nrepresents a single check that is to be preformed during the installation and upgrade steps of the\napplication lifecycle.\n\n| Name | Type | Required | Description |\n|----------|----------|--------------|-----------------|\n| id | string | yes | A unique identifier for the requirement |\n| message | string or Message | yes | A short description of the requirement |\n| details | string or Message | no | A more detailed description of the requirement |\n| when | string | no | Will determine if this requirement should be run (evaluated to a boolean value) |\n| command | Command | yes | The command that will be run |\n| results | array[Result] | yes | An array of result objects that when evaluated will determine success or failure |\n\nCommand\n\nThe command resource represents the command that is to be run. The command will return messages, a\nstatus code and possibly an error. See the commands section for a list of supported\noperations.\n\n| Name | Type | Required | Description |\n|----------|----------|--------------|-----------------|\n| id | string | yes | The command id |\n| timeout | int | no | Timeout in seconds, default 15 seconds, -1 denotes no timeout |\n| data | object | no | The command data |\n\nResult\n\nThe result resource represents the different possible outcomes of the command. A result contains\na status, message and condition. Result are evaluated in order and the first matching result will\ndetermine the requirement status. If no condition properties are specified that result will always\nevaluate to true. If no results match the requirement will receive status error.\n\n| Name | Type | Required | Description |\n|----------|----------|--------------|-----------------|\n| status | string | yes | One of success, warn or error |\n| message | string or Message | yes | A description of the result |\n| condition | Condition | no | The condition that must be met |\n\nCondition\n\nAll properties of a condition must be met to determine that condition to be true. The bool_expr\nproperty is intended to be evaluated using Replicated templates. This template will receive\nthe following variables from the result of the command: .Results (array of messages), .Result\n(the first message), .StatusCode, .Error.\n\n| Name | Type | Required | Description |\n|----------|----------|--------------|-----------------|\n| error | boolean | no | Did the command result in an error? |\n| status_code | int | no | The command status code |\n| bool_expr | string | no | An expression that can be evaluated and parsed as a boolean |\n\nMessage\n\nA message resource can be localized via the id. It contains a default message that will be\ndisplayed when no localization is present. Messages have arguments that can be substituted into the\ntext via templates.\n\n| Name | Type | Required | Description |\n|----------|----------|--------------|-----------------|\n| id | string | no | The message identifier. Can be used to localize the message. |\n| default_message | string | yes | The default message |\n| args | map[string]string | no | Arguments to the message |\n\nStatus Codes\n\n| Code | Description |\n|----------|-----------------|\n| 1 | Catchall for general errors |\n| 22 | Invalid argument |\n| 62 | Timeout |\n| 98 | Address already in use |\n| 111 | Connection refused |\n",
        "title": "Custom Preflight Checks",
        "description": "A guide to implementing the Custom Preflight Checks feature to analyze customer systems to determine if the environment meets the minimum requirements for installation or update.",
        "weight": "214",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2017-07-11T00:00:00Z",
        "uri": "/content/docs/packaging-an-application/docker-swarm.md",
        "content": "\nIf your application is defined as a Docker Compose version 3 or 3.1 yaml file, Replicated can provide the same standard functionality deploying your application via the Docker Swarm scheduler as a Docker Stack as of Replicated {{}}. Using the Swarm scheduler, you can use all of the Swarm functionality including overlay networks, DNS service discovery, Docker secrets and more. To see a full example, check out the Voting App example.\n\nDifferences from the Replicated scheduler\n\nLike the standard Replicated scheduler, when shipping an application using Swarm mode, Replicated provides the same simple cluster management for your customer. Replicated is an application that runs within the Swarm cluster, and will additionally leverage the Docker Swarm API to provide cluster management support.\n\nSome of the standard Replicated features operate differently or are not supported on Swarm:\n\nNew YAML format\nThe Swarm scheduler requires a different YAML format which combines some of the Replicated YAML with your own  Docker Compose V3 YAML. For an example of how this is done see here: Swarm Voting App which is utilizing the kind: tag to designate Replicated and Swarm YAML sections.\n\nkind: scheduler-swarm\nversion: \"3.1\"\n\nExternal Private Images\nExternal private images are not supported currently. Replicated hosts a private registry that you can use to ship private images. Replicated also supports public (unauthenticated) images in any registry.\n\nAirgapped Installations\nAirgapped installations work as expected when running in swarm mode. All images included in your swarm application must be specified in the new images section of your YAML in order to be included in the airgap bundle your customer will download. See below for an example.\n\nimages:\nname: redis\n  tag: 3.2-alpine\n  source: public\nname: postgres\n  tag: 9.4\n  source: public\nname: example-voting-app-vote\n  tag: good\n  source: replicated\n\nReplicated Auto Updates\nReplicated auto updates work as expected when running in Swarm mode. While the Replicated update is applying, the UI will not be available. Once it finishes, refresh the UI to get the update.\n\nSnapshots\nStandard Replicated snapshots are not supported when running in Swarm mode. This functionality will be included in an upcoming release.\n\nCustom Preflight Checks\nCustom preflight checks are not currently supported when running in Swarm mode. These will be available in a future release.\n\nAdmin Commands\nAdmin commands are fully supported when running in Swarm mode. Your yaml will need to specify a Swarm service in which to run the admin command. If multiple containers are part of the service then replicated will choose a random container in which to run the command. See the example below:\n\nproperties:\n  shell_alias: mycli\nadmin_commands:\nalias: redis-sadd\n  command: [redis-cli, sadd]\n  run_type: exec\n  service: redis\n\nDashboard Metrics\nWhen running Replicated in Swarm mode, the standard statsd endpoint is still running. The only difference here is that the standard CPU and Memory usage graphs will not be available and will be included in an upcoming release. You can use the custom metrics feature to define you own application-specific metrics to show on the admin console dashboard.\n\nReady State/Health Checks\nReplicated will consider the application running when all replicas of the Swarm services are running. Ready state functionality is not currently supported when running in Swarm mode. This functionality will be included in an upcoming release.\n\nTemplate Functions\nThere are some additional template functions available when running in Swarm mode.\n\nSecrets\nReplicated supports secrets through the use of template functions. It is possible to request a secret from the user using a combination of config settings and the ConfigOption template function. For more information on configuring the replicated settings screen see the docs on customizing the Admin Console settings page. See below for an example of creating a secret in your application.\n\nkind: replicated\n...\nconfig:\nname: secrets\n  title: Secrets\n  items:\n  name: configmysecret\n    title: My Secret\n    type: password\n...\nswarm:\n  secrets:\n  name: my_secret\n    value: '{{repl ConfigOption \"configmysecret\" }}'\n    labels:\n      foo: bar\n      baz:\n\nkind: scheduler-swarm\nversion: \"3.1\"\nservices:\n  redis:\n    image: redis:latest\n    deploy:\n      replicas: 1\n    secrets:\n      my_secret\nsecrets:\n  my_secret:\n    external: true\n",
        "title": "Docker Swarm",
        "description": "Packaging a Docker Swarm application in Replicated",
        "weight": "219",
        "categories": [
            "Packaging"
        ],
        "aliases": [
            "/packaging-an-application/swarm/"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/events-and-orchestration.md",
        "content": "\nEvents are provided to help with the startup, orchestration and service discovery between your containers. There are\nseveral reasons to use events in your application yaml:\n\nWhen one container must be started and in a running state before another container starts\nWhen one container depends on the ip address or exposed ports of another container\n\nReplicated provides this functionality in a pub/sub style model. Containers can publish events and list the subscriptions.\nWhen the event is fired, the subscribed event starts.\n\nPublished Events\nContainers can publish a message when specific events occur. These events can be subscribed to by other containers in your\napplication and are designed to be used to help manage the state of your application. Some of these events are published\nwhen a container changes state. These events are published as soon as the Replicated operator reports that\nthe container state has changed.\n\nWhen setting up event orchestration use unique event names. When starting a container with constraints from multiple parents, the first event to fire causes the container to start.\n\nContainer Started Event\nA container-start event is published as soon as the Docker Engine reports that your container is started. The container\nmay still be initializing and loading, but control of the process has been transferred to the ENTRYPOINT or CMD\nspecified in the container. When this event is received, all of the template functions are available for this container.\n\nExample of a container-start event\ncontainers:\n  source: public\n    image_name: redis\n    version: 3.0.5\n    publish_events:\n    name: Redis started\n      trigger: container-start\n      subscriptions:\n      component: App\n        container: app\n        action: start\n\nContainer Stopped Event\nWhen a container-stop event is published, your container has been terminated. It may restart, but the current state is stopped. A\ncontainer-start event will be published if the container is ever restarted.\n\nContainer Port Listen Event\nWhen a port-listen event is published, your container has started accepting connections on an exposed public port. This can be useful\nin cases where you need to know when a specific service inside your container is actually up and running. The port you're interested in\nis specified in the data field, e.g. data: \"3306\"\n\nExample of a port-listen event\ncontainers:\n  source: public\n    image_name: mysql\n    version: 5.7\n    publish_events:\n    name: Mysql ready\n      trigger: port-listen\n      data: \"3306\"\n      subscriptions:\n      component: App\n        container: app\n        action: start\n\nContainer Exec Event\nThe exec event type is provided to execute arbitrary scripts in a container. This command is attempted immediately after the\ncontainer starts, and again at an interval of 2 seconds until it returns 0 or 10 minutes has elapsed. If the command succeeds\nwith a result code of 0, the event triggers. If not, the command will retry for up to 10 minutes and result in a failure if\nthe exit code is never 0. The command to be executed is given in the args field as the arguments are represented as an array\nof strings.\n\ncontainers:\n  source: public\n    image_name: mysql\n    version: 5.7\n    publish_events:\n    name: Minecraft Server Started\n      trigger: exec\n      args: [\"grep\", \"Done\", \"/data/logs/latest.log\"]\n      subscriptions:\n      component: Redis\n        container: redis\n        action: start\n\nSubscribed Events\nAny container can subscribe to events from any other container in your application. A subscribed event must also define an action to\ntake upon triggering of the event. The only action which is currently available is the start action, which causes the subscribing\ncontainer to start.\n\nTimeouts\n{{}} The timeout parameter must be an integer and indicates the number of seconds an event has to execute before a timeout is initiated.\nIf the event does not execute before the timeout is reached, then an error will show in the UI and the event sequence will terminate.\nBy default the timeout option will be set to 10 minutes.  Setting the timeout parameter to -1 will disable the timeout feature.\n\ncontainers:\n  -source: public\n    image_name: example\n    publish_events:\n    name: Some Event Started\n      trigger: container-stop\n      timeout: 30\n      subscriptions:\n      component: DB\n        container: mysql\n        action: start\n",
        "title": "Events and Orchestration",
        "description": "The `events` section of the Replicated YAML allows application vendors to sequence and orchestrate containers based on events from other containers.",
        "weight": "207",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/index.md",
        "content": "\nThe first step to shipping your application in Replicated is to create a YAML file that defines\nthe properties, containers, optional configuration and more.  This section of docs explains how\nto write this YAML file.\n",
        "title": "Packaging an Application",
        "description": "An overview of the Replicated YAML.",
        "weight": "201",
        "type": "section",
        "categories": [
            "Packaging"
        ],
        "hideSection": true
    },
    {
        "date": "2017-06-16T00:00:00Z",
        "uri": "/content/docs/packaging-an-application/kubernetes.md",
        "content": "\nIf your application is defined as a set of Kubernetes resources, and your customer can run a Kubernetes cluster, Replicated can provide the same standard functionality but on top of a Kubernetes cluster. To see a full example, check out the Guestbook example.\n\nDifferences from the Replicated scheduler\nUnlike the standard Replicated scheduler, when shipping a Kubernetes application on Replicated, it's expected that your customer will supply and maintain the cluster. Replicated is an application that runs on top of the cluster, and does not provide cluster management support.\n\nSome of the standard Replicated features operate differently or are not supported on Kubernetes:\n\nReplicated Private Images\nImages stored in the Replicated private registry can be accessed by adding a static imagePullSecret to any container definition that references a private image. Replicated will automatically create a secret named replicatedregistrykey and deploy it with your application. Refererencing this secret will make your private images available on the target cluster.\n\nExternal Private Images\nExternal private images are not supported currently. Replicated hosts a private registry that you can use to ship private images. Replicated also supports public (unauthenticated) images in any registry.\n\nReplicated Auto Updates\nReplicated auto updates work as expected when running in Kubernetes. While the Replicated update is applying, the UI will not be available. Once it finishes, refresh the UI to get the update.\n\nSnapshots\nStandard Replicated snapshots are not supported when running in Kubernetes. This functionality will be included in an upcoming release.\n\nPreflight Checks\nThere is limited support for preflight checks as of {{}}. See the Kubernetes Preflight Checks section of the docs for more details. Additional support will be available in a future release.\n\nAdmin Commands\nAdmin commands are supported on Kubernetes. Replicated uses Kubernetes selectors to identify the target pod in which to run the admin command. If multiple pods match the selector then replicated will choose a random pod in which to run the command. Specifying a container is optional as well. If no container is specified the first in the container in the pod will be chosen. See below for an example command.\n\nadmin_commands:\nalias: redis-sadd\n  command: [redis-cli, sadd]\n  run_type: exec\n  selector:\n    app: redis\n    tier: backend\n    role: master\n  container: master # optional, will choose first in pod\n\nDashboard Metrics\nWhen running Replicated in Kubernetes, the standard statsd endpoint is still running. The only difference here is that the standard CPU and Memory usage graphs will not be available. You can use the custom metrics feature to define you own application-specific metrics to show on the admin console dashboard.\n\nReady State/Health Checks\nReplicated will consider the application running when all of the Kubernetes resources are running. Different resources types have various methods to determine when they are started.\n\n| Resource Type | Replicated Considers Running When... |\n|-----|----|\n| Deployments | Same as [Kubernetes rollout status] (https://kubernetes.io/docs/user-guide/kubectl/kubectlrolloutstatus/) |\n| Replication Controller | Same as Kubernetes rollout status |\n| Persistent Volume Claim | When the claim is bound |\n| Service | When type is set to LoadBalancer, it's running when the IP address is assigned. |\n| Ingress | When the LoadBalancer IP is assigned. |\n| Pod | Deployed pods are not monitored. The higher level object is. |\n| Job | Jobs are not expected to stay running and are not monitored. |\n\nTemplate Functions\nThere are some additional template functions available when running in Kubernetes.\n\nSecrets\nReplicated supports runtime secrets through the use of template functions. It is possible to request a secret from the user using a combination of config settings and the ConfigOption template function. For more information on configuring the replicated settings screen see the docs on customizing the Replicated Admin Console settings page. See below for an example of creating a secret in your application.\n\nFor example:\nkind: replicated\n...\nconfig:\nname: auth\n  title: Authentication\n  items:\n  name: config_username\n    title: Username\n    type: password\n  name: config_password\n    title: Password\n    type: password\n\nkind: scheduler-kubernetes\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\n  labels:\n    app: guestbook\ndata:\n  username: {{repl ConfigOption \"config_username\" | Base64Encode }}\n  password: {{repl ConfigOption \"config_password\" | Base64Encode }}\n\nReplicated CLI\nThe Replicated CLI can be run via the kubectl exec command. See below for an example of running a Replicated CLI command.\n\nkubectl exec -it \"$(kubectl get pods -l=app=replicated -o=jsonpath='{.items..metadata.name}')\" -c replicated -- replicated apps\n\nAdvanced Replicated Configuration\nReplicated can be deployed into your kubernetes cluster or it can be deployed independently with the following environment variables:\n\nSCHEDULER_ENGINE=kubernetes\nK8SOVERRIDECLUSTER_CONFIG=true\nK8S_HOST=https://10.10.10.10\nK8STLSINSECURE=true\nK8S_USERNAME=admin\nK8S_PASSWORD=password\nK8SCLIENTCERT_DATA=\"-----BEGIN CERTIFICATE-----…\"\nK8SCLIENTKEY_DATA=\"-----BEGIN RSA PRIVATE KEY-----…\"\nK8SCLUSTERCACERTDATA=\"-----BEGIN CERTIFICATE-----\"\n\nLoad Balancers and Ingress\nOnly some environments (typically cloud providers) have support for the Service resource type LoadBalancer. An Ingress resource is recommended for more broad support for allowing inbound connections to the cluster. Replicated does not provide an Ingress controller and therefore one must be included in your application yaml. For more details on Ingress see https://kubernetes.io/docs/user-guide/ingress/.\n",
        "title": "Kubernetes",
        "description": "Packaging a Kubernetes application in Replicated",
        "weight": "218",
        "categories": [
            "Packaging"
        ],
        "aliases": [
            "/kb/developer-resources/kubernetes-prerelease/"
        ]
    },
    {
        "date": "2016-07-07T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/ldap-integration.md",
        "content": "\nReplicated can be integrated with LDAP servers to provide real time user authentication & sync. A quick overview of this feature is available. Announcement: DS Auth & Sync Support\n\nConfiguration\nAt the root level, configure the identity object\n\nidentity:\n  enabled: '{{repl if ConfigOptionNotEquals \"authsource\" \"authtype_internal\"}}true{{repl else}}false{{repl end}}'\n  provisioner: 'http://{{repl NodePrivateIPAddress \"MyContainerName\" \"Container Image Name\"}}:6006'\n  sources:\n  source: ldap\n    enabled: '{{repl if ConfigOptionEquals \"authsource\" \"authtype_ldap\"}}true{{repl else}}false{{repl end}}'\n\n| Field |\tDescription |\n|-------|-------------|\n| enabled | This should be copied as shown in the example. The value will depend on the settings provided below. |\n| provisioner | (Optional) Host that provides provisioning API for synchronization with LDAP. This field can be omitted if synchronization is not needed. |\n| sources | Only ldap source is supported at this time. Leave this setting as shown in the example. |\n\nIn the config section, add LDAP server configuration\n\n{{}}\nSetting labels can be customized if needed. However, setting names must remain exactly as shown in this example.\n{{}}\n\nname: auth\n  title: Authentication\n  description: Where will user accounts be provisioned\n  items:\n  name: auth_source\n    default: authtypeinternal\n    type: select_one\n    items:\n    name: authtypeinternal\n      title: Built In\n    name: authtypeldap\n      title: LDAP\nname: ldap_settings\n  title: LDAP Server Settings\n  when: authsource=authtype_ldap\n  test_proc:\nOptional.\nWhen defined, the Test button will be shown on the LDAP settings section which will allow validating\nthe supplied credentials.  When this is enabled ldaploginusername and ldaploginpassword must be defined.\n    display_name: Test Credentials\n    command: ldap_auth\n    arg_fields:\n    ldap_type\n    ldap_hostname\n    ldap_port\n    ldap_encryption\n    ldapsearchuser\n    ldapsearchpassword\n    ldapbasedn\n    ldapusersearchdn\n    ldaprestricteduser_group\n    ldapusernamefield\n    ldaploginusername\n    ldaploginpassword\n    ldapadvancedsearch\n    ldapuserquery\n    ldaprestrictedgroup_query\n  items:\n  name: ldap_type\nLDAP server type.  All standard LDAP implementations are supported.\nIn order to use Provisioning API, the LDAP server (AD being an exception) must support the Content Sync feature.\n    title: LDAP Server Type\n    type: select_one\n    default: ldaptypeopenldap\n    items:\n    name: ldaptypeopenldap\n      title: OpenLDAP\n    name: ldaptypead\n      title: Active Directory\n    name: ldaptypeother\n      title: Other\n  name: ldap_hostname\nLDAP host name without port or protocol.  Example: ad.bigbank.com\n    title: Hostname\n    type: text\n    value: '{{repl LdapCopyAuthFrom \"Hostname\"}}'\n    required: true\n  name: ldap_port\nLDAP port number.  This can be different for different encryption types.\n    title: Port\n    type: text\n    value: '{{repl LdapCopyAuthFrom \"Port\"}}'\n    default: 389\n    required: true\n  name: labelencryptionlabel\nEncryption type.  Self-signed certificates are not supported at this time when LDAPS or StartTLS is selected.\n    title: Encryption Type\n    type: label\n  name: ldap_encryption\n    type: select_one\n    default: ldapencryptionplain\n    items:\n    name: ldapencryptionplain\n      title: Plain\n    name: ldapencryptionstarttls\n      title: StartTLS\n    name: ldapencryptionldaps\n      title: LDAPS\n  name: ldapsearchuser\nDN of the user with search privileges.\n    title: Search user\n    type: text\n    value: '{{repl LdapCopyAuthFrom \"SearchUsername\"}}'\n    required: true\n  name: ldapsearchpassword\nThe password to use to login as the Search User.\n    title: Search password\n    type: password\n    value: '{{repl LdapCopyAuthFrom \"SearchPassword\"}}'\n    required: true\n  name: ldap_schema\n    type: heading\n    title: LDAP schema\n  name: ldapbasedn\nBase DN for user search.  Example: DC=ad,DC=bigbank,DC=com\n    title: Base DN\n    type: text\n    value: '{{repl LdapCopyAuthFrom \"BaseDN\"}}'\n    required: true\n  name: ldapusersearchdn\nUser search DN.  Together with Base DN, it should form a valid search DN: ,\n    title: User search DN\n    type: text\n    value: '{{repl LdapCopyAuthFrom \"UserSearchDN\"}}'\n    default: ou=users\n    required: true\n  name: ldapadvancedsearch\nThis option must be selected in order to use advanced search features.   Otherwise, it can be omitted.\n    title: Show Advanced Search Options\n    description: Enable this option if you need to write a custom LDAP search query.\n    type: bool\n    value: 0\n  name: ldaprestricteduser_group\nGroup name that the user must belong to.  This string must only contain the group name\nwithout an LDAP search query.\n    title: Restricted User Group\n    type: text\n    value: '{{repl LdapCopyAuthFrom \"RestrictedGroupCNs\"}}'\n    required: false\n    when: ldapadvancedsearch=0\n  name: ldapuserquery\nLDAP query that should be used to lookup users.  The query should contain a {{username}} placeholder that\nwill be replaced with the actual user name that is being looked up.\nExample: (sAMAccountName={{username}})\n    title: User Query\n    type: text\n    value: '{{repl LdapCopyAuthFrom \"UserQuery\"}}'\n    required: false\n    when: ldapadvancedsearch=1\n  name: ldaprestrictedgroup_query\nLDAP query that should be used to validate user group membership.\nThe query can container two placeholders:\n{{userdn}} - this placeholder will be replaced with the user's DN value.\n{{username}} - this placeholder will be replaced with the user name\nExample: (|(&(cn=Accounting)(memberuid={{username}})))\n    title: Restricted User Group Query\n    type: text\n    value: '{{repl LdapCopyAuthFrom \"RestrictedGroupQuery\"}}'\n    required: false\n    when: ldapadvancedsearch=1\n  name: ldapusernamefield\nThe LDAP field that will contain the user name.  Typically it will be uid.\nActive Directory uses sAMAccountName.\n    title: Username field\n    type: text\n    value: '{{repl LdapCopyAuthFrom \"FieldUsername\"}}'\n    default: uid\n    required: true\n  name: ldaploginusername\nOptional user name filed.  This can be used with test_proc to validate credentials before saving.\n    title: Test username\n    type: text\n    required: false\n  name: ldaploginpassword\nOptional password filed.  This can be used with test_proc to validate credentials before saving.\n    title: Test password\n    type: password\n    required: false\n\nNote the use of the LdapCopyAuthFrom function. This is optional, but when LDAP is used to secure the Replicated console, settings entered on that screen will be copied as default values.\n\nIdentity API\nSee Identity API for information on how to authenticate and sync with LDAP server.\n",
        "title": "LDAP and Identity Integration",
        "description": "Enabling LDAP and AD user auth and sync in an application through Replicated.",
        "weight": "216",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/preflight-checks-k8s.md",
        "content": "\nSupport for Kubernetes preflight checks has been added as of Replicated {{}}.\n\nBy default, Replicated automatically adds preflight checks for:\n\n| Category | Check |\n|--------------|-----------|\n| Outbound internet access (if required) | Replicated APIs, external registries |\n\nAdditional Kubernetes system requirements can be specified in the kubernetes.requirements section of the application YAML.\n\nPossible checks include:\n\n| Property | Check |\n|--------------|-----------|\n| server_version | Kubernetes server version (must be specified as a semver range) |\n| api_versions | Supported API versions on the server (in the form of \"group/version\") |\n| cluster_size | Minumum cluster size (nodes) |\n| total_cores | Minumum total cores |\n| total_memory | Minumum total memory |\n\nExample:\n\nkubernetes:\n  requirements:\n    server_version: \"=1.6.0\"\n    api_versions: [\"apps/v1beta1\"]\n    cluster_size: 3\n    total_cores: 3\n    total_memory: 11.25GB\n",
        "title": "Kubernetes Preflight Checks",
        "description": "A guide to implementing the Kubernetes Preflight Checks feature to analyze customer systems to determine if the environment meets the minimum requirements for installation or update.",
        "weight": "213",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/preflight-checks.md",
        "content": "\nA preflight check is a test that is run before installing and running an application.  The test will analyze the system to determine if the environment meets the minimum requirements.\n\nThe preflight check may be manually run for an existing installation by visiting https://&lt;your server address&gt;:8800/run-checks\n\nBy default, Replicated automatically adds preflight checks for:\n\n| Category | Check |\n|--------------|-----------|\n| OS | Linux |\n| Distribution | Supported Linux distributions |\n| Linux Kernel | 3.10 or greater |\n| Memory | 1 GB |\n| Docker Version | {{}} - {{}} |\n| Disk Space | /tmp 1 GB  /var/lib/replicated 250 MB  /var/lib/docker/aufs 1 GB (aufs storage driver root directory) |\n| TCP Ports (Replicated services) | 9870-9880 on docker0 |\n| Outbound internet access (if required) | Replicated APIs, external registries |\n\nAdditionally, it's recommended to specify additional system requirements in the host_requirements section of the\napplication YAML. These host requirements will apply to single node installs, as well as each node on distributed\ninstalls.\n\nname: My Counter App\nhost_requirements:\n  docker_version: \"1.10.3\"\n  cpu_cores: 2\n  cpu_mhz: 2400\n  memory: 8GB\n  disk_space: 8GB\n  replicatedversion: \"=2.3.0 }} The application level hostrequirements key can be used to automatically upgrade Replicated.  This feature can be enabled by specifying a version range in the the replicated_version key.  Version range syntax is similar to that used by npm.  Versions that don't support this feature will simply ignore the value.  This key is also ignored by the pre-flight checks.\n\nIt is possible to override all properties (except docker version) of the root host_requirements on a per-component basis. On distributed installs, the component host requirements will only apply to nodes tagged for that component.\n\ndocker_version refers to the lowest acceptable version of docker on the host. Any host running a docker version at or above this value will meet the requirement.\n\ncomponents:\nname: DB\n  tags: [\"db\"]\n  host_requirements:\n    cpu_cores: 2\n    cpu_mhz: 2400\n    memory: 8GB\n    disk_space: 30GB\n\nNote that component host requirements are not additive, thus when multiple components are allocated to a single host, each requirement\nwill be evaluated individually.\n\nIt is also possible to define minimum disk space requirements for volumes on the host machine via the host_volumes property of the\ncomponent configuration.\n\ncomponents:\nname: DB\n  tags: [\"db\"]\n  host_volumes:\n  host_path: /data\n    mindiskspace: 30GB\n  containers:\n  image_name: redis\n\nReplicated enforces these requirements and will not allow the customer to start the application without either meeting these requirements or\ndismissing the warnings. Upon dismissing preflight warnings, an entry will be recorded in the on-premise audit log.\n\n{{% page_notes %}}\nmindiskspace does not guarantee free space, it refers to the disk size mounted at the specified location.\nThe requested docker_version must be one of the versions Replicated supports.\n{{% /page_notes %}}\n",
        "title": "Preflight Checks",
        "description": "A guide to implementing the Preflight Checks feature to analyze customer systems to determine if the environment meets the minimum requirements for installation or update.",
        "weight": "213",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-07T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/snapshots.md",
        "content": "\nFor detailed information on restoring a snapshot take a look at this restore guide.\n\nReplicated gives customers the ability to take a snapshot of a running app. The customer will have the option to restore this snapshot as an option on the \"Upload license\" screen when starting the Replicated management container. Snapshots can be taken at an automatic interval and can also be manually triggered via the dashboard of the console.\n\nSnapshots include customer console configuration, data from bind mounted volumes of all containers and if the customer instance is a multi-host instance, docker registry data will be backed up as well. You also have the ability to specify a custom script that will be run at the time of a backup. This script will run on the host that is running the primary Replicated container, not inside any of your containers. If you need this script to execute something within a container, you can call Admin Commands.\n\nYAML Properties\nenabled: A boolean or template function that evaluates to a boolean and then determines if backups are enabled.\n\npause_containers: A string that can equal \"true\" or \"false\". If true, Replicated will pause all containers and then resume them upon completion (note your app will potentially have downtime). Take a look at this article for tips on zero downtime backups.\n\nscript: A bash script that will run on the server at the time of backup.\n\nhidden: Defaults to false (the snapshot tile is visible by default).\n\nbackup:\n  enabled: '{{repl ConfigOptionEquals \"backup_enabled\" \"1\" }}'\n  hidden: '{{repl ConfigOptionEquals \"backup_enabled\" \"0\" }}'\n  pausecontainers: '{{repl LicenseFieldValue \"zerodowntimebackupsenabled\" }}'\n  script: |\n    #!/bin/sh\n    replicated admin --no-tty backup\n\nTurn off Snapshotting by Volume\nYou can exclude a volume from being snapshotted by using the isexcludedfrom_backup variable inside your container YAML.\n\nWe recommend that you exclude anything that's not necessary to restore the running system.\n\nname: DB\n  containers:\n  source: public\n    image_name: redis\n    version: latest\n    cmd: \"[\\\"redis-server\\\", \\\"--appendonly\\\", \\\"yes\\\"]\"\n    volumes:\n    host_path: /data\n      container_path: /data:rw\n      isexcludedfrom_backup: true\n\nCustomer Snapshot Configuration Options\nIf snapshots are enabled for an application, end customers can configure the destination, retention, timeout and schedule automated snapshots on the Console Settings screen.\n\n{{}}\nThe default location for saving a snapshot on a Replicated enabled host is\n/var/lib/replicated/snapshots.   This location may not be suitable for keeping\nlarge amounts of data.  Additionally, by default, it is likely to be on the same physical volume as all other critical data.  We highly recommend this location is configured to be\non a separate volume (possibly a SAN) with large capacity to ensure data can be\nrecovered in case of a disaster.\n{{}}\n",
        "title": "Snapshots",
        "description": "Guide to enabling application snapshots for backup and restore functionality.",
        "weight": "215",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/support-bundle.md",
        "content": "\nA support bundle is an archive that is available for the customer to download via the Support tab of the On-Prem Console.\n\nContents of the support page can be customized by including markdown in the top-level of the YAML.\n\nreplicatedapiversion: \"{{}}\"\nname: ELK\nconsolesupportmarkdown: |\nEmail Us for help:\nsupport@getelk.com\n  Or don't, your loss.\n\nThe support bundle has a default timeout of 120 seconds, after which only files and commands that have completed will be included in the downloaded bundle. A custom timeout in seconds can be specified in the support section of the yaml.\n\nsupport:\n  timeout: 300\n\nCustom Files and Commands\n\nIn addition to the default support files included in the support bundle, addtional files can be added via the support section of your yaml. Files from within the application’s containers can be included, as well as output of commands executed in the container. Support files and commands are supported by both the native and kubernetes schedulers. For more complex support commands it is possible to create a config file and execute that file from a support command. These files will be available withing the /scheduler directory of the support bundle.\n\nsupport:\n  files:\n    filename: /var/log/nginx/access.log\n      source:\n        replicated:\n          component: Nginx\n          container: my-nginx\n        kubernetes:\n          selector:\n            run: my-nginx\n  commands:\n    filename: accesslast1000.log\n      command: [tail, -n1000, /var/log/nginx/access.log]\n      source:\n        replicated:\n          component: Nginx\n          container: my-nginx\n        kubernetes:\n          selector:\n            run: my-nginx\n\nDefault Support Files\n\nBy default the support bundle will include the following files:\n\n| File | Description |\n|------|-------------|\n| /daemon/auditlogs/* | Audit log events. |\n| /daemon/commands/date | Result of the command date. Print the system date and time. |\n| /daemon/commands/df | Result of the command df -al. Report file system disk space usage for the local file systems. |\n| /daemon/commands/df_inodes | Result of the command df -ali. Report file system inode usage for the local file systems. |\n| /daemon/commands/dmesg | Result of the command dmesg. Print the kernel ring buffer. |\n| /daemon/commands/free | Result of the command free -m. Display amount of free and used memory in the system. |\n| /daemon/commands/hostname | Result of the command hostname. Show the system's host name. |\n| /daemon/commands/ipaddrshow | Result of the command ip -o addr show. Show protocol (IP or IPv6) addresses on a device. |\n| /daemon/commands/iplinkshow | Result of the command ip -o link show. Show network devices. |\n| /daemon/commands/iprouteshow | Result of the command ip -o route show. Show routing table entries. |\n| /daemon/commands/ps | Result of the command ps fauxwww. Report a snapshot of the current processes. |\n| /daemon/commands/uptime | Result of the command uptime. Tell how long the system has been running. |\n| /daemon/docker/docker_info.json | Display system-wide information. |\n| /daemon/docker/dockerpsa.json | Result of the command docker ps -a. List all containers. |\n| /daemon/etc/centos-release | A copy of the /etc/centos-release file. Contain operating system identification data for centos distribution. |\n| /daemon/etc/default/docker | A copy of the /etc/default/docker file. Upstart docker configuration. |\n| /daemon/etc/default/replicated | A copy of the /etc/default/replicated file. Upstart replicated configuration. |\n| /daemon/etc/default/replicated-operator | A copy of the /etc/default/replicated-operator file. Upstart replicated-operator configuration. |\n| /daemon/etc/hostname | A copy of the /etc/hostname file. The system's host name. |\n| /daemon/etc/hosts | A copy of the /etc/hosts file. Static table lookup for hostnames. |\n| /daemon/etc/os-release | A copy of the /etc/os-release file. Contain operating system identification data. |\n| /daemon/etc/replicated.conf | A copy of the /etc/replicated.conf file. Replicated legacy 1.x configuration. |\n| /daemon/etc/sysconfig/docker | A copy of the /etc/sysconfig/docker file. Legacy systemd docker configuration. |\n| /daemon/etc/sysconfig/replicated | A copy of the /etc/sysconfig/replicated file. Systemd replicated configuration. |\n| /daemon/etc/sysconfig/replicated-operator | A copy of the /etc/sysconfig/replicated-operator file. Systemd replicated-operator configuration. |\n| /daemon/etc/system-release | A copy of the /etc/system-release file. Contain operating system identification data. |\n| /daemon/etc/systemd/system/docker.service.d/http-proxy.conf | A copy of the /etc/systemd/system/docker.service.d/http-proxy.conf file. Systemd docker proxy configuration. |\n| /daemon/etc/timezone | A copy of the /etc/timezone file. The system's timezone. |\n| /daemon/journald/replicated.log | Result of the command journalctl -u replicated file. Journald replicated logs. |\n| /daemon/journald/replicated-operator.log | Result of the command journalctl -u replicated-operator file. Journald replicated-operator logs. |\n| /daemon/journald/replicated-ui.log | Result of the command journalctl -u replicated-ui file. Journald replicated-ui logs. |\n| /daemon/proc/cpuinfo | A copy of the /proc/cpuinfo file. Information about the processor, such as its type, make, model, and performance. |\n| /daemon/proc/meminfo | A copy of the /proc/meminfo file. Information about memory usage, both physical and swap. |\n| /daemon/proc/mounts | A copy of the /proc/mounts file. Mounted filesystems. |\n| /daemon/proc/uptime | A copy of the /proc/uptime file. The time the system has been up. |\n| /daemon/proc/version | A copy of the /proc/version file. The kernel version. |\n| /daemon/proc/vmstat | A copy of the /proc/vmstat file. Detailed virtual memory statistics from the kernel. |\n| /daemon/replicated/config-commands.txt | A list of all configuration test commands that were run and the results. |\n| /daemon/replicated/daemon.json | Daemon properties and runtime configuration. |\n| /daemon/replicated/ledis-app.dump | A dump of the Replicated database. |\n| /daemon/replicated/ledis-registry.dump | A dump of the Replicated registry database. |\n| /daemon/replicated/params.json | Daemon runtime configuration. |\n| /daemon/replicated/replicated-inspect.json | Result of the command docker inspect replicated. Return low-level information on the replicated container. |\n| /daemon/replicated/replicated-operator-inspect.json | Result of the command docker inspect replicated-operator. Return low-level information on the replicated-operator container. |\n| /daemon/replicated/replicated-operator.log | Result of the command docker logs replicated-operator --tail 10000. Docker replicated-operator container logs. |\n| /daemon/replicated/replicated-ui-inspect.json | Result of the command docker inspect replicated-ui. Return low-level information on the replicated-ui container. |\n| /daemon/replicated/replicated-ui.log | Result of the command docker logs replicated-ui --tail 10000. Docker replicated-ui container logs. |\n| /daemon/replicated/replicated-versions.txt | A list of all running replicated components and their versions. |\n| /daemon/replicated/replicated.log | Result of the command docker logs replicated --tail 10000. Docker replicated container logs. |\n| /daemon/replicated/runtime/goroutines.txt | Stack traces of all current goroutines. |\n| /daemon/replicated/tasks.txt | A list of all current tasks: queued, executing, or sleeping. |\n| /daemon/var/log/upstart/docker.log | A copy of the /var/log/upstart/docker.log file. Upstart docker logs. |\n| /daemon/var/log/upstart/replicated-operator.log | A copy of the /var/log/upstart/replicated-operator.log file. Upstart replicated-operator logs. |\n| /daemon/var/log/upstart/replicated-ui.log | A copy of the /var/log/upstart/replicated-ui.log file. Upstart replicated-ui logs. |\n| /daemon/var/log/upstart/replicated.log | A copy of the /var/log/upstart/replicated.log file. Upstart replicated logs. |\n| /scheduler/container/&lt;container_id&gt;/inspect | Result of the command docker inspect . Displays low-level information on a Docker container. |\n| /scheduler/container/&lt;container_id&gt;/stdout.log | Result of the command docker logs . Docker container logs stdout. |\n| /scheduler/container/&lt;container_id&gt;/stderr.log | Result of the command docker logs . Docker container logs stderr. |\n| /scheduler/container/&lt;container_id&gt;/files/* | Contains any custom container files as specified by the vendor application. |\n| /scheduler/container/&lt;container_id&gt;/commands/* | Contains any custom container commands as specified by the vendor application. |\n| /scheduler/node/&lt;node_id&gt;/commands/date | Result of the command date. Print the system date and time. |\n| /scheduler/node/&lt;node_id&gt;/commands/df | Result of the command df -al. Report file system disk space usage for the local file systems. |\n| /scheduler/node/&lt;nodeid&gt;/commands/dfinodes | Result of the command df -ali. Report file system inode usage for the local file systems. |\n| /scheduler/node/&lt;node_id&gt;/commands/dmesg | Result of the command dmesg. Print the kernel ring buffer. |\n| /scheduler/node/&lt;node_id&gt;/commands/free | Result of the command free -m. Display amount of free and used memory in the system. |\n| /scheduler/node/&lt;node_id&gt;/commands/hostname | Result of the command hostname. Show the system's host name. |\n| /scheduler/node/&lt;node_id&gt;/commands/ipaddrshow | Result of the command ip -o addr show. Show protocol (IP or IPv6) addresses on a device. |\n| /scheduler/node/&lt;node_id&gt;/commands/iplinkshow | Result of the command ip -o link show. Show network devices. |\n| /scheduler/node/&lt;node_id&gt;/commands/iprouteshow | Result of the command ip -o route show. Show routing table entries. |\n| /scheduler/node/&lt;node_id&gt;/commands/ps | Result of the command ps fauxwww. Report a snapshot of the current processes. |\n| /scheduler/node/&lt;node_id&gt;/commands/uptime | Result of the command uptime. Tell how long the system has been running. |\n| /scheduler/node/&lt;nodeid&gt;/docker/dockerinfo.json | Display system-wide information. |\n| /scheduler/node/&lt;node_id&gt;/docker/dockerpsa.json | Result of the command docker ps -a. List all containers. |\n| /scheduler/node/&lt;node_id&gt;/etc/centos-release | A copy of the /etc/centos-release file. Contain operating system identification data for centos distribution. |\n| /scheduler/node/&lt;node_id&gt;/etc/default/docker | A copy of the /etc/default/docker file. Upstart docker configuration. |\n| /scheduler/node/&lt;node_id&gt;/etc/default/replicated | A copy of the /etc/default/replicated file. Upstart replicated configuration. |\n| /scheduler/node/&lt;node_id&gt;/etc/default/replicated-operator | A copy of the /etc/default/replicated-operator file. Upstart replicated-operator configuration. |\n| /scheduler/node/&lt;node_id&gt;/etc/hostname | A copy of the /etc/hostname file. The system's host name. |\n| /scheduler/node/&lt;node_id&gt;/etc/hosts | A copy of the /etc/hosts file. Static table lookup for hostnames. |\n| /scheduler/node/&lt;node_id&gt;/etc/os-release | A copy of the /etc/os-release file. Contain operating system identification data. |\n| /scheduler/node/&lt;node_id&gt;/etc/replicated.conf | A copy of the /etc/replicated.conf file. Rplicated legacy 1.x configuration. |\n| /scheduler/node/&lt;node_id&gt;/etc/sysconfig/docker | A copy of the /etc/sysconfig/docker file. Legacy systemd docker configuration. |\n| /scheduler/node/&lt;node_id&gt;/etc/sysconfig/replicated | A copy of the /etc/sysconfig/replicated file. Systemd replicated configuration. |\n| /scheduler/node/&lt;node_id&gt;/etc/sysconfig/replicated-operator | A copy of the /etc/sysconfig/replicated-operator file. Systemd replicated-operator configuration. |\n| /scheduler/node/&lt;node_id&gt;/etc/system-release | A copy of the /etc/system-release file. Contain operating system identification data. |\n| /scheduler/node/&lt;node_id&gt;/etc/systemd/system/docker.service.d/http-proxy.conf | A copy of the /etc/systemd/system/docker.service.d/http-proxy.conf file. Systemd docker proxy configuration. |\n| /scheduler/node/&lt;node_id&gt;/etc/timezone | A copy of the /etc/timezone file. The system's timezone. |\n| /scheduler/node/&lt;node_id&gt;/proc/cpuinfo | A copy of the /proc/cpuinfo file. Information about the processor, such as its type, make, model, and performance. |\n| /scheduler/node/&lt;node_id&gt;/proc/meminfo | A copy of the /proc/meminfo file. Information about memory usage, both physical and swap. |\n| /scheduler/node/&lt;node_id&gt;/proc/mounts | A copy of the /proc/mounts file. Mounted filesystems. |\n| /scheduler/node/&lt;node_id&gt;/proc/uptime | A copy of the /proc/uptime file. The time the system has been up. |\n| /scheduler/node/&lt;node_id&gt;/proc/version | A copy of the /proc/version file. The kernel version. |\n| /scheduler/node/&lt;node_id&gt;/proc/vmstat | A copy of the /proc/vmstat file. Detailed virtual memory statistics from the kernel. |\n| /scheduler/node/&lt;node_id&gt;/scheduler/params.json | Operator runtime configuration. |\n| /scheduler/node/&lt;node_id&gt;/scheduler/replicated-operator-inspect.json | Result of the command docker inspect replicated-operator. Return low-level information on the replicated-operator container. |\n| /scheduler/node/&lt;node_id&gt;/scheduler/replicated-operator.log | Result of the command docker logs replicated-operator --tail 10000. Docker replicated-operator container logs. |\n| /scheduler/node/&lt;node_id&gt;/scheduler/runtime/goroutines.txt | Stack traces of all current goroutines. |\n| /scheduler/node/&lt;node_id&gt;/scheduler/var/lib/replicated-operator/logs/* | Archived vendor application container logs. |\n| /scheduler/node/&lt;node_id&gt;/scheduler/var/lib/replicated-operator/replicated-operator.conf | A copy of the /var/lib/replicated-operator/replicated-operator.conf file. Replicated operator generated configuration file. |\n| /scheduler/node/&lt;node_id&gt;/var/log/upstart/docker.log | A copy of the /var/log/upstart/docker.log file. Upstart docker logs. |\n| /scheduler/node/&lt;node_id&gt;/var/log/upstart/replicated-operator.log | A copy of the /var/log/upstart/replicated-operator.log file. Upstart replicated-operator logs. |\n| /scheduler/node/&lt;node_id&gt;/var/log/upstart/replicated-ui.log | A copy of the /var/log/upstart/replicated-ui.log file. Upstart replicated-ui logs. |\n| /scheduler/node/&lt;node_id&gt;/var/log/upstart/replicated.log | A copy of the /var/log/upstart/replicated.log file. Upstart replicated logs. |\n| /scheduler/nodes.txt | A list of all scheduler nodes. |\n",
        "title": "Support Bundle",
        "description": "Installed instances can generate a support bundle with relevant logs and instance information.",
        "weight": "212",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/template-functions.md",
        "content": "\nTemplate functions are marked by the double curly bracket + \"repl\" escape sequence. They allow for user input to be dynamically inserted into application configuration values. The sequence should be {{repl, not {{ repl.\n\nTemplate functions that refer to your containers are always addressed in pairs with \"component name\" and \"image name\".  You should use the full image name as it appears in your container definition.\n\nGo Templates\nReplicated uses Go's template engine to execute the following functions.  In addition to the functions listed here, all of the Go template runtime is available.  Please note that Go template functions must still be escaped with \"repl\" escape sequence as demonstrated below.\n\n{{repl if pipeline}} T1 {{repl else}} T0 {{repl end}}\n\nReplicated Template Functions\n\n{{}}\nfunc ConfigOption(optionName string) string\nReturns the value of the config option as a string.\nproperties:\n  app_url: http://{{repl ConfigOption \"hostname\" }}\n\n{{}}\n(only supports type: file)\n\nfunc ConfigOptionData(fileName string) string\nReturns the contents of the file uploaded for a configuration option as a string.\nconfig_files:\nfilename: /opt/certs/server.key\n  contents: {{repl ConfigOptionData \"ssl_key\"}}\n\n{{}}\nfunc ConfigOptionEquals(optionName string, expectedValue string) bool\nReturns true if the configuration option value is equal to the supplied value.\nports:\n   private_port: \"80\"\n     public_port: \"80\"\n     port_type: tcp\n     when: '{{repl ConfigOptionEquals \"http_enabled\" \"1\" }}'\n\n{{}}\nfunc ConfigOptionNotEquals(optionName string, expectedValue string) bool\nReturns true if the configuration option value is not equal to the supplied value.\nports:\n   private_port: \"443\"\n     public_port: \"443\"\n     port_type: tcp\n     when: '{{repl ConfigOptionNotEquals \"http_enabled\" \"1\" }}'\n\n{{}}\nfunc NodePrivateIPAddress(componentName string, imageName string) string\nReturns Private IP Address of a given Component as a string.\n\nenv_vars:\nname: REDISHOSTPRIVATE\n  static_val: '{{repl NodePrivateIPAddress \"DB\" \"redis\" }}'\nReplaces HostPrivateIpAddress which is deprecated.\n\n{{}}\nfunc NodePrivateIPAddressFirst(componentName string, imageName string) string\nReturns the first node's Private IP Address of a given Component as a string.\n\n{{}}\nfunc NodePrivateIPAddressAll(componentName string, imageName string) []string\nReturns node private IP addresses for all instances of a given Component as an array of strings.\nReplaces HostPrivateIpAddressAll which is deprecated.\n\nNote: ContainerExposedPortAll, NodePrivateIPAddressAll, NodePublicIPAddressAll are guaranteed to return in the same order\n\n{{}}\nfunc NodePublicIPAddress(componentName string, imageName string) string\nReturns Public IP Address of a given Component as a string.\nenv_vars:\nname: REDISHOSTPUBLIC\n  static_val: '{{repl NodePublicIPAddress \"DB\" \"redis\" }}'\nReplaces HostPublicIpAddress which is deprecated.\n\n{{}}\nfunc NodePublicIPAddressFirst(componentName string, imageName string) string\nReturns first node's public IP addresses for a given Component as a string.\n\n{{}}\nfunc NodePublicIPAddressAll(componentName string, imageName string) []string\nReturns node public IP addresses for all instances of a given Component as an array of strings.\nReplaces HostPublicIpAddressAll which is deprecated.\n\nNote: ContainerExposedPortAll, NodePrivateIPAddressAll, Node PublicIPAddressAll are guaranteed to return in the same order\n\n{{}}\nfunc ContainerExposedPort(componentName string, imageName string, internalPort string) string\nReturns the node's public port mapped to the supplied exposed container port as a string.\n\nenv_vars:\nname: REDIS_PORT\n  static_val: '{{repl ContainerExposedPort \"DB\" \"redis\" \"6379\" }}'\n\n{{}}\nfunc ContainerExposedPortFirst(componentName string, imageName string, internalPort string) string\nReturns the first node's public port mapped to the supplied exposed container port as a string.\n\nenv_vars:\nname: REDIS_PORT\n  static_val: '{{repl ContainerExposedPortFirst \"DB\" \"redis\" \"6379\" }}'\n\n{{}}\nfunc ContainerExposedPortAll(componentName string, imageName string, internalPort string) string\nReturns the node public port mapped to the supplied exposed container port for all instances of a given Component as an array of strings.\n\nNote: ContainerExposedPortAll, NodePrivateIPAddressAll, NodePublicIPAddressAll are guaranteed to return in the same order\n\n{{}}\nfunc LicenseFieldValue(customLicenseFieldName string) string\nReturns the value for the Custom License Field as a string.\nconfig_files:\n  filename: /opt/app/config.yml\n    contents: |\n      maxusers: '{{repl LicenseFieldValue \"maximumusers\" }}'\n\n{{}}\nfunc LicenseProperty(propertyName string) string\nReturns a property from the License as a string.  Valid propertyNames are \"assignee\", \"channel.name\", \"expiration.date\", and \"expiration.policy\".\nconfig_files:\n  filename: /opt/app/config.yml\n    contents: |\n      expiration.date: {{repl LicenseProperty \"expiration.date\"}}\n\n{{}}\nfunc AppID() string\nReturns the app id.\nenv_vars:\nname: APP_ID\n  static_val: '{{repl AppID }}'\n\n{{}}\nfunc AppVersion() int\nReturns the app version sequence.\nenv_vars:\nname: APP_VERSION\n  static_val: '{{repl AppVersion }}'\n\n{{}}\nfunc AppVersionFirst() int\nReturns the version sequence of the first version installed.\nenv_vars:\nname: APPVERSIONFIRST\n  static_val: '{{repl AppVersionFirst }}'\n\n{{}}\nfunc AppVersionCurrent() int\nReturns the current app version sequence.\nenv_vars:\nname: APPVERSIONCURRENT\n  static_val: '{{repl AppVersionCurrent }}'\n\n{{}}\nfunc RunOffline() bool\nReturns whether or not we are running in airgap mode. This is available in the Kubernetes and Swarm implementations, but will always return false.\nenv_vars:\nname: IS_AIRGAP\n  static_val: '{{repl RunOffline }}'\n\n{{}}\nfunc AppSetting(key string) string\nReturns a setting from the current app release.\n\nPossible Options:\nversion.label\nrelease.notes\nrelease.date\ninstall.date\nrelease.channel\n\nenv_vars:\nname: VERSION\n  static_val: '{{repl AppSetting \"version.label\"}}'\nname: RELEASE_NOTES\n  static_val: '{{repl AppSetting \"release.notes\"}}'\nname: INSTALL_DATE\n  static_val: '{{repl AppSetting \"install.date\"}}'\nname: RELEASE_DATE\n  static_val: '{{repl AppSetting \"release.date\"}}'\nname: RELEASE_CHANNEL\n  static_val: '{{repl AppSetting \"release.channel\"}}'\n\n{{}}\nfunc ConsoleSetting(consoleSettingName string) string\nReturns customer defined console settings for the TLS data or proxy settings. Values are returned as a string.\n\n|Option|Returned Value|\n|---|-----------|\n|tls.key.name|TLS key filename|\n|tls.key.data|TLS key contents|\n|tls.cert.name|TLS cert filename|\n|tls.cert.data|TLS cert contents|\n|tls.hostname|Hostname used to secure Replicated TLS traffic|\n|tls.source|Source of the TLS cert, either \"self-signed\", \"key-cert\" or \"server-path\"|\n|http.proxy|Proxy http address (e.g. http://10.128.0.4:3128)|\n|http.proxy.enabled|Proxy is enabled when value is 1, not enabled when it is 0|\n\nconfig:\nname: console_info\n  title: Console Info\n  items:\n  name: key_filename\n    type: text\n    readonly: true\n    value: '{{repl ConsoleSetting \"tls.key.name\"}}'\n\n{{}}\nfunc ConsoleSettingEquals(name string, value string) bool\nReturns a bool indicating if the value is the currently applied value for ConsoleSetting with name.\n\n{{}}\nfunc ConsoleSettingNotEquals(name string, value string) bool\nReturns a bool indicating if the value is not the currently applied value for ConsoleSetting with name.\n\n{{}}\nDeprecated, please use ThisNodePublicIPAddress, ThisNodePrivateIPAddress or ThisNodeDockerAddress instead.\nfunc ThisHostInterfaceAddress(interfaceName string) string\nReturns the valid IPv4 address associated with the given network interface of the host on which the current container instance is deployed as a string. For a clustered application this value will be different for each host.\nenv_vars:\nname: CASSANDRABROADCASTADDRESS_INTERNAL\n  static_val: '{{repl ThisHostInterfaceAddress \"docker0\" }}'\n\n{{}}\nfunc ThisNodePublicIPAddress() string\nReturns the public IP address of the host on which the current container instance is deployed as a string. For a clustered application this value will be different for each host.\nenv_vars:\nname: CASSANDRAADDRESSPUBLIC\n  static_val: '{{repl ThisNodePublicIPAddress }}'\nReplaces ThisHostPublicIpAddress which is deprecated.\n\n{{}}\nfunc ThisNodePrivateIPAddress() string\nReturns the private IP address of the host on which the current container instance is deployed as a string. This address is either what was entered manually when host was provisioned or detected from eth0 interface by default. For a clustered application this value will be different for each host.\nenv_vars:\nname: CASSANDRABROADCASTADDRESS_INTERNAL\n  static_val: '{{repl ThisNodePrivateIPAddress }}'\nReplaces ThisHostPrivateIpAddress which is depreciated.\n\n{{}}\nfunc ThisNodeDockerAddress() string\nReturns the docker0 address on the host on which the current container instance is deployed.\nFor a clustered application this value will be different for each host.\n\n{{}}\nfunc LdapCopyAuthFrom(keyName string) string\nPossible Options:\nHostname\nPort\nSearchUsername\nSearchPassword\nBaseDN\nUserSearchDN\nRestrictedGroupCNs\nFieldUsername\nLoginUsername\nenv_vars:\nname: LDAP_HOSTNAME\n  static_val: '{{repl LdapCopyAuthFrom \"Hostname\"}}'\n\n{{}}\nfunc Now() string\nReturns the current timestamp as an RFC3339 formatted string.\nenv_vars:\nname: START_TIME\n  static_val: '{{repl Now }}'\n\n{{}}\nfunc NowFmt(format string) string\nReturns the current timestamp as a formatted string. See Golang's time formatting guidelines [here](https://golang.org/pkg/time/#pkg-constants.\nenv_vars:\nname: START_DATE\n  static_val: '{{repl Now \"20060102\" }}'\n\n{{}}\nfunc TrimSpace(s string) string\nTrim returns a string with all leading and trailing spaces removed.\nenv_vars:\nname: VALUE\n  staticval: '{{repl ConfigOption \"strvalue\" | Trim }}\n\n{{}}\nfunc Trim(s string, args ...string) string\nTrim returns a string with all leading and trailing string contained in the optional args removed (default space).\nenv_vars:\nname: VALUE\n  staticval: '{{repl ConfigOption \"strvalue\" | Trim \" \" \".\" }}\n\n{{}}\nfunc Split(s string, sep string) []string\nSplit slices s into all substrings separated by sep and returns an array of the substrings between those separators.\nenv_vars:\nname: BROKENAPARTABC\n  static_val: '{{repl Split \"A,B,C\" \",\" }}'\n\n{{}}\nfunc ToLower(stringToAlter string) string\nReturns the string, in lowercase.\nenv_vars:\nname: COMPANY_NAME\n  staticval: '{{repl ConfigOption \"companyname\" | ToLower }}'\n\n{{}}\nfunc ToUpper(stringToAlter string) string\nReturns the string, in uppercase.\nenv_vars:\nname: COMPANY_NAME\n  staticval: '{{repl ConfigOption \"companyname\" | ToUpper }}'\n\n{{}}\nfunc HumanSize(size interface{}) string\nHumanSize returns a human-readable approximation of a size in bytes capped at 4 valid numbers (eg. \"2.746 MB\", \"796 KB\"). The size must be a integer or floating point number.\nenv_vars:\nname: MINSIZEHUMAN\n  staticval: '{{repl ConfigOption \"minsize_bytes\" | HumanSize }}\n\n{{}}\nfunc UrlEncode(stringToEncode string) string\nReturns the string, url encoded.\nenv_vars:\nname: SMTPCONNECTIONURL\n  staticval: '{{repl ConfigOption \"smtpemail\" | UrlEncode }}:{{repl ConfigOption \"smtp_password\" | UrlEncode }}@smtp.example.com:587'\n\n{{}}\nfunc Base64Encode(stringToEncode string) string\nReturns a Base64 encoded string.\nenv_vars:\nname: NAME64VALUE\n  static_val: '{{repl ConfigOption \"name\" | Base64Encode }}'\n\n{{}}\nfunc Base64Decode(stringToDecode string) string\nReturns decoded string from a Base64 stored value.\nenv_vars:\nname: NAMEPLAINTEXT\n  staticval: '{{repl ConfigOption \"base64encodedname\" | Base64Decode }}'\n\n{{}}\nfunc ParseBool(str string) bool\nParseBool returns the boolean value represented by the string.\nenv_vars:\nname: VALUE\n  staticval: '{{repl ConfigOption \"strvalue\" | ParseBool }}'\n\n{{}}\nfunc ParseFloat(str string) float64\nParseFloat returns the float value represented by the string.\nenv_vars:\nname: VALUE\n  staticval: '{{repl ConfigOption \"strvalue\" | ParseFloat }}'\n\n{{}}\nfunc ParseInt(str string, args ...int) int64\nParseInt returns the integer value represented by the string with optional base (default 10).\nenv_vars:\nname: VALUE\n  staticval: '{{repl ConfigOption \"strvalue\" | ParseInt }}'\n\n{{}}\nfunc ParseUint(str string, args ...int) uint64\nParseUint returns the unsigned integer value represented by the string with optional base (default 10).\nenv_vars:\nname: VALUE\n  staticval: '{{repl ConfigOption \"strvalue\" | ParseUint }}'\n\n{{}}\nfunc Add(x interface{}, y interface{}) interface{}\nAdds x and y.\n\nIf at least one of the operands is a floating point number, the result will be a floating point number.\n\nIf both operands are integers, the result will be an integer.\nenv_vars:\nname: MAXUSERSPLUS_ONE\n  staticval: '{{repl Add (LicenseFieldValue \"maximumusers\") 1}}'\n\n{{}}\nfunc Sub(x interface{}, y interface{}) interface{}\nSubtracts y from x.\n\nIf at least one of the operands is a floating point number, the result will be a floating point number.\n\nIf both operands are integers, the result will be an integer.\nenv_vars:\nname: MAXUSERSMINUS_ONE\n  staticval: '{{repl Sub (LicenseFieldValue \"maximumusers\") 1}}'\n\n{{}}\nfunc Mult(x interface{}, y interface{}) interface{}\nMultiplies x and y.\n\nIf at least one of the operands is a floating point number, the result will be a floating point number.\n\nIf both operands are integers, the result will be an integer.\nenv_vars:\nname: DOUBLENUMADDRESSES\n  static_val: '{{repl Mult (NodePrivateIPAddressAll \"DB\" \"redis\" | len) 2}}'\n\n{{}}\nfunc Div(x interface{}, y interface{}) interface{}\nDivides x by y.\n\nIf at least one of the operands is a floating point number, the result will be a floating point number.\n\nIf both operands are integers, the result will be an integer and will be rounded down.\nenv_vars:\nname: HALFMAXUSERS\n  staticval: '{{repl Div (LicenseFieldValue \"maximumusers\") 2.0}}'\n\n{{}}\nfunc Namespace() string\n\nNamespace returns the value of the namespace the vendor application is installed in.\n\n{{}}\nServiceAddress(name string, port int32) string\n\nServiceAddress returns the address of the ingress.\n\nproperties:\n  app_url: '{{repl ServiceAddress \"frontend\" 80 }}'\n\n{{}}\nIngressAddress(name string, port int32) string\n\nIngressAddress returns the address of the ingress.\n\nproperties:\n  app_url: '{{repl IngressAddress \"frontend\" 80 }}'\n\n{{}}\nSwarmIngressAddress() string\n\nSwarmIngressAddress returns the ingress address of the swarm cluster.\n\nproperties:\n  app_url: '{{repl SwarmIngressAddress }}'\n\n{{}}\nPremkitAPIAddress() string\n\nPremkitAPIAddress returns the address of the Premkit service in the cluster.\n\nspec:\n  containers:\n  name: myservice\n    image: mycompany/myservice:1.0\n    env:\n    name: REPLICATED_INTEGRATIONAPI\n      value: {{repl PremkitAPIAddress }}\n\n{{}}\nPremkitNetworkName() string\n\nPremkitNetworkName returns the name of the premkit docker network.\n\n{{}}\nStatsdAddress() string\n\nStatsdAddress returns the address of the Statsd service in the cluster.\n\nspec:\n  containers:\n  name: myservice\n    image: mycompany/myservice:1.0\n    env:\n    name: REPLICATEDSTATSDADDRESS\n      value: {{repl StatsdAddress }}\n\n{{}}\nStatsdNetworkName() string\n\nStatsdNetworkName returns the name of the Statsd docker network.\n\nNotes\n\nWhen referencing another container in a template object, you must make sure the referenced container is started first.  Please see the Events and Orchestration section for more information on orchestrating container startup.\n",
        "title": "Template Functions",
        "description": "The dynamic configuration management functionality available throughout the Replicated YAML.",
        "weight": "210",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/test-procs.md",
        "content": "\nUsing the test_proc directive in your app definition YAML, you enable your customers to\neasily test the validity of the unsaved configuration parameters they're entering.\n\nYou can specify a runonsave property in any test_proc. if this evaluates to true, all\ninstallations will automatically run the test_proc whenever the settings page is saved.\nIf the test_proc fails, a dialog will be displayed.\n\nTest commands are built directly into the Replicated platform. We currently offer the\nfollowing set of commands:\n\nResolve Host\nresolve_host – Test whether or not a hostname can be resolved. Applies to a single item.\nThe item's value will be resolved on the Replicated Management machine. Success or failure\nis reported directly on the configuration screen.\n\nSee sample YAML for Hostname & SSL inputs and\nleveraging the test_procs for each.\n\nconfig:\n  items:\n  name: hostname\n    title: Hostname\n    type: text\n    runonsave: true\n    test_proc:\n      display_name: Test Hostname Resolution\n      command: resolve_host\n      arg_fields:\n      hostname\n\nGithub App Auth\ngithubappauth – Test whether or not the supplied GitHub app key and secret are valid.\nApplies to a group of items. Both mainline and private enterprise versions of GitHub are\nsupported. This command expects exactly 5 arguments. These arguments come from values\nentered by your customer into the config items within this group. The arguments, in\nexpected order:\n\nGitHub service type. Either githubtypepublic, or githubtypeenterprise.\nEnterprise service hostname, e.g. github.mydomain.com\nEnterprise service protocol. Either githubenterpriseprotocolhttp, or githubenterpriseprotocolhttps.\nGitHub app OAuth key.\nGitHub app OAuth secret.\n\nSee sample YAML for displaying common GitHub inputs and\nleveraging the test_proc.\n\nconfig:\nname: github\n  title: Github Integration\n  description: Provide the location of your Github account\n  test_proc:\n    display_name: Test GitHub Authentication\n    command: githubappauth\n    timeout: 5 # in seconds, default is 10\n    arg_fields:\n    github_type\n    githubenterprisehost\n    githubenterpriseprotocol\n    githubclientid\n    githubclientsecret\n  items:\n  name: github_type\n\nAWS Auth\naws_auth – Test whether or not the supplied AWS key and secret are valid. Applies to a\ngroup of items. This command expects 3 arguments. These arguments come from values entered\nby your customer into the config items within this group. The arguments, in expected order:\n\nAccess key ID.\nSecret access key.\nAWS service. Either ec2, s3, or sqs.\n\nconfig:\nname: aws\n  title: AWS Integration\n  description: Provide your AWS access credentials\n  test_proc:\n    display_name: Test Authentication\n    command: aws_auth\n    timeout: 5\n    arg_fields:\n    awsaccesskey_id\n    awssecretaccess_key\n    aws_service\n  items:\n  name: awsaccesskey_id\n\nCertificate Verification\ncertificate_verify – Test whether or not the supplied x509 certificate is valid. Optionally\nvalidate the key pair and hostname. Applies to a group of items. This command expects the\ncertificate as the first argument with additional optional arguments private key and hostname.\nThese arguments come from values entered by your customer into the config items within this\ngroup. The arguments, in expected order:\n\nx509 certificate\nPrivate key (optional)\nHostname (optional)\nSample YAML for Hostname & SSL inputs\nand leveraging the test_procs for each.\n\nconfig:\nname: web_server\n  title: Web server settings\n  description: Please provide your hostname and TLS cert and key.\n  test_proc:\n    display_name: Verify TLS settings\n    command: certificate_verify\n    timeout: 5\n    arg_fields:\n    ssl_cert\n    ssl_key\n    hostname\n  items:\n  name: hostname\n\nSMTP Auth\nsmtp_auth – Test whether or not the supplied credentials are valid for the given SMTP\nserver. Note that this procedure only tests authentication; it does not test whether or\nnot mail is actually deliverable.\n\nThe command expects 5 arguments. These arguments come from values entered by your customer\ninto the config items within this group. The arguments, in expected order:\n\nAddress:port of the SMTP server to test against\nWhether or not to use SSL/TLS. Possible values: \"0\" or \"1\"\nType of authentication to try. Possible values: \"Plain\", \"CRAM-MD5\", \"Login\" or \"None\"\nUsername to send\nPassword to send\n\n**Note that the address of the SMTP server must contain the correct port number ie\nsmtp.gmail.com:587 for the test proc to validate correctly. A type of \"None\" will only\nensure the socket is evailable.**\n\nSee sample YAML for common SMTP inputs\nand leveraging the test_proc.\n\nconfig:\nname: email_settings\n  title: E-mail Settings\n  description: Provide SMTP details which will enable the sending of e-mails\n  test_proc:\n    display_name: Test SMTP Authentication\n    command: smtp_auth\n    arg_fields:\n    smtp_address\n    smtpusessl\n    smtpauthtype\n    smtpauthusername\n    smtpauthpassword\n  items:\n  name: smtp_address\n    type: text\n\nFile Exists\nfile_exists - Test if a file exists on the host by giving its expected path. This can be\napplied to a group or an item, if applied to an item they imply the value, if applied to a\ngroup, then you must specify the item to get the value from. The following arguments are\noptional:\n\nRegular expression to run against file path (optional)\nFile mode. (optional)\nFile mode is at least (optional - default exact match)\n\nconfig:\nname: some_itmes\n  title: Config Items\n  items:\n  name: afilethat_exists\n    type: text\n    title: Path to file that needs to exist\n    test_proc:\n      display_name: Check that file exists\n      command: file_exists\n      args:\n      \"^\\\\/data\\\\/.+\\\\.tar$\"\n      \"644\"\n      true\n\nRegex Match\nregex_match - Test if a given value matches a a regular expression for validation purposes.\nThis can be applied to a group or an item, if applied to an item they imply the value, if\napplied to a group, then you must specify the item to get the value from.\n\nRegular expression to run against text area\nError message if regular expression doesn't match. (optional)\n\nconfig:\nname: some_items\n  title: Config Items\n  items:\n  name: phone_number\n    type: text\n    test_proc:\n      display_name: Is this a Phone Number?\n      command: regex_match\n      args:\n      \"^[0-9()-]+$\"\n      \"That doesn't seem to be a phone number!\"\n\nLDAP Auth\nldap_auth - This test will ensure that the fields that your customer is supplying are\ncomplete and valid, for a detailed implementation reference see our LDAP integration section.\nNote you have to pass all the arg_fields for the test to validate correctly.\n\nThese arguments come from values entered by your customer\ninto the config items within this group.  For more details see LDAP Integration\n\nRequired arguments:\n\nldap_type - Type of LDAP integration:\n  ldaptypeopenldap (A LDAP server other then Active Directory)\n  ldaptypead (Active directory)\nldap_hostname - The LDAP host, ie. ldap.customer.com\nldap_port Port LDAP services are listening on ie 389 or 636\nldap_encryption - Type of encryption your LDAP is using:\n  ldapencryptionplain (Plain)\n  ldapencryptionstarttls (upgrade to SSL/TLS encrypted communication after initial communication)\n  ldapencryptionldaps (fully encrypted from start)\nldapsearchuser - The LDAP search user that performs user lookup.\nldapsearchpassword - The password for the LDAP search user.\nldapbasedn - The Distinguished Name (DN) of an LDAP subtree you want to search for users and groups.\nldapusersearchdn - DN to search for users.\nldaprestricteduser_group - Restricted DN group that only users from this group will be allowed to log in with. This can be an empty string.\nldapusernamefield - The username that will appear on the user's app name field.\nldaploginusername - The LDAP username that identifies the LDAP user who attempts authentication.\nldaploginpassword - The LDAP password for the user attempting authentication.\n\nOptional arguments:\n\nldapadvancedsearch - True value indicates that advanced search queries is to be used.\nldapuserquery - The LDAP query to use to find the user.\nldaprestrictedgroup_query - The LDAP query to use to validate user group membership.\n\nconfig:\nname: ldap_settings\n  title: LDAP Server Settings\n  when: authsource=authtype_ldap\n  test_proc:\n      display_name: Test Credentials\n      command: ldap_auth\n      arg_fields:\n        ldap_type\n        ldap_hostname\n        ldap_port\n        ldap_encryption\n        ldapsearchuser\n        ldapsearchpassword\n        ldapbasedn\n        ldapusersearchdn\n        ldaprestricteduser_group\n        ldapusernamefield\n        ldaploginusername\n        ldaploginpassword\n        ldapadvancedsearch\n        ldapuserquery\n        ldaprestrictedgroup_query\n\nJSON Validation\n\njsonvalidate - Tests for valid JSON.  The validatejson test_proc takes a single argument which is the item name that contains the JSON to test.\n\nconfig:\nname: json\n  title: Advanced JSON Value\n  type: text\n  test_proc:\n    display_name: Validate JSON\n    command: validate_json\n    arg_fields:\n    json\n",
        "title": "Test Procs",
        "description": "Test Procs enable customers to easily test the validity of the unsaved configuration parameters they're entering during installation.",
        "weight": "205",
        "categories": [
            "Packaging"
        ]
    },
    {
        "date": "2016-07-03T04:02:20Z",
        "uri": "/content/docs/packaging-an-application/yaml-overview.md",
        "content": "\nReplicated will deploy an application that is defined in a YAML spec. We currently support deploying an application that uses the Replicated scheduler or deploying a Kubernetes application. Understanding how each of these will be installed and maintained is an important consideration when choosing the scheduler to use to deploy your application.\n\nReplicated Scheduler\nThe Replicated scheduler is a propietary, mature container scheduler that supports the features required by enterprise customers. It was designed and built to enable your customer to install a complex application on one or a cluster of servers, without having to preinstall anything else. They can simply bring Linux servers that are compatible with Docker 1.7.1 or newer, and can deploy and manage your application on their servers. This scheduler supports airgap installations and many other features that enterprise users need. We recommend choosing the Replicated scheduler for most installations.\n\nOur YAML definition is stored in a public repo at https://github.com/replicatedhq/libyaml/.\n\nSwarm Scheduler\nMore recent versions of the Docker Engine include swarm mode for natively scheduling containers across a cluster of Docker Engines called a swarm. Replicated supports Docker version 13.1 or greater and the Swarm scheduler. A Docker Compose version 3 YAML is required to distribute your application in swarm mode. We recommend choosing the Docker Swarm scheduler if you have existing Compose YAML and if your customer does not require a Linux distribution without support for newer Docker versions.\n\nKubernetes Scheduler\nKubernetes is a popular cluster and orchestration tool when running Docker containers. Often, you may already have Kubernetes resources written to deploy your application. Replicated can deliver this down to an existing Kubernetes cluster, and provide all of the enterprise features that will be required to support, maintain, update and run your application behind the firewall. We recommend choosing the Kubernetes scheduler if you have existing Kubernetes YAML and if you customer is able to provision and maintain a Kubernetes cluster.\n\nReplicated API Version\nAt the top of the YAML file, regardless of the scheduler, there must be a Replicated API version. The current API version to use is {{}}. Note: The Changelog tracks the API version.\n\nreplicatedapiversion: {{}}\n\nApp Basics\nThe next section includes some basic information about your application release including the app name.\n\nname: My Enterprise Application\n\nDetailed App Properties Description\nThe properties section includes definitions of some optional (but recommended) application properties. For a list of available properties see Application Properties. You will notice the {{repl escape sequence. This invokes a Replicated template function, which will be discussed in more detail soon.\n\nproperties:\n  app_url: http://{{repl ThisNodePrivateIPAddress }}\n  console_title: My Enterprise Application\n\nSupport Page\nReplicated supports displaying custom markdown content on the Support page of the admin console. This can be defined in the consolesupportmarkdown key.\n\nconsolesupportmarkdown: |\n  Documentation for My Enterprise Application can be found here.\n\n  When contacting us for help, please download a Support Bundle (below) and attach it to the ticket.  The support\n  bundle contains generic system information collected from this server.  It does not contain any data from\n  your instance of My Enterprise Application.\n\nSnapshots (Backups)\nThe snapshots key is available to to enable and configure Snapshots. The following example will allow your customer to enable snapshots and create a script to run the snapshot.\n\nbackup:\n  enabled: '{{repl ConfigOptionEquals \"backup_enabled\" \"1\" }}'\n  hidden: '{{repl ConfigOptionEquals \"backup_enabled\" \"0\" }}'\n  pause_all: false\n  script: |\n    #!/bin/sh\n    myappcli backup\n\nCMD\nThe Replicated platform has some built in commands that make writing your configuration much more powerful. In the cmds section you can write commands which we will use later.  These are useful to generate install-time values such as default certs/keys, randomized passwords, JWT token keys, etc.\n\ncmds:\nname: postgres_password\n  cmd: random\n  args:\n  \"64\"\n\nComponents\nThe components section defines the container runtime environment for your containers, if using the Replicated scheduler.  This will include everything from the container image, environment variables, application orchestration, config files, optional clustering and more.\n\ncomponents:\nname: Redis\n  containers:\n  source: public\n    image_name: redis\n    version: 3.0.5\n\nMonitors\nWhen using the Replicated scheduler, the containers which make up your components can be monitored for resource usage metrics on an individual basis. For each metric, simply specify each component and container image pair. For example, if you want to see CPU and memory usage metrics for some of your Redis container and your private worker image pulled from quay.io (in a Worker component):\n\nmonitors:\n  cpuacct:\n  Redis,redis\n  Worker,quay.io/myenterpriseapp/worker\n  memory:\n  Redis,redis\n  Worker,quay.io/myenterpriseapp/worker\n\nCustom Metrics\nRegardless of the scheduler used, Replicated can also display custom metrics sent from the running instance to the Admin Console by including the stats names in a custom_metrics key.\n\ncustom_metrics:\ntarget: stats.gauges.myapp100.disk..\n  retention: \"1s:10m,1m:20m,1h:30d\"\n  aggregation_method: \"average\"\n  xfiles_factor: 0.3\ntarget: stats.gauges.myapp100.ping.*\n  retention: \"1s:7d\"\n  aggregation_method: \"last\"\n  xfiles_factor: 0.6\n\nReady State\n(Note: The Ready State is only compatible with the Replicated scheduler. To learn how Replicated starts a Kubernetes application, see the detail in the Kubernetes document).\n\nYou can add a health check that Replicated will poll after your containers have all been started. The purpose of this is to report when your application is fully running and ready to start using. Once your application is running, we stop polling this health check and rely on other methods to monitor the status. The timeout parameter allows you to specify (in seconds) how long to keep retrying the command, if it fails. You can use a timeout value of -1 to indicate infinite polling. A timeout of 0 is not supported and causes the default of 10 minutes to be used.\n\n{{}} You can specify an optional third argument to set the HTTP timeout. Replicated will use a default timeout of 5 seconds if not specified.\n\nAvailable Commands:\nhttpstatuscode\ntcpportaccept\n\nstate:\n  ready:\n    command: httpstatuscode\n    args:\n    'http://{{repl NodePublicIPAddress \"My Component\" \"my-web-container\" }}/ping'\n    '200'\n    '5'\n    timeout: 900\n\nCustomer Config Section\nThis section is where you can configure fields to gather input from the user. This input can be used to further configure your application. The values here can be used as inputs to container environment variables, config files, and more using template functions or tested for validity with test procs. The config section is comprised of configuration groups and items. These items will render as a form in the Settings screen of the Replicated admin console.\n\nconfig:\nname: hostname\n  title: Hostname\n  description: Ensure this domain name is routable on your network.\n  items:\n  name: hostname\n    title: Hostname\n    type: text\n    value_cmd:\n      name: host_ip\n      value_at: 0\n    ...\n\nAdmin Commands\nOptionally you can expose admin commands in your containers. To configure the commands, add the following section. This example will allow the customer to run the redis-cli command with any arbitrary arguments. The command will be executed only in the Docker containers that match image name and version as well as defined in the named component. A command that will work with this configuration is replicated admin redis-cli info. Replicated will find the appropriate node to run this command on; the customer can run these on the main admin console.\n\nadmin_commands:\nalias: redis-cli\n  command: [redis-cli]\n  run_type: exec\n  component: DB\n  container: redis\n\nCustom Preflight Checks\nA preflight check is a test that is run before installing and running an application. The test will analyze the system to determine if the environment meets the minimum requirements and provide feedback during installation if these requirements are not met.\n\nhost_requirements:\n  docker_version: \"1.10.3\"\n  cpu_cores: 2\n  cpu_mhz: 2400\n  memory: 8GB\n  disk_space: 8GB\n  replicated_version: \"=2.3.0 ",
        "title": "YAML Overview",
        "description": "An overview of the various sections of the Replicated YAML.",
        "weight": "202",
        "categories": [
            "Packaging"
        ]
    }
]